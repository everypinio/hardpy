{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hardware testing that simply works","text":""},{"location":"#hardware-testing-that-simply-works_1","title":"Hardware testing that simply works","text":"<p>For embedded and test engineers building production jigs and end-of-line stations.</p> <p>Write tests in pytest. Run them with a production-line UI for operators. Ready for mass production.  </p> <p>Get started</p>"},{"location":"#benefits","title":"Benefits","text":"<ul> <li> <p> Ship tests faster</p> <p>Spend time on measurements and tooling, not on building a custom test runner and UI from scratch.</p> </li> <li> <p> More reliable test stands</p> <p>Consistent execution and clearer failures mean fewer \u201crandom\u201d line issues and fewer escalations back to engineering.</p> </li> <li> <p> No rewrites as requirements grow</p> <p>Start with a simple bench flow, then add serial tracking, richer prompts, more instruments, data exports, and fleet/analytics integrations without throwing your test code away.</p> </li> </ul>"},{"location":"#dont-want-to-spend-nights-at-the-factory","title":"Don\u2019t want to spend nights at the factory?","text":"<p>Know what\u2019s happening online from day 0. Take control of your tests with instant, out-of-the-box connectivity.</p> <p>Capture your test data with StandCloud</p>"},{"location":"#everything-you-need-for-testing","title":"Everything you need for testing","text":"<ul> <li> <p> Plain Python</p> <p>Write test plans in plain Python. Easy to review, version, and maintain.</p> </li> <li> <p> Production-ready</p> <p>Runs on PCs and single-board computers (SBCs), works offline, and supports continuous integration (CI).</p> </li> <li> <p> Reusable drivers and building blocks</p> <p>Connect instruments through drivers/adapters and reuse steps across products.</p> </li> <li> <p> Operator-friendly steps</p> <p>Inline instructions, images, confirmations, and data entry. Operators see exactly what to do.</p> </li> <li> <p> Client-server operator UI</p> <p>Open the operator panel anywhere there is a browser (tablet, phone) while tests execute on the host PC.</p> </li> <li> <p> Multilingual operator panel</p> <p>Localize the operator UI with built-in language switching for global teams.</p> </li> <li> <p> Traceability by serial number</p> <p>Store results and measurements in a structured way: serial, run, steps, metrics.</p> </li> <li> <p> Flexible storage</p> <p>Store results and logs in CouchDB, JSON, or your own backend, and optionally push to StandCloud.</p> </li> <li> <p> StandCloud analytics (optional)</p> <p>Track pass rate (yield), failures, and retests with dashboards and serial-number history. Go to StandCloud</p> </li> </ul>"},{"location":"#trusted-by-teams-shipping-hardware","title":"Trusted by teams shipping hardware","text":"<p>HardPy is used by engineers building production test rigs and end-of-line stations. Pair it with StandCloud to store results by serial number, track pass rate (yield), and catch failure trends early.</p> <ul> <li> Manufacturing test engineer \u2014 \"HardPy made our test flow consistent across operators - fewer mistakes, cleaner logs.\"</li> <li> Hardware lead \u2014 \"Serial-level traceability in StandCloud helped us spot a failing step in the first day.\"</li> <li> Production engineer \u2014 \"We reused the same test plan from the lab to EOL with minimal changes.\"</li> </ul> <p>Get started</p>"},{"location":"changelog/","title":"Changelog","text":"<p>Versions follow Semantic Versioning: <code>&lt;major&gt;.&lt;minor&gt;.&lt;patch&gt;</code>.</p>"},{"location":"changelog/#0221","title":"0.22.1","text":"<ul> <li>Add the <code>--hardpy-config-file</code> to the pytest plugin.    This feature enables tests to be run in a folder that is different    to the one containing the hardpy.toml file.   [PR-259]</li> </ul>"},{"location":"changelog/#0220","title":"0.22.0","text":"<ul> <li>Add the <code>get_hardpy_function</code> to retrieve the current configuration from the hardpy.toml.   [PR-257]</li> <li>Add support for custom fields to the hardpy.toml file.   [PR-257]</li> </ul>"},{"location":"changelog/#0211","title":"0.21.1","text":"<ul> <li>Fix double encoding in the dialog boxes.   [PR-255]</li> </ul>"},{"location":"changelog/#0210","title":"0.21.0","text":"<ul> <li>Add the ability to change the text for buttons in dialog boxes.   [PR-253]</li> </ul>"},{"location":"changelog/#0201","title":"0.20.1","text":"<ul> <li>Add the <code>--version</code> option to hardpy call.   [PR-251]</li> <li>Update some dependencies to their latest versions to improve overall system security.   [PR-251]</li> </ul>"},{"location":"changelog/#0200","title":"0.20.0","text":"<ul> <li>Add JSON file storage support as alternative to CouchDB.   [PR-239]</li> </ul>"},{"location":"changelog/#0191","title":"0.19.1","text":"<ul> <li>Add a <code>stop_time</code> validator function. If a case or module has a start time,   it must also have a stop time.   [PR-237]</li> <li>Add a \"Manual mode\" string to the tests name if the <code>manual_collect</code>    setting in the hardpy.toml file is set to <code>true</code>.    [PR-237]</li> <li>Add a Czech translation of the operator panel.   [PR-236]</li> </ul>"},{"location":"changelog/#0190","title":"0.19.0","text":"<ul> <li>Add manual test selection mode allowing users to choose specific    tests to run via UI checkboxes.   [PR-224]</li> <li>Add multiple test plan configurations.   [PR-225]</li> </ul>"},{"location":"changelog/#0183","title":"0.18.3","text":"<ul> <li>Add support for pytest 9.   [PR-233]</li> </ul>"},{"location":"changelog/#0182","title":"0.18.2","text":"<ul> <li>Add support for working with macOS.   [PR-231]</li> </ul>"},{"location":"changelog/#0181","title":"0.18.1","text":"<ul> <li>Expand the log for the StandCloud data synchronization process.   [PR-228]</li> </ul>"},{"location":"changelog/#0180","title":"0.18.0","text":"<ul> <li>Add a mechanism to automatically synchronize test data with the StandCloud platform.   [PR-205]</li> <li>Add an authorization mechanism to StandCloud via API keys.   [PR-205]</li> <li>Enhance measurement display with mathematical interval notation and proper bracket handling.   [PR-226]</li> <li>Add measurement display functionality to operator panel showing numeric and string measurements with name-value-unit formatting.   [PR-223]</li> </ul>"},{"location":"changelog/#0171","title":"0.17.1","text":"<ul> <li>Fix issue where when using a large button, it overlaps the content.   [PR-219]</li> </ul>"},{"location":"changelog/#0170","title":"0.17.0","text":"<ul> <li>Add a named volume to the docker-compose.yaml generator.   [PR-216]</li> <li>Add database updating after the alert has been called.   [PR-211]</li> <li>Add a handler for JSON objects that cannot be serialized for the database.   [PR-211]</li> <li>Add <code>sound_on</code> configuration option to the frontend settings for enabling/disabling test completion sound notifications.   [PR-208]</li> <li>Add pass/fail button functionality to dialog boxes for manual test verification.   [PR-207]</li> <li>Add configurable full-size start/stop button for the operator panel with full-size layout option.   [PR-206]</li> <li>Add configurable modal result windows that display test completion status (PASS/FAIL/STOP)   with auto-dismiss functionality and enhanced keyboard interaction handling.   [PR-204]</li> <li>Exclude the <code>Skipped</code> status from the <code>caused_dut_failure_id</code> reason.   [PR-202]</li> </ul>"},{"location":"changelog/#0160","title":"0.16.0","text":"<ul> <li>Add error code cleanup in the <code>attempts</code>.   [PR-200]</li> <li>Add an event handler for the inability to collect tests and the   absence of a database launch.   [PR-200]</li> <li>Add unknown values in the <code>StandCloudLoader</code> class.   [PR-200]</li> <li>Add the type <code>None</code> to info field in the <code>Dut</code>, <code>SubUnit</code>, <code>Instrument</code>,   and <code>TestStand</code> tables.   [PR-200]</li> <li>Add <code>serial_number</code> and <code>part_number</code> to the Instrument table.   [PR-200]</li> <li>Update js packages.   [PR-197]</li> <li>Add the charts view to the HardPy operator panel.   [PR-186]</li> <li>Added storage of the operator message state to avoid reopening the window after closing.   [PR-190]</li> <li>Change the database and interface synchronization mechanism.   The statestore and runstore databases can contain multiple documents,   which are defined by the frontend host name and port.   The current document is no longer being created.   The name of the document in the database consists of the   host name and port for the frontend, as described in the hardpy.toml file.   [PR-187]</li> <li>Simplify the process of working with multiple HardPy instances by   using a single CouchDB instance instead of multiple instances.   [PR-187]</li> </ul>"},{"location":"changelog/#hardpy-0152","title":"HardPy 0.15.2","text":"<ul> <li>Change the logic of the test case attempts.   The test case fields (msg, assertion_message, chart, measurements, and artifact)   are now cleared when the test case passes on the second or subsequent attempt.   The error code is now cleared when a test case passes on the second or subsequent attempt.   [PR-192]</li> </ul>"},{"location":"changelog/#hardpy-0151","title":"HardPy 0.15.1","text":"<ul> <li>Remove the <code>datetime</code> type from the info fields in all tables.   [PR-188]</li> </ul>"},{"location":"changelog/#hardpy-0150","title":"HardPy 0.15.0","text":"<ul> <li>Remove <code>tests_dir</code> variable from hardpy.toml.    [PR-182]</li> <li>Add check for message presence in set_operator_message.   [PR-180]</li> <li>Add the <code>ErrorCode</code> class.   [PR-180]</li> <li>Add the ability to store charts (data series) in the database.   Add <code>set_case_chart</code> function and <code>Chart</code> class.   [PR-179]</li> <li>Add arguments for <code>hardpy start</code>.   [PR-175]</li> <li>Add numeric and string measurements: <code>set_case_measurement</code> function and   <code>NumericMeasurement</code> and <code>StringMeasurement</code> classes.   [PR-177]</li> <li>Update database schema by SubUnit table.   [PR-174]</li> <li>Add <code>set_dut_sub_unit</code> function.   [PR-174]</li> <li>Add markers <code>case_group</code> and <code>module_group</code>.   [PR-173]</li> <li>Add functions: <code>set_user_name</code>, <code>set_batch_serial_number</code>, <code>set_stand_revision</code>, <code>set_instrument</code> <code>set_process_name</code>, <code>set_process_number</code>, <code>set_process_info</code>, <code>set_dut_name</code>, <code>set_dut_type</code>,   <code>set_dut_revision</code>, <code>set_stand_info</code>.   [PR-172]</li> <li>Change <code>DuplicateSerialNumberError</code> and <code>DuplicatePartNumberError</code> to <code>DuplicateParameterError</code>.   [PR-172]</li> <li>Update database schema by some tables: Process, Instrument, NumericMeasurement.   [PR-162]</li> <li>Update database schema by some fields: user, batch_serial_number, caused_dut_failure_id, error_code process, revision, instruments, group, measurements.   [PR-162]</li> <li>Add the <code>caused_dut_failure_id</code> logic to the filling process.   [PR-162]</li> </ul>"},{"location":"changelog/#hardpy-0140","title":"HardPy 0.14.0","text":"<ul> <li>Add status display in words depending on the testing status.   [PR-165]</li> <li>Add a timeout to the <code>load</code> function of the <code>StandCloudLoader</code> class.   [PR-166]</li> <li>Fix logic for processing spacebar pressing.   [PR-164]</li> <li>Fix the display of the module duration after the operator panel has been restarted.   [PR-163]</li> <li>Add the display of last test run duration.   [PR-163]</li> <li>Add the option to translate the HardPy operator panel using ISO 639 language codes.   [PR-159]</li> </ul>"},{"location":"changelog/#hardpy-0130","title":"HardPy 0.13.0","text":"<ul> <li>Change CI settings, add testing of tests on different versions of packages.   [PR-157]</li> <li>Add an alert to operator panel when the following are called not from tests:   <code>set_message</code>, <code>set_case_artifact</code>, <code>set_module_artifact</code>, <code>run_dialog_box</code>,   and <code>get_current_attempt</code>.   [PR-154]</li> <li>Update the status of the test case and module when the test stops.   Only one case and one module receive the \"stopped\" status, while the rest are marked as \"skipped.\"   [PR-154]</li> <li>Change StandCloud authorization process to OAuth2 Device Flow by   RFC8628.   [PR-152]</li> </ul>"},{"location":"changelog/#hardpy-0121","title":"HardPy 0.12.1","text":"<ul> <li>Fix the situation in which the module and case stop times stop updating when the user stops the tests.   [PR-149]</li> </ul>"},{"location":"changelog/#hardpy-0120","title":"HardPy 0.12.0","text":"<ul> <li>Update <code>test_run</code> StandCloudReader function.   The <code>test_run</code> function provides access to 2 endpoints: <code>/test_run/{test_run_id}</code> and <code>/test_run</code>.   [PR-145]</li> <li>Update database scheme description.   [PR-140]</li> <li>Add test stand number by using <code>set_stand_number</code> function.   [PR-140]</li> <li>Add prohibition to run tests on the port if the port is busy.   [PR-136]</li> <li>Add marker critical.   [PR-135],   [PR-142],   [PR-143]</li> <li>Change dependency behaviour when a case or module name does not exist.   Such a test or module will be executed.   [PR-130]</li> <li>Add the ability to add dependency from multiple tests.   [PR-130]</li> <li>Transfer frontend from CRA to Vite.   [PR-122]</li> <li>Remove bug with scrolling tests with operator messages.   [PR-122]</li> <li>Logging to pytest.ini is no longer configured during HardPy project initialization by <code>hardpy init</code>.   [PR-122]</li> </ul>"},{"location":"changelog/#hardpy-0112","title":"HardPy 0.11.2","text":"<ul> <li>Fix StandCloud login support in version 3.12+.   [PR-134]</li> </ul>"},{"location":"changelog/#hardpy-0111","title":"HardPy 0.11.1","text":"<ul> <li>Change StandCloud URL for publish test report to <code>/test_report</code>.   [PR-131]</li> <li>Add <code>StandCloudReader</code> class for reading data from StandCloud.   [PR-131]</li> <li>Fix the behavior with empty module start time with skipped test.   [PR-131]</li> <li>Fix the behavior with empty module stop time with skipped test.   [PR-129]</li> <li>Add Stand info to Operator panel.   [PR-127]</li> <li>Add the <code>tests_name</code> field to hardpy.toml.   The <code>tests_name</code> allows to name the test suite in the operator panel.   [PR-126]</li> <li>Add CLI commands: <code>hardpy start</code>, <code>hardpy stop</code> and <code>hardpy status</code>.   [PR-126]</li> <li>Change the StandCloud API address to <code>/hardpy/api</code>.   [PR-125]</li> </ul>"},{"location":"changelog/#hardpy-0110","title":"HardPy 0.11.0","text":"<ul> <li>Remove the internal HardPy socket on port 6525. Pytest plugin arguments <code>--hardpy-sh</code> and <code>--hardpy-sp</code>   are left for backward compatibility with version below 0.10.1.   [PR-114]</li> <li>Add the ability to add HTML pages using <code>HTMLComponent</code> to dialog boxes and operator messages.   [PR-104]</li> </ul>"},{"location":"changelog/#hardpy-0101","title":"HardPy 0.10.1","text":"<ul> <li>Fix StandCloud authorization process in Windows.   [PR-110]</li> </ul>"},{"location":"changelog/#hardpy-0100","title":"HardPy 0.10.0","text":"<ul> <li>Add the <code>[stand_cloud]</code> section to the hardpy.toml configuration file.   [PR-85]</li> <li>Add the <code>StandCloudLoader</code> class to append the test result to the StandCloud.   [PR-85]</li> <li>Add support for StandCloud login and logout with <code>sc-login</code> and <code>sc-logout</code> commands.   [PR-85]</li> <li>Add alert field in statestore database.   [PR-85]</li> <li>Add alert to control panel by calling <code>set_alert</code> method in <code>HardpyPlugin</code>.   [PR-85]</li> </ul>"},{"location":"changelog/#hardpy-090","title":"HardPy 0.9.0","text":"<ul> <li>Add the ability to add images to operator messages like a dialog box.   [PR-95]</li> <li>Add non-blocking mode for operator message.   [PR-95]</li> <li>Add <code>clear_operator_message</code> function for closing operator message.   [PR-95]</li> <li>Add font_size parameter for operator message and dialog box text.   [PR-97]</li> <li>Add a 1 second pause between attempts in the <code>attempt</code> marker.   [PR-98]</li> <li>Fix an issue where operator messages and dialog boxes sometimes did not   open before the browser page reloaded.   [PR-98]</li> </ul>"},{"location":"changelog/#hardpy-080","title":"HardPy 0.8.0","text":"<ul> <li>Modify API for dialog boxes with images.   [PR-84]</li> <li>Add the ability to add images to all widgets.   [PR-84]</li> <li>Add ability to add borders to images.   [PR-84]</li> <li>Fix timezone format using the tzlocal package.   [PR-79]</li> <li>Add ability to close <code>set_operator_message</code> with <code>Escape</code> button.   [PR-84]</li> <li>Fix skipped test case and module status. Now skipped test status is skipped, not ready.   [PR-77]</li> <li>Add an exception when entering the same selection items or step names in dialog boxes.   [PR-86]</li> <li>Add the ability to clear the runstore database before running hardpy   using the the <code>--hardpy-clear-database</code> option of the pytest-hardpy plugin.   [PR-83]</li> </ul>"},{"location":"changelog/#hardpy-070","title":"HardPy 0.7.0","text":"<ul> <li>Add an attempt marker to indicate the number of attempts to run a test before it passes successfully.   [PR-65]</li> <li>Add a get_current_attempt method to get the current attempt number.   [PR-65]</li> <li>Add the ability to run multiple dialog boxes in a single test.   [PR-65]</li> <li>Fix the problem of freezing the dialog box in some test cases.   [PR-65]</li> <li>Add autofocus on dialog boxes (on the Confirm button or the first item in the list).   [PR-65]</li> <li>Remove the progress field from the runstore database.   [PR-66]</li> <li>Add the hw_id variable to the test_stand field in the database obtained from the stand computer.   [PR-67]</li> <li>Add the location variables to the test_stand field in the database.   [PR-66]</li> <li>Move the timezone and driver database variables to the test_stand field.   [PR-66]</li> <li>Add a schema version. The schema version is fixed to version 1.   [PR-66]</li> <li>Change timezone from two strings to one string.   [PR-69]</li> <li>Replace the Flake8 linter with a Ruff linter.   [PR-58]</li> </ul>"},{"location":"changelog/#hardpy-061","title":"HardPy 0.6.1","text":"<ul> <li>Fix running tests with a simple <code>pytest</code> command.   [PR-60]</li> </ul>"},{"location":"changelog/#hardpy-060","title":"HardPy 0.6.0","text":"<p>In HardPy, the startup principle has changed compared to version 0.5.0 and lower. The <code>hardpy-panel</code> command is no longer available.</p> <p>The HardPy project of version 0.6.0 or later must contain the file hardpy.toml.</p> <ul> <li>Add the ability to clear the statestore database before running hardpy   using the the <code>--hardpy-clear-database</code> option of the pytest-hardpy plugin.   [PR-49]</li> <li>Add the name and info fields to test_stand in the database schema.   [PR-49]</li> <li>Add the part_number field to dut in the database schema.   [PR-49]</li> <li>Add the attempt field to the test case in the database schema.   [PR-54]</li> <li>Add <code>set_stand_name</code> and <code>set_dut_part_number</code> functions.   [PR-49]</li> <li>Add a hardpy template project using the <code>hardpy init</code> command.   [PR-44]</li> <li>Add a hardpy config .toml file - hardpy.toml.   [PR-44]</li> <li>Refactor pytest-hardpy plugin options.   [PR-44]</li> <li>Add CLI to hardpy as an entry point. The <code>hardpy-panel</code> command is no longer available.   [PR-44]</li> <li>Fix use of special characters in dialog boxes. ASCII symbols are passed from frontend to backend.   [PR-40]</li> <li>Fix status of stopped tests.   [PR-45]</li> <li>Fix progress bar for skipped tests. Progress bar fills to the end when tests are skipped.   [PR-43]</li> <li>Add report name generation when serial number is missing.   [PR-47]</li> </ul>"},{"location":"changelog/#hardpy-051","title":"HardPy 0.5.1","text":"<ul> <li>Add the ability to work with cloud couchdb via couchdb config.   [PR-51]</li> </ul>"},{"location":"changelog/#hardpy-050","title":"HardPy 0.5.0","text":"<ul> <li>Refactor dialog box API.   [PR-33]</li> <li>Add conda.yaml example.   [PR-32]</li> <li>Add .vscode folder.   [PR-32]</li> <li>Fix catching exceptions and displaying them in the operator panel.   [PR-32]</li> <li>Add dialog box with radiobutton, checkbox, image, multiple steps.   [PR-29]   [PR-31]   [PR-31]</li> </ul>"},{"location":"changelog/#hardpy-040","title":"HardPy 0.4.0","text":"<ul> <li>Add base dialog box, text input and numeric input.   [PR-24]</li> <li>Add dialog box invocation functionality.   [PR-25]</li> <li>Add a socket mechanism to transfer data from the uvicorn server to the pytest subprocess.   [PR-25]</li> </ul>"},{"location":"changelog/#hardpy-030","title":"HardPy 0.3.0","text":"<ul> <li>Add implementation of test dependencies without using third-party plugins.   [PR-15]</li> <li>Reduce the number of database calls.   [PR-12]</li> <li>Speed up test collection.   [PR-12]</li> </ul>"},{"location":"changelog/#hardpy-020","title":"HardPy 0.2.0","text":"<ul> <li>Add documentation page.   [PR-5]</li> <li>Remove the ability to access the <code>HardPyPlugin</code>.   Users can now only register via the ini file.   [PR-6]</li> </ul>"},{"location":"changelog/#hardpy-010","title":"HardPy 0.1.0","text":"<ul> <li>Add pytest-hardpy and hardpy panel to the package.   [PR-1]</li> <li>Add frontend data synchronization via CouchDB data replication to PouchDB.   [PR-1]</li> <li>Add documentation.   [PR-1]</li> <li>CouchDB is the main database.   [PR-1]</li> </ul>"},{"location":"getting_started/","title":"Getting started","text":""},{"location":"getting_started/#overview","title":"Overview","text":"<p>HardPy allows you to:</p> <ul> <li>Create test benches for devices using pytest;</li> <li>Use a browser to view, start, stop, and interact with tests;</li> <li>Store test results in the CouchDB database or to simple JSON files;</li> <li>Store test results on the StandCloud analytics platform.</li> </ul>"},{"location":"getting_started/#to-install","title":"To Install","text":"<pre><code>pip install hardpy\n</code></pre>"},{"location":"getting_started/#launch","title":"Launch","text":""},{"location":"getting_started/#with-couchdb","title":"With CouchDB","text":"<ol> <li>Create your first test bench.   <pre><code>hardpy init\n</code></pre></li> <li>Launch CouchDB database via docker compose    in the   background.   <pre><code>cd tests\ndocker compose up -d\n</code></pre></li> <li>Launch HardPy operator panel.   <pre><code>hardpy run\n</code></pre></li> <li> <p>View operator panel in browser: http://localhost:8000/ </p> </li> <li> <p>View the latest test report: http://localhost:5984/_utils</p> <p>Login and password: dev, database - runstore.</p> </li> </ol>"},{"location":"getting_started/#without-a-database","title":"Without a database","text":"<ol> <li>Create your first test bench.   <pre><code>hardpy init --no-create-database --storage-type json\n</code></pre></li> <li>Launch HardPy operator panel.   <pre><code>hardpy run\n</code></pre></li> <li>View operator panel in browser: http://localhost:8000/</li> </ol>"},{"location":"getting_started/#measurement-instruments","title":"Measurement instruments","text":"<p>HardPy does not contain any drivers for interacting with measuring equipment.  However, HardPy allows you to work with any Python code, meaning you can use  open libraries to interact with measuring equipment.</p> <ul> <li>InstrumentKit</li> <li>Instrumental</li> <li>PyMeasure</li> <li>PyTango</li> <li>QCoDeS</li> <li>QCoDeS contrib drivers</li> <li>Labgrid</li> </ul>"},{"location":"about/development/","title":"Development","text":""},{"location":"about/development/#environment","title":"Environment","text":""},{"location":"about/development/#requirements","title":"Requirements","text":"<ul> <li>python version must be equal to or greater than  3.10;</li> <li>yarn version must be equal to 4.0.1;</li> <li>node.js version must be equal to or greater than 18.12.0;</li> <li>CouchDB version must be equal to or greater than 3.2.0;</li> </ul>"},{"location":"about/development/#conda","title":"Conda","text":"<p>Create a conda.yaml file if you prefer to work through Anaconda or Miniconda:</p> <pre><code>name: hardpy\nchannels:\n  - defaults\n  - conda-forge\ndependencies:\n  - python=3.10\n  - pip&gt;=22\n  - nodejs=20\n  - yarn=4.0.1\n  - pip:\n      - -r requirements.txt\n      - -r requirements-doc.txt\n</code></pre> <p>create environment:</p> <pre><code>conda env create -f conda.yaml\n</code></pre> <p>Activate:</p> <pre><code>conda activate hardpy\n</code></pre>"},{"location":"about/development/#venv","title":"venv","text":"<p>If you prefer to work through venv:</p> <pre><code>python -m venv venv\n</code></pre> <p>Activate:</p> <pre><code>source venv/bin/activate\n</code></pre> <p>Install dependencies:</p> <pre><code>pip install -r requirements.txt\npip install -r requirements-doc.txt\n</code></pre>"},{"location":"about/development/#frontend","title":"Frontend","text":""},{"location":"about/development/#overview","title":"Overview","text":"<p>The frontend of the project can be developed, debugged, and built using Visual Studio Code (VSCode) or command-line tools. Below are the steps to run, debug, and build the frontend application.</p> <p>node.js and yarn are required to build the frontend.</p>"},{"location":"about/development/#1-running-the-frontend-for-hardpy-debugging","title":"1. Running the frontend for Hardpy debugging","text":"<p>To run the frontend application for debugging purposes, you can use the Run Frontend configuration in VSCode or execute commands manually.</p>"},{"location":"about/development/#using-vscode","title":"Using VSCode","text":"<ol> <li>Open the \"Run and Debug\" in VSCode.</li> <li>Select the Run Frontend configuration.</li> <li>Click the \"Run\" button (or press <code>F5</code>).</li> </ol> <p>This will:</p> <ul> <li>Launch <code>yarn</code>.</li> <li>Start the frontend application in development mode with debugging enabled.</li> </ul>"},{"location":"about/development/#manually","title":"Manually","text":"<p>Navigate to the frontend directory and run the following commands:</p> <pre><code>npm install &amp;&amp; npm run dev\n</code></pre> <p>You can specify your own port by adding it as an environment variable:</p> <pre><code>PORT=4000\n</code></pre> <p>By default, the frontend opens on port 3000 at the address http://localhost:3000/.</p>"},{"location":"about/development/#2-debugging-the-frontend","title":"2. Debugging the frontend","text":"<p>To debug the frontend application in the Chrome browser, use the Debug Frontend configuration in VSCode.</p>"},{"location":"about/development/#using-vscode_1","title":"Using VSCode","text":"<ol> <li>Ensure the frontend is running (use the Run Frontend configuration or manually start it).</li> <li>Open the \"Run and Debug\" in VSCode.</li> <li>Select the Debug Frontend configuration.</li> <li>Click the \"Run\" button (or press <code>F5</code>).</li> </ol> <p>This will:</p> <ul> <li>Launch Chrome and attach the debugger to the running frontend application at <code>http://localhost:3000</code>.</li> <li>Enable source maps for easier debugging.</li> </ul>"},{"location":"about/development/#manually_1","title":"Manually","text":"<ol> <li>Start the frontend application (as described in the Run Frontend section).</li> <li>Open Chrome and navigate to <code>http://localhost:3000</code>.</li> <li>Use Chrome DevTools (<code>F12</code>) to debug the application.</li> </ol>"},{"location":"about/development/#3-building-the-frontend-for-hardpy-package","title":"3. Building the frontend for Hardpy package","text":"<p>To build the frontend and include it in the Hardpy package, you can use the provided scripts or run the commands manually.</p> <p>Use the <code>compile_front.sh</code> script from <code>scripts</code> folder or run the scripts manually:</p> <pre><code>pip install -r requirements.txt\npython -m build\n</code></pre> <p>For frontend rebuilding use the <code>recompile_front.sh</code> from <code>scripts</code> folder or run the scripts manually:</p> <pre><code>pip uninstall $PRJ -y\n\nrm -rf hardpy/hardpy_panel/frontend/dist\nrm -rf hardpy/hardpy_panel/frontend/node_modules\nrm -rf *.egg-info\nrm -rf dist\nrm -rf __pycache__\n\npip install -r requirements.txt\npython -m build\n</code></pre>"},{"location":"about/development/#4-setting-the-debug_frontend-variable","title":"4. Setting the <code>DEBUG_FRONTEND</code> variable","text":"<p>To enable debugging for the frontend application, you must set the <code>DEBUG_FRONTEND</code> environment variable. Without this variable, the frontend will not run in debug mode, and the Run Frontend and Debug Frontend configurations will not work as expected.</p>"},{"location":"about/development/#option-1-setting-debug_frontend-in-vscode-configuration","title":"Option 1: setting <code>DEBUG_FRONTEND</code> in VSCode configuration","text":"<p>You can specify the <code>DEBUG_FRONTEND</code> variable directly in the VSCode launch configuration. Here's an example of how to include it in a test configuration:</p> <pre><code>{\n    \"name\": \"Python: Example Dialog box\",\n    \"type\": \"debugpy\",\n    \"request\": \"launch\",\n    \"module\": \"hardpy.cli.cli\",\n    \"console\": \"integratedTerminal\",\n    \"env\": {\n        \"DEBUG_FRONTEND\": \"1\"\n    },\n    \"args\": [\n        \"run\",\n        \"examples/dialog_box\"\n    ]\n}\n</code></pre> <p>This configuration ensures that the frontend runs in debug mode when the test is executed.</p>"},{"location":"about/development/#option-2-setting-debug_frontend-in-env-file","title":"Option 2: setting <code>DEBUG_FRONTEND</code> in <code>.env</code> file","text":"<p>Alternatively, you can define the <code>DEBUG_FRONTEND</code> variable in a <code>.env</code> file located in the root of your project. </p> <ol> <li>Create a <code>.env</code> file in the root directory of your project (if it doesn't already exist).</li> <li>Add the following line to the <code>.env</code> file:</li> </ol> <pre><code>DEBUG_FRONTEND=1\n</code></pre> <ol> <li>Ensure your VSCode or runtime environment loads the <code>.env</code> file. Many tools and frameworks (e.g., <code>python-dotenv</code>) automatically load variables from <code>.env</code>.</li> </ol>"},{"location":"about/development/#adding-translations","title":"Adding Translations","text":"<p>To add or modify translations for the HardPy operator panel:</p> <ol> <li> <p>Navigate to the translations directory:    <pre><code>cd hardpy/hardpy_panel/frontend/public/locales/\n</code></pre></p> </li> <li> <p>Create a new folder for your language using its ISO 639 code (e.g., <code>en</code> for English, <code>ru</code> for Russian, <code>fr</code> for French).</p> </li> <li> <p>Inside the language folder, create a <code>translation.json</code> file with the following structure:    <code>json   {     \"app\": {       \"title\": \"HardPy Operator Panel\",       \"lastLaunch\": \"Last launch:\",       \"duration\": \"Duration\",       \"seconds\": \"s\",       \"soundOn\": \"Turn on the sound\",       \"soundOff\": \"Turn off the sound\",       \"debugOn\": \"Turn on the debug mode\",       \"debugOff\": \"Turn off the debug mode\",       \"connection\": \"Establishing a connection... \ud83e\uddd0\ud83d\udd0e\",       \"dbError\": \"Database connection error. \ud83d\ude45\ud83c\udffd\u200d\u2640\ufe0f\ud83d\udeab\",       \"noEntries\": \"No entries in the database \ud83d\ude45\ud83c\udffd\u200d\u2640\ufe0f\ud83d\udeab\",       \"stoppedTestCase\": \"Stopped Test Case\",       \"failedTestCases\": \"Failed Test Cases\",       \"modalResultDismissHint\": \"Click anywhere or press any key to dismiss\",       \"modalResultAutoDismissHint\": \"Auto-dismissing in {{seconds}} seconds...\",       \"status\": {         \"ready\": \"Ready\",         \"run\": \"Run\",         \"passed\": \"Pass\",         \"failed\": \"Fail\",         \"stopped\": \"Stopped\",         \"unknown\": \"Unknown\"       }     },     \"button\": {       \"start\": \"Start\",       \"stop\": \"Stop\",       \"confirm\": \"Confirm\"     },     \"error\": {       \"dbConnectionTitle\": \"Database Connection Error\",       \"dbConnectionMessage\": \"Failed to establish connection with the database\"     },     \"chart\": {       \"dataChart\": \"Chart Data\",       \"xAxis\": \"X Axis\",       \"yAxis\": \"Y Axis\",       \"chart\": \"Chart\",       \"showChart\": \"Show chart {{title}}\",       \"fullscreenButton\": \"Open chart in full screen\",       \"series\": \"Series {{number}}\"     },       \"operatorDialog\": {       \"defaultTitle\": \"Message\",       \"imageAlt\": \"Operator message image\",       \"htmlCodeTitle\": \"HTML Code\",       \"htmlLinkTitle\": \"HTML Link\",       \"enterAnswer\": \"Enter answer\",       \"fieldNotEmpty\": \"The field must not be empty\",       \"notificationTitle\": \"Notification\",       \"notificationDesc\": \"The window was closed. Tests stopped.\",       \"numericInputError\": \"Please enter a number\",       \"radioButtonError\": \"Please select one option\",       \"checkboxError\": \"Please select at least one option\"     },     \"suiteList\": {       \"loadingTests\": \"Loading tests... \ud83e\udd14\",       \"refreshHint\": \"Try refreshing the page.\",       \"standName\": \"Stand name\",       \"status\": \"Status\",       \"startTime\": \"Start time\",       \"finishTime\": \"Finish time\",       \"alert\": \"Alert\"     },     \"testSuite\": {       \"nameColumn\": \"Name\",       \"dataColumn\": \"Data\",       \"loading\": \"Loading...\",       \"stubName\": \"Test Suite\"     }   }</code></p> </li> <li> <p>Translate all values while keeping the same JSON structure and keys.</p> </li> </ol> <p>Note: Always use valid ISO 639 language codes for folder names.</p>"},{"location":"about/development/#excluding-tests-from-ci","title":"Excluding tests from CI","text":"<p>To exclude specific tests from running in Continuous Integration (CI) environments, you can mark them with the <code>@pytest.mark.manual</code> decorator. These tests will be skipped during automated test runs but can still be executed locally when explicitly selected.</p> <pre><code>@pytest.mark.manual\ndef test_of_manual():\n    assert True\n</code></pre>"},{"location":"about/development/#launch","title":"Launch","text":"<ol> <li>Install dependencies or create environment.</li> <li>Compile frontend if it's the first launch.</li> <li>Launch <code>hardpy init</code> with path to tests folder.</li> <li>Launch CouchDB instance.</li> <li>Launch <code>hardpy run</code> with path to tests folder.</li> </ol> <p>Addresses:</p> <ul> <li>HardPy panel: http://localhost:8000/</li> <li>CouchDB: http://localhost:5984/_utils/</li> </ul>"},{"location":"about/development/#documentation","title":"Documentation","text":""},{"location":"about/development/#server","title":"Server","text":"<p>Documentation server command is:</p> <pre><code>mkdocs serve\n</code></pre> <p>Documentation address: http://localhost:8000/</p>"},{"location":"about/development/#build","title":"Build","text":"<p>Documentation building command is:</p> <pre><code>mkdocs build\n</code></pre> <p>The result is in the folder <code>public</code>.</p>"},{"location":"about/frontend_sync/","title":"Frontend data synchronization","text":"<p>Data synchronization uses the replication mechanism of CouchDB and PouchDB.</p> <ul> <li>The statestore database contains the document, which is a JSON object that stores the current state of the test run without artifacts. The name of the synchronization document is generated based on the host  and port specified for the server in the hardpy.toml file:  <code>&lt;frontend_host&gt;_&lt;frontend_port&gt;</code>. The plugin updates the document as testing progresses using the StateStore class.</li> <li>The runstore database contains the document,  which is a JSON object that stores the current state of the test run with artifacts - a report on the current test run.</li> </ul>"},{"location":"about/frontend_sync/#statestore-scheme","title":"Statestore scheme","text":"<p>The runstore database is similar to statestore database, but there are differences:</p> <ul> <li>runstore contains the artifact field for test run, module, and case.</li> <li>runstore does not contain some fields: progress, dialog_box, attempt,   alert, operator_data, operator_msg.</li> </ul> <p>The document of the statestore database contains some section.</p>"},{"location":"about/frontend_sync/#main","title":"main","text":"<ul> <li>_rev: a CouchDB revision MVCC token;   The variable is assigned automatically.</li> <li>_id: unique document identifier.   The variable is assigned automatically.</li> <li>progress: test run progress.   The variable is assigned automatically.</li> <li>stop_time: the end time of the test in Unix seconds. The variable is assigned automatically.</li> <li>start_time: the start time of the test in Unix seconds. The variable is assigned automatically.</li> <li>status: test execution status from pytest: passed, failed, skipped, stopped.   The variable is assigned automatically.</li> <li>name: the name of the test suite. It is displayed in the header of the operator panel.   The user can specify the name using the <code>tests_name</code> variable in the hardpy.toml file.   If this variable is not set, the name will be taken from the directory name containing the tests.</li> <li>dut: DUT information. See the dut section for more information.</li> <li>process information about the testing process. See the process section for more information.</li> <li>modules: module (pytest files) information. See the modules section for more information.</li> <li>user: HardPy operator panel user name.   The variable is assigned by set_user_name function.   It can only be set once per test run.</li> <li>batch_serial_number: the serial number of the device batch.   The variable is assigned by set_batch_serial_number function.   It can only be set once per test run.</li> <li>caused_dut_failure_id: the ID of the first failed test that caused the test failure.   Format id: <code>module_name::case_name</code>. The variable is assigned automatically.</li> <li>error_code the error code (non-negative integer) of the caused DUT failure test.   The variable is assigned by ErrorCode class.</li> <li>test_stand: test stand information. See the test_stand section for more information.</li> <li>modules: module (pytest files) information. See the modules section for more information.</li> <li>alert: operator panel alert information.   Alert information is displayed at the top of the operator panel.   The variable is assigned automatically.</li> <li>operator_data operator panel data for operator message.   This is filled in when the set_operator_message function is called.</li> <li>operator_msg: text of an operator message.   This is filled in when the set_operator_message or   clear_operator_message functions is called.</li> </ul>"},{"location":"about/frontend_sync/#test_stand","title":"test_stand","text":"<p>The test_stand section contains information about the test stand. It is a computer on which HardPy is running and to which the DUT test equipment is connected.</p> <ul> <li>name: test stand name. It can only be set once per test run.   The user can specify the stand name by using set_stand_name function.</li> <li>revision test stand revision. It can only be set once per test run.   The user can specify the stand revision by using set_stand_revision function.</li> <li>drivers: DEPRECATED, DO NOT USE IT.   Information about drivers in the form of a dictionary, including test equipment and test equipment software.</li> <li>instruments: list of information about the instruments (i.e. equipment) that form part of the test bench.   See the instrument section for more information.</li> <li>drivers: information about drivers in the form of a dictionary, including test equipment and test equipment software.   The user can specify the driver info by using set_driver_info function.</li> <li>info: dictionary containing additional information about the test stand.   The user can specify the additional info by using set_stand_info function.</li> <li>timezone: timezone of test stand as a string. The variable is assigned automatically.</li> <li>location: the location of the test stand, e.g., the country, city, or laboratory number.   It can only be set once per test run. The user can specify the location by using   set_stand_location function.</li> <li>number: test stand number. Some stands may have the same name and   run on the same computer but have different numbers. It can only be set once per test run.   The user can specify the stand number by using set_stand_number function.</li> <li>hw_id: test stand machine id (GUID) or host name. The variable is assigned automatically by   the py-machineid package</li> </ul>"},{"location":"about/frontend_sync/#instrument","title":"instrument","text":"<p>Information about the instrument (i.e equipment) that form part of the test bench. Information about equipment such as power supplies and voltmeters, which may be used in the test stand, should be stored. The user can specify the instrument information by using set_instrument function.</p> <ul> <li>name: instrument name.</li> <li>revision instrument revision.</li> <li>serial_number instrument serial number.</li> <li>part_number instrument part number.</li> <li>number: instrument number.</li> <li>comment: comment on the instrument.</li> <li>info: dictionary containing additional information about the instrument.</li> </ul>"},{"location":"about/frontend_sync/#process","title":"process","text":"<p>Information about the testing process. For example, the device undergoes several stages of testing during the manufacturing process, including acceptance testing, firmware testing and functional testing of the board both with and without the case. Each stage may have its own name, number and other attributes.</p> <ul> <li>name: process name. It can only be set once per test run.   The user can specify the process name by using set_process_name function.</li> <li>number: process number. It can only be set once per test run.   The user can specify the process number by using set_process_number function.</li> <li>info: dictionary containing additional information about the process.   The user can specify the additional info by using set_process_info function.</li> </ul>"},{"location":"about/frontend_sync/#dut","title":"dut","text":"<p>The device under test section contains information about the DUT.</p> <ul> <li>name: human-readable name of the DUT.   The user can specify the DUT name by using set_dut_name function.   It can only be set once per test run.</li> <li>type: type of DUT, f.e \"PCBA\", \"Casing\", etc.   The user can specify the DUT type by using set_dut_type function.   It can only be set once per test run.</li> <li>serial_number: DUT serial number. This identifier is unique to the testing device or board.   It can only be set once per test run.   The user can specify the DUT serial number by using set_dut_serial_number function.</li> <li>part_number: DUT part number. This identifier of a particular part design, board or device.   It can only be set once per test run.   The user can specify the DUT part number by using set_dut_part_number function.</li> <li>revision: DUT revision. The user can specify the DUT revision by using   set_dut_revision function.   It can only be set once per test run.</li> <li>sub_units: list of sub units of main DUT. Each sub-unit has a similar structure to the DUT itself, but does not contain any other sub units.   The user can add the sub unit by using set_dut_sub_unit function.</li> <li>info: dictionary containing additional information about the the DUT, such as batch, board revision, etc.   The user can specify the additional info by using set_dut_info function.</li> </ul>"},{"location":"about/frontend_sync/#operator_msg","title":"operator_msg","text":"<p>The operator_msg section contains operator message data. The user can open a operator message box by using the set_operator_message function. The user can close a operator message box by using the clear_operator_message function.</p> <ul> <li>msg: message for operator.</li> <li>title: the title of operator message dialog box.</li> <li>visible: should a message be displayed on the operator panel.</li> <li>id: operator message id.</li> <li>font_size: operator message font size.</li> <li>image: information about image.</li> <li>address: image address.</li> <li>width: image width in percent.</li> <li>border: image border in pixels.</li> <li>base64: image in base64 code.</li> <li>html: information about html.</li> <li>code_or_url: html code or link.</li> <li>is_raw_html: is html code is raw.</li> <li>width: html width in percent.</li> <li>border: html border in pixels.</li> </ul>"},{"location":"about/frontend_sync/#modules","title":"modules","text":"<p>The modules section contains the information about tests. Each module contains information from a single test file. The module's name is the same as the file's name.</p> <ul> <li>test_{module_name}: an object containing information about a specific module.   The <code>{module_name}</code> variable is assigned automatically.   Contains the following fields:</li> <li>status: module test execution status. The variable is assigned automatically.</li> <li>name: module name, by default the same as module_id.     The user can specify the module name by using module_name marker.</li> <li>start_time: start time of module testing in Unix seconds. The variable is assigned automatically.</li> <li>stop_time: end time of module testing in Unix seconds. The variable is assigned automatically.</li> <li>group: the group of module: Setup, Main or Teardown (Main by default).     The user can specify the module group by using module_group marker.</li> <li>cases: an object that contains information about each test case within the module.<ul> <li>test_{case_name}: an object containing information about a specific case.   The <code>{case_name}</code> variable is assigned automatically.   Contains the following fields:</li> <li>status: test case execution status. The variable is assigned automatically.</li> <li>name: case name, by default the same as case_id.     The user can specify the case name by using case_name marker.</li> <li>start_time: start time of case testing in Unix seconds. The variable is assigned automatically.</li> <li>stop_time: end time of case testing in Unix seconds. The variable is assigned automatically.</li> <li>assertion_msg: assert or error message if the test case fails. The variable is assigned automatically.     However, the user can write their own message in case of an assertion, which will be written to this variable.     For example:     <pre><code>assert False, \"This is an example\"\n</code></pre>     The assertion_msg is displayed in the operator panel next to the test case in which it was called.</li> <li>msg: the log message is displayed in the operator panel next to the test case in which it was called.     The user can specify and update current message by using set_message function.</li> <li>group: the group of case: Setup, Main or Teardown (Main by default).     The user can specify the case group by using case_group marker.</li> <li>measurements: list of measurements.     See the measurements section for more information.</li> <li>attempt: number of attempts per successful test case.     The user can specify the case name by using attempt marker.</li> <li>dialog_box: information about dialog box.     The user can open a dialog box within a test case by using the run_dialog_box function.     Data from the dialog box is passed through the DialogBox class.<ul> <li>title_bar: title bar of the dialog box.</li> <li>dialog_text: text displayed in the dialog box.</li> <li>widget: information about the widget.</li> <li>info: widget additional information.</li> <li>type: type of the widget.</li> <li>image: information about image.</li> <li>address: image address.</li> <li>width: image width in percent.</li> <li>border: image border in pixels.</li> <li>base64: image in base64 code.</li> <li>html: information about html.</li> <li>code_or_url: html code or link.</li> <li>is_raw_html: is html code is raw.</li> <li>width: html width in percent.</li> <li>border: html border in pixels.</li> <li>visible: should a dialog box be displayed on the operator panel.</li> <li>id: dialog box id.</li> <li>font_size: dialog box font size.</li> </ul> </li> </ul> </li> </ul>"},{"location":"about/frontend_sync/#measurements","title":"Measurements","text":"<p>The measurements section contains the information about measurements. The user fills in the list of measurements for each case by  set_case_measurement function.</p>"},{"location":"about/frontend_sync/#numeric-measurement","title":"Numeric measurement","text":"<p>A NumericMeasurement is a structured container for storing numerical measurements. For example, the measured voltage must fall within a specific range.</p> <ul> <li>type: <code>numeric</code> by default.</li> <li>value: numeric measure value.</li> <li>name: numeric measure name.</li> <li>unit: unit of numeric measure.</li> <li>operation: comparison operators of numeric measure.</li> <li>comparison_value value to compare against.</li> <li>lower_limit: lower limit for range operations.</li> <li>upper_limit upper limit for range operations.</li> <li>result the result of the measurement if the operation exists; otherwise, it is empty.   Filled in without user involvement.</li> </ul> <pre><code>// voltage measure\n{\n  \"type\": \"numeric\",\n  \"value\": \"3.57\",\n  \"name\": \"Main voltage\",\n  \"lower_limit\": \"3.45\",\n  \"upper_limit\": \"3.65\",\n  \"unit\": \"V\",\n  \"operation\": \"GTLT\",\n  \"result\": true\n}\n</code></pre> <pre><code>// temperature measure\n{\n  \"type\": \"numeric\",\n  \"value\": \"14\",\n}\n</code></pre>"},{"location":"about/frontend_sync/#string-measurement","title":"String measurement","text":"<p>A StringMeasurement is a structured container for storing string measurements. For example, the firmware version comparison.</p> <ul> <li>type: <code>string</code> by default.</li> <li>value: string measure value.</li> <li>name: string measure name.</li> <li>operation: comparison operators of string measure.</li> <li>comparison_value value to compare against.</li> <li>casesensitive: case sensitivity, default is <code>True</code>.</li> <li>result the result of the measurement if the operation exists; otherwise, it is empty.   Filled in without user involvement.</li> </ul> <pre><code>{\n  \"type\": \"string\",\n  \"value\": \"3.1.2\",\n  \"operation\": \"EQ\",\n  \"casesensitive\": true,\n  \"result\": true\n}\n</code></pre>"},{"location":"about/frontend_sync/#statestore-current-document-example","title":"Statestore current document example","text":"<pre><code>    {\n      \"_rev\": \"44867-3888ae85c19c428cc46685845953b483\",\n      \"_id\": \"current\",\n      \"progress\": 100,\n      \"stop_time\": 1695817266,\n      \"start_time\": 1695817263,\n      \"status\": \"failed\",\n      \"alert\": \"\",\n      \"operator_data\": {\n        \"dialog\": \"\"\n      },\n      \"name\": \"hardpy-stand\",\n      \"user\": null,\n      \"batch_serial_number\": \"0613\",\n      \"caused_dut_failure\": \"test_1_a::test_minute_parity\",\n      \"error_code\": null,\n      \"dut\": {\n        \"name\": \"analogue\",\n        \"type\": \"PCBA\",\n        \"serial_number\": \"92c5a4bb-ecb0-42c5-89ac-e0caca0919fd\",\n        \"part_number\": \"0507\",\n        \"revision\": \"rev_1\",\n        \"sub_units\": [],\n        \"info\": {\n          \"sw_version\": \"3.2.0\"\n        }\n      },\n      \"test_stand\": {\n        \"hw_id\": \"840982098ca2459a7b22cc608eff65d4\",\n        \"name\": \"test_stand_1\",\n        \"revision\": \"1.0\",\n        \"timezone\": \"Europe/Belgrade\",\n        \"drivers\": {},\n        \"location\": \"Belgrade_1\",\n        \"number\": 2,\n        \"instruments\": [\n          {\n            \"name\": \"Everypin Power Supply\",\n            \"revision\": \"2.0\",\n            \"serial_number\": \"4235098\",\n            \"part_number\": \"E012\",\n            \"number\": 1,\n            \"comment\": \"\",\n            \"info\": {}\n          }\n        ],\n        \"info\": {}\n      },\n      \"operator_msg\": {\n        \"msg\": \"Operator message\",\n        \"title\": \"Message\",\n        \"visible\": true,\n        \"id\": \"f45ac1e7-2ce8-4a6b-bb9d-8863e30bcc78\"\n      },\n      \"process\": {\n        \"name\": \"acceptance\",\n        \"number\": 1,\n        \"info\": {}\n      },\n      \"modules\": {\n        \"test_1_a\": {\n          \"status\": \"failed\",\n          \"name\": \"Module 1\",\n          \"start_time\": 1695816884,\n          \"stop_time\": 1695817265,\n          \"group\": \"MAIN\",\n          \"cases\": {\n            \"test_dut_info\": {\n              \"status\": \"passed\",\n              \"name\": \"DUT info \",\n              \"start_time\": 1695817263,\n              \"stop_time\": 1695817264,\n              \"assertion_msg\": null,\n              \"measurements\": [],\n              \"msg\": null,\n              \"group\": \"MAIN\",\n              \"attempt\": 1,\n              \"dialog_box\": {\n                \"title_bar\": \"Example of text input\",\n                \"dialog_text\": \"Type some text and press the Confirm button\",\n                \"widget\": {\n                  \"info\": {\n                    \"text\": \"some text\"\n                  },\n                  \"type\": \"textinput\"\n                },\n                \"image\": {\n                  \"address\": \"assets/test.png\",\n                  \"width\": 100,\n                  \"border\": 0,\n                },\n                \"html\": {\n                  \"code_or_url\": \"https://everypinio.github.io/hardpy/\",\n                  \"width\": 100,\n                  \"border\": 0,\n                  \"is_raw_html\": false\n                },\n                \"visible\": true,\n                \"id\": \"b57ab1e7-8cf8-4a6a-bb9d-8863ea0bcc78\"\n              }\n            },\n            \"test_minute_parity\": {\n              \"status\": \"failed\",\n              \"name\": \"Test 1\",\n              \"start_time\": 1695817264,\n              \"stop_time\": 1695817264,\n              \"assertion_msg\": \"The test failed because minute 21 is odd! Try again!\",\n              \"attempt\": 1,\n              \"measurements\": [],\n              \"msg\": [\n                \"Current minute 21\"\n              ],\n              \"group\": \"MAIN\",\n            }\n          }\n        }\n      }\n    }\n</code></pre>"},{"location":"documentation/","title":"Documentation","text":""},{"location":"documentation/#python-version","title":"Python version","text":"<p>HardPy is based on the python 3.10 and supports versions 3.11, 3.12, 3.13.</p>"},{"location":"documentation/#hardpy-structure","title":"HardPy structure","text":"<p>HardPy includes several parts.</p>"},{"location":"documentation/#hardpy-cli","title":"HardPy CLI","text":"<p>HardPy CLI on a structural scheme.</p> <ul> <li>Entry point for HardPy.</li> <li>HardPy test bench creator.</li> <li>Launcher for operator panel.</li> <li>StandCloud authorization tool.</li> </ul> <p>For more info, read CLI.</p>"},{"location":"documentation/#hardpy-pytest-plugin","title":"HardPy pytest plugin","text":"<p>HardPy includes the pytest-hardpy plugin for pytest. Compatible with pytest versions above 7. You can run tests not only through the operator panel but also through the pytest itself.</p> <p>pytest-hardpy on a structural scheme.</p> <ul> <li>The pytest wrapper for running pytest from the HardPy operator panel.</li> <li>The pytest plugin with API for storing data in a database.</li> </ul> <p>For more info, read pytest-hardpy.</p>"},{"location":"documentation/#hardpy-operator-panel","title":"HardPy operator panel","text":"<p>HardPy includes a React application - HardPy operator panel. It allows you to use a browser to view and interact with your tests and write test results to a database.</p> <p>hardpy-panel on a structural scheme.</p> <ul> <li>Web interface for viewing tests and starting/stopping tests.</li> <li>FastAPI application for processing frontend commands.</li> <li>PouchDB - web database for synchronizing data from CouchDB and the hardpy operator panel.</li> </ul> <p>For more info, read hardpy-panel.</p>"},{"location":"documentation/#couchdb","title":"CouchDB","text":"<p>HardPy uses CouchDB as its database but you can write final result to any database because CouchDB stores data in a simple document. Developers can create their adapter for any database and store the test report in a way that suits them. By default HardPy allows you to store all reports in CouchDB. HardPy is compatible with CouchDB versions above 3.2.</p> <ul> <li>Database to store current test data and store all test results.</li> </ul> <p>For more info, read database.</p>"},{"location":"documentation/#database-adapter","title":"Database adapter","text":"<ul> <li>HardPy allows you to use a simple database adapter to store test results in CouchDB   using the CouchdbLoader.</li> <li>HardPy allows you to use a StandCloud database adapter to store test results in StandCloud   using the StandCloudLoader.</li> <li>A developer can create a database adapter to store test results in any database.</li> </ul>"},{"location":"documentation/#structural-scheme","title":"Structural scheme","text":"HardPy structure"},{"location":"documentation/cli/","title":"Command line interface","text":"<p>HardPy uses the CLI (command line interface) as an entry point.</p> <p>For more information use:</p> <pre><code>hardpy --help\n</code></pre>"},{"location":"documentation/cli/#hardpy-init","title":"hardpy init","text":"<p>The <code>hardpy init</code> command is used to create a test bench. By default, it creates the <code>tests</code> directory.</p> <p>It consists of:</p> <ul> <li><code>test_1.py</code> - a pytest file with a simple test;</li> <li><code>conftest.py</code> - the pytest conftest file;</li> <li><code>pytest.ini</code> - pytest configuration .ini file for pytest;</li> <li><code>hardpy.toml</code> - HardPy configuration file;</li> <li><code>docker-compose.yaml</code> - docker-compose file for running the database;</li> <li><code>database</code> - CouchDB database directory;</li> <li><code>couchdb.ini</code> - the couchdb configuration .ini file in the database directory;</li> </ul> <p>You can run <code>hardpy init &lt;test_bench_name&gt;</code>, where <code>&lt;test_bench_name&gt;</code> is the name of your test bench.</p> <p>The <code>hardpy init</code> command allows you to change the initial HardPy settings. More info in hardpy config.</p> <pre><code> Usage: hardpy init [OPTIONS] [TESTS_DIR]\n\n Initialize HardPy tests directory.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502   tests_dir      [TESTS_DIR]  [default: None]                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --tests-name                                   TEXT     Specify a tests suite name.                        \u2502\n\u2502 --create-database      --no-create-database             Create CouchDB database.                           \u2502\n\u2502                                                         [default: create-database]                         \u2502\n\u2502 --database-user                                TEXT     Specify a database user. [default: dev]            \u2502\n\u2502 --database-password                            TEXT     Specify a database user password. [default: dev]   \u2502\n\u2502 --database-host                                TEXT     Specify a database host. [default: localhost]      \u2502\n\u2502 --database-port                                INTEGER  Specify a database port. [default: 5984]           \u2502\n\u2502 --frontend-port                                INTEGER  Specify a frontend port. [default: 8000]           \u2502\n\u2502 --frontend-host                                TEXT     Specify a frontend host. [default: localhost]      \u2502\n\u2502 --sc-address                                   TEXT     Specify a StandCloud address.                      \u2502\n\u2502 --sc-connection-only --no-sc-connection-only            Check StandCloud service availability before start.\u2502\n|                                                         [default: no-sc-connection-only]                   \u2502\n\u2502 --sc-autosync        --no-sc-autosync                   Enable StandCloud auto syncronization.             | \n|                                                         [default: no-sc-autosync]                          \u2502\n|                                                         [default: check-stand-cloud]                       \u2502\n\u2502 --sc-api-key                                  TEXT      Specify a StandCloud API key.                      \u2502\n\u2502 --storage-type                                TEXT      Specify a storage type. [default: couchdb]         |\n\u2502 --help                                                  Show this message and exit.                        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>To obtain this information, use:</p> <pre><code>hardpy init --help\n</code></pre>"},{"location":"documentation/cli/#hardpy-run","title":"hardpy run","text":"<p>The <code>hardpy run</code> command is used to start the operator panel server. By default, it starts HardPy in the current directory.</p> <p>You can run the <code>hardpy run &lt;tests_directory&gt;</code> command, where <code>&lt;tests_directory&gt;</code> is the path to the directory with your tests.</p> <pre><code> Usage: hardpy run [OPTIONS] [TESTS_DIR]\n\n Run HardPy server.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502   tests_dir      [TESTS_DIR]  [default: None]                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>To obtain this information, use:</p> <pre><code>hardpy run --help\n</code></pre>"},{"location":"documentation/cli/#hardpy-start","title":"hardpy start","text":"<p>The <code>hardpy start</code> command is used to launch HardPy tests while the HardPy opener panel is running. By default, it starts tests in the current directory.</p> <pre><code> Usage: hardpy start [OPTIONS] [TESTS_DIR]\n\n Usage with arguments: hardpy start --arg test_mode=debug --arg device_id=DUT-007\n\n Start HardPy tests.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502   tests_dir      [TESTS_DIR]  [default: None]                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --arg  -a        TEXT  Dynamic start arguments (format: key=value) [multiple]                              \u2502\n\u2502 --help           Show this message and exit.                                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"documentation/cli/#hardpy-stop","title":"hardpy stop","text":"<p>The <code>hardpy stop</code> command is used to stop HardPy tests while the HardPy opener panel is running. By default, it stops tests in the current directory.</p> <pre><code> Usage: hardpy stop [OPTIONS] [TESTS_DIR]\n\n Stop HardPy tests.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502   tests_dir      [TESTS_DIR]  [default: None]                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"documentation/cli/#hardpy-status","title":"hardpy status","text":"<p>The <code>hardpy status</code> command is used to get HardPy tests launch status.</p> <pre><code> Usage: hardpy status [OPTIONS] [TESTS_DIR]\n\n Get HardPy test launch status.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502   tests_dir      [TESTS_DIR]  [default: None]                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"documentation/cli/#sc-login","title":"sc-login","text":"<p>The <code>hardpy sc-login</code> command is used to login in StandCloud.</p> <p>You can run the <code>hardpy sc-login &lt;stand_cloud_address&gt;</code> command, where <code>&lt;stand_cloud_address&gt;</code> is the StandCloud service address.</p> <pre><code> Usage: hardpy sc-login [OPTIONS] [TESTS_DIR]\n\n Login HardPy in StandCloud.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502   address   TEXT  [default: None] [required]                                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --check         --no-check           Check StandCloud connection. [default: no-check]                      \u2502\n\u2502 --help                               Show this message and exit.                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>To obtain this information, use:</p> <pre><code>hardpy sc-login --help\n</code></pre>"},{"location":"documentation/cli/#sc-logout","title":"sc-logout","text":"<p>The <code>hardpy sc-logout</code> command is used to logout from StandCloud.</p> <pre><code> Usage: hardpy sc-logout [OPTIONS]\n\n Logout HardPy from StandCloud.\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502   address   TEXT  [default: None] [required]                                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>hardpy sc-logout --help\n</code></pre>"},{"location":"documentation/database/","title":"Database","text":""},{"location":"documentation/database/#about","title":"About","text":"<p>We use CouchDB because it's a simple document-oriented NoSQL database. The database has two main purposes:</p> <ul> <li>Saving test report;</li> <li>Data synchronization with the web interface.</li> </ul> <p>The CouchDB version must be equal to or greater than the 3.2 version.</p>"},{"location":"documentation/database/#database-in-pytest-hardpy","title":"Database in pytest-hardpy","text":""},{"location":"documentation/database/#description-of-databases","title":"Description of databases","text":"<p>The pytest plugin has 2 databases: statestore and runstore.</p> <ul> <li>The statestore database uses for frontend data synchronization.</li> <li>The runstore database contains the document, which is a JSON object that stores the current state of the test run.</li> </ul> <p>A separate database is required to store the list of reports. The report database is used as an example of storing reports on past testing runs. It can be launched in the same instance as the statestore, runstore database, or in a different one. The database is accessed through the CouchdbLoader class, which can be called at the end of each launch. To read the current report, use the <code>get_current_report()</code> function.</p> <p>Sample code for saving a report at the end of testing:</p> <pre><code># conftest.py\ndef save_report_to_couchdb():\n    report = get_current_report()\n    if report:\n        loader = CouchdbLoader(CouchdbConfig())\n        loader.load(report)\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef fill_actions_after_test(post_run_functions: list):\n    post_run_functions.append(save_report_to_couchdb)\n    yield\n</code></pre> <p>HardPy users have the flexibility to choose their preferred database. They can write custom classes to record reports at the end of testing.</p>"},{"location":"documentation/database/#runstore-scheme","title":"Runstore scheme","text":"<p>The runstore and report databases have the same schema, as runstore stores the current report and report stores reports.</p> <p>The document of runstore database contains some section.</p>"},{"location":"documentation/database/#main","title":"main","text":"<ul> <li>_rev: a CouchDB revision MVCC token;   The variable is assigned automatically.</li> <li>_id: unique document identifier.   The variable is assigned automatically.</li> <li>stop_time: the end time of the test in Unix seconds. The variable is assigned automatically.</li> <li>start_time: the start time of the test in Unix seconds. The variable is assigned automatically.</li> <li>status: test execution status from pytest: passed, failed, skipped, stopped.   The variable is assigned automatically.</li> <li>name: the name of the test suite. It is displayed in the header of the operator panel.   The user can specify the name using the <code>tests_name</code> variable in the hardpy.toml file.   If this variable is not set, the name will be taken from the directory name containing the tests.</li> <li>test_stand: test stand information. See the test_stand section for more information.</li> <li>dut: DUT information. See the dut section for more information.</li> <li>process information about the testing process. See the process section for more information.</li> <li>modules: module (pytest files) information. See the modules section for more information.</li> <li>user: HardPy operator panel user name.   The variable is assigned by set_user_name function.   It can only be set once per test run.</li> <li>batch_serial_number: the serial number of the device batch.   The variable is assigned by set_batch_serial_number function.   It can only be set once per test run.</li> <li>caused_dut_failure_id: the ID of the first failed test that caused the test failure.   Format id: <code>module_name::case_name</code>. The variable is assigned automatically.</li> <li>error_code the error code (non-negative integer) of the caused DUT failure test.   The variable is assigned by ErrorCode class.</li> <li>artifact: an object that contains information about the artifacts created during the test run.   The user can specify the run artifact by using set_run_artifact function.   The artifact contains a dictionary where the user can store any data at the test run level.   The artifacts are not displayed on the operator panel.</li> </ul>"},{"location":"documentation/database/#test_stand","title":"test_stand","text":"<p>The test_stand section contains information about the test stand. It is a computer on which HardPy is running and to which the DUT test equipment is connected.</p> <ul> <li>name: test stand name. It can only be set once per test run.   The user can specify the stand name by using set_stand_name function.</li> <li>revision test stand revision. It can only be set once per test run.   The user can specify the stand revision by using set_stand_revision function.</li> <li>drivers: DEPRECATED, DO NOT USE IT.   Information about drivers in the form of a dictionary, including test equipment and test equipment software.</li> <li>instruments: list of information about the instruments (i.e. equipment) that form part of the test bench.   See the instrument section for more information.</li> <li>info: dictionary containing additional information about the test stand.   The user can specify the additional info by using set_stand_info function.</li> <li>timezone: timezone of test stand as a string. The variable is assigned automatically.</li> <li>location: the location of the test stand, e.g., the country, city, or laboratory number.   It can only be set once per test run. The user can specify the location by using   set_stand_location function.</li> <li>number: test stand number. Some stands may have the same name and   run on the same computer but have different numbers. It can only be set once per test run.   The user can specify the stand number by using set_stand_number function.</li> <li>hw_id: test stand machine id (GUID) or host name. The variable is assigned automatically by   the py-machineid package.</li> </ul>"},{"location":"documentation/database/#instrument","title":"instrument","text":"<p>Information about the instrument (i.e equipment) that form part of the test bench. Information about equipment such as power supplies and voltmeters, which may be used in the test stand, should be stored. The user can specify the instrument information by using set_instrument function.</p> <ul> <li>name: instrument name.</li> <li>revision instrument revision.</li> <li>serial_number instrument serial number.</li> <li>part_number instrument part number.</li> <li>number: instrument number.</li> <li>comment: comment on the instrument.</li> <li>info: dictionary containing additional information about the instrument.</li> </ul> <p>Power supply information:</p> <pre><code>// power supply info\n{\n  \"name\": \"Everypin Power Supply\",\n  \"revision\": \"1.1.0\",\n  \"serial_number\": \"4235098\",\n  \"part_number\": \"E012\",\n  \"number\": 1,\n  \"comment\": \"\",\n  \"info\": {\n    \"sw_version\": \"1.1.3\"\n  },\n}\n</code></pre>"},{"location":"documentation/database/#test_stand-examples","title":"test_stand examples","text":"<p>Two stands on one computer:</p> <pre><code>// test stand 1\n\"test_stand\": {\n  \"hw_id\": \"840982098ca2459a7b22cc608eff65d4\",\n  \"name\": \"Test stand A\",\n  \"revision\": \"1.0.0\",\n  \"timezone\": \"Europe/Helsinki\",\n  \"drivers\": {},\n  \"location\": \"Helsinki\",\n  \"number\": 1,\n  \"instruments\": [\n    {\n      \"name\": \"Everypin Power Supply\",\n      \"revision\": \"1.1.0\",\n      \"serial_number\": \"1238\",\n      \"part_number\": \"EPS_08\",\n      \"number\": 1,\n      \"comment\": \"\",\n      \"info\": {\n        \"sw_version\": \"1.1.3\"\n      }\n    }\n  ],\n  \"info\": {}\n},\n</code></pre> <pre><code>// test stand 2\n\"test_stand\": {\n  \"hw_id\": \"840982098ca2459a7b22cc608eff65d4\",\n  \"name\": \"Test stand A\",\n  \"revision\": \"1.0.1\",\n  \"timezone\": \"Europe/Helsinki\",\n  \"drivers\": {},\n  \"location\": \"Helsinki\",\n  \"number\": 2,\n  \"instruments\": [\n    {\n      \"name\": \"Everypin Power Supply\",\n      \"revision\": \"1.1.0\",\n      \"serial_number\": \"1237\",\n      \"part_number\": \"EPS_08\",\n      \"number\": 1,\n      \"comment\": \"\",\n      \"info\": {\n        \"sw_version\": \"1.1.3\"\n      }\n    }\n  ],\n  \"info\": {},\n\n},\n</code></pre> <p>Two different stands:</p> <pre><code>// test stand 1\n\"test_stand\": {\n  \"hw_id\": \"840982098ca2459a7b22cc608eff65d4\",\n  \"name\": \"ABC\",\n  \"revision\": \"1.0.0\",\n  \"timezone\": \"Europe/Helsinki\",\n  \"drivers\": {},\n  \"location\": \"Laboratory 1\",\n  \"number\": null,\n  \"instruments\": [],\n  \"info\": {},\n},\n</code></pre> <pre><code>// test stand 2\n\"test_stand\": {\n  \"hw_id\": \"156731093ab759a7b11ac108eaf69d2\",\n  \"name\": \"DEF\",\n  \"revision\": \"2.1\",\n  \"timezone\": \"Europe/Helsinki\",\n  \"drivers\": {},\n  \"location\": \"Laboratory 2\",\n  \"number\": null,\n  \"instruments\": [],\n  \"info\": {}\n},\n</code></pre>"},{"location":"documentation/database/#process","title":"process","text":"<p>Information about the testing process. For example, the device undergoes several stages of testing during the manufacturing process, including acceptance testing, firmware testing and functional testing of the board both with and without the case. Each stage may have its own name, number and other attributes.</p> <ul> <li>name: process name. It can only be set once per test run.   The user can specify the process name by using set_process_name function.</li> <li>number: process number. It can only be set once per test run.   The user can specify the process number by using set_process_number function.</li> <li>info: dictionary containing additional information about the process.   The user can specify the additional info by using set_process_info function.</li> </ul>"},{"location":"documentation/database/#dut","title":"dut","text":"<p>The device under test section contains information about the DUT.</p> <ul> <li>name: human-readable name of the DUT.   The user can specify the DUT name by using set_dut_name function.   It can only be set once per test run.</li> <li>type: type of DUT, f.e \"PCBA\", \"Casing\", etc.   The user can specify the DUT type by using set_dut_type function.   It can only be set once per test run.</li> <li>serial_number: DUT serial number. This identifier is unique to the testing device or board.   It can only be set once per test run.   The user can specify the DUT serial number by using set_dut_serial_number function.</li> <li>part_number: DUT part number. This identifier of a particular part design, board or device.   It can only be set once per test run.   The user can specify the DUT part number by using set_dut_part_number function.</li> <li>revision: DUT revision. The user can specify the DUT revision by using   set_dut_revision function.   It can only be set once per test run.</li> <li>sub_units: list of sub units of main DUT. Each sub-unit has a similar structure to the DUT itself, but does not contain any other sub units.   The user can add the sub unit by using set_dut_sub_unit function.</li> <li>info: dictionary containing additional information about the the DUT, such as batch, board revision, etc.   The user can specify the additional info by using set_dut_info function.</li> </ul>"},{"location":"documentation/database/#dut-examples","title":"dut examples","text":"<p>Testing of two devices of the same type (same part number):</p> <pre><code>// dut 1\n\"dut\": {\n  \"name\": \"mainboard\",\n  \"type\": \"PCBA\",\n  \"serial_number\": \"1000-10\",\n  \"part_number\": \"ABC11\",\n  \"revision\": \"rev_1\",\n  \"sub_units\": [],\n  \"info\": {\n    \"sw_version\": \"2.2\"\n  }\n}\n</code></pre> <pre><code>// dut 2\n\"dut\": {\n  \"name\": \"mainboard\",\n  \"type\": \"PCBA\",\n  \"serial_number\": \"1000-11\",\n  \"part_number\": \"ABC11\",\n  \"revision\": \"rev_1\",\n  \"sub_units\": [],\n  \"info\": {\n    \"sw_version\": \"2.2\"\n  }\n}\n</code></pre>"},{"location":"documentation/database/#modules","title":"modules","text":"<p>The modules section contains the information about tests. Each module contains information from a single test file. The module's name is the same as the file's name.</p> <ul> <li>test_{module_name}: an object containing information about a specific module.   The <code>{module_name}</code> variable is assigned automatically.   Contains the following fields:</li> <li>status: module test execution status. The variable is assigned automatically.</li> <li>name: module name, by default the same as module_id.     The user can specify the module name by using module_name marker.</li> <li>start_time: start time of module testing in Unix seconds. The variable is assigned automatically.</li> <li>stop_time: end time of module testing in Unix seconds. The variable is assigned automatically.</li> <li>group: the group of module: Setup, Main or Teardown (Main by default).     The user can specify the module group by using module_group marker.</li> <li>artifact: an object that contains information about the artifacts created during the test module.     The user can specify the module artifact by using set_module_artifact function.     The artifact contains a dictionary where the user can store any data at the test module level.     The artifacts are not displayed on the operator panel.</li> <li>cases: an object that contains information about each test case within the module.<ul> <li>test_{case_name}: an object containing information about a specific case.   The <code>{case_name}</code> variable is assigned automatically.   Contains the following fields:</li> <li>status: test case execution status. The variable is assigned automatically.</li> <li>name: case name, by default the same as case_id.     The user can specify the case name by using case_name marker.</li> <li>start_time: start time of case testing in Unix seconds. The variable is assigned automatically.</li> <li>stop_time: end time of case testing in Unix seconds. The variable is assigned automatically.</li> <li>assertion_msg: assert or error message if the test case fails. The variable is assigned automatically.     However, the user can write their own message in case of an assertion, which will be written to this variable.     For example:     <pre><code>assert False, \"This is an example\"\n</code></pre>     The assertion_msg is displayed in the operator panel next to the test case in which it was called.</li> <li>msg: the log message is displayed in the operator panel next to the test case in which it was called.     The user can specify and update current message by using set_message function.</li> <li>group: the group of case: Setup, Main or Teardown (Main by default).     The user can specify the case group by using case_group marker.</li> <li>measurements: list of measurements.     See the measurements section for more information.</li> <li>artifact: an object that contains information about the artifacts created during the test case.     The user can specify the case artifact by using set_case_artifact function.     The artifact contains a dictionary where the user can store any data at the test case level.     The artifacts are not displayed on the operator panel.</li> </ul> </li> </ul>"},{"location":"documentation/database/#measurements","title":"Measurements","text":"<p>The measurements section contains the information about measurements. The user fills in the list of measurements for each case by  set_case_measurement function.</p>"},{"location":"documentation/database/#numeric-measurement","title":"Numeric measurement","text":"<p>A NumericMeasurement is a structured container for storing numerical measurements. For example, the measured voltage must fall within a specific range.</p> <ul> <li>type: <code>numeric</code> by default.</li> <li>value: numeric measure value.</li> <li>name: numeric measure name.</li> <li>unit: unit of numeric measure.</li> <li>operation: comparison operators of numeric measure.</li> <li>comparison_value value to compare against.</li> <li>lower_limit: lower limit for range operations.</li> <li>upper_limit upper limit for range operations.</li> <li>result the result of the measurement if the operation exists; otherwise, it is empty.   Filled in without user involvement.</li> </ul> <pre><code>// voltage measure\n{\n  \"type\": \"numeric\",\n  \"value\": \"3.57\",\n  \"name\": \"Main voltage\",\n  \"lower_limit\": \"3.45\",\n  \"upper_limit\": \"3.65\",\n  \"unit\": \"V\",\n  \"operation\": \"GTLT\",\n  \"result\": true\n}\n</code></pre> <pre><code>// temperature measure\n{\n  \"type\": \"numeric\",\n  \"value\": \"14\",\n}\n</code></pre>"},{"location":"documentation/database/#string-measurement","title":"String measurement","text":"<p>A StringMeasurement is a structured container for storing string measurements. For example, the firmware version comparison.</p> <ul> <li>type: <code>string</code> by default.</li> <li>value: string measure value.</li> <li>name: string measure name.</li> <li>operation: comparison operators of string measure.</li> <li>comparison_value value to compare against.</li> <li>casesensitive: case sensitivity, default is <code>True</code>.</li> <li>result the result of the measurement if the operation exists; otherwise, it is empty.   Filled in without user involvement.</li> </ul> <pre><code>{\n  \"type\": \"string\",\n  \"value\": \"3.1.2\",\n  \"operation\": \"EQ\",\n  \"casesensitive\": true,\n  \"result\": true\n}\n</code></pre>"},{"location":"documentation/database/#report-example","title":"Report example","text":"<p>Example of a document:</p> <pre><code>    {\n      \"_rev\": \"44867-3888ae85c19c428cc46685845953b483\",\n      \"_id\": \"current\",\n      \"stop_time\": 1695817266,\n      \"start_time\": 1695817263,\n      \"status\": \"failed\",\n      \"name\": \"hardpy-stand\",\n      \"user\": null,\n      \"batch_serial_number\": \"0613\",\n      \"caused_dut_failure\": \"test_1_a::test_minute_parity\",\n      \"error_code\": null,\n      \"dut\": {\n        \"name\": \"analogue\",\n        \"type\": \"PCBA\",\n        \"serial_number\": \"92c5a4bb-ecb0-42c5-89ac-e0caca0919fd\",\n        \"part_number\": \"0507\",\n        \"revision\": \"rev_1\",\n        \"sub_units\": [],\n        \"info\": {\n          \"sw_version\": \"3.2.0\"\n        }\n      },\n      \"test_stand\": {\n        \"hw_id\": \"840982098ca2459a7b22cc608eff65d4\",\n        \"name\": \"test_stand_1\",\n        \"revision\": \"1.0\",\n        \"timezone\": \"Europe/Belgrade\",\n        \"drivers\": {},\n        \"location\": \"Belgrade_1\",\n        \"number\": 2,\n        \"instruments\": [\n          {\n            \"name\": \"Everypin Power Supply\",\n            \"revision\": \"2.0\",\n            \"serial_number\": \"1238\",\n            \"part_number\": \"EPS_08\",\n            \"number\": 1,\n            \"comment\": \"\",\n            \"info\": {}\n          }\n        ],\n        \"info\": {}\n      },\n      \"process\": {\n        \"name\": \"acceptance\",\n        \"number\": 1,\n        \"info\": {}\n      },\n      \"artifact\": {},\n      \"modules\": {\n        \"test_1_a\": {\n          \"status\": \"failed\",\n          \"name\": \"Module 1\",\n          \"start_time\": 1695816884,\n          \"stop_time\": 1695817265,\n          \"group\": \"MAIN\",\n          \"artifact\": {},\n          \"cases\": {\n            \"test_dut_info\": {\n              \"status\": \"passed\",\n              \"name\": \"DUT info\",\n              \"start_time\": 1695817263,\n              \"stop_time\": 1695817264,\n              \"assertion_msg\": null,\n              \"measurements\": [],\n              \"msg\": null,\n              \"group\": \"MAIN\",\n              \"artifact\": {}\n            },\n            \"test_minute_parity\": {\n              \"status\": \"failed\",\n              \"name\": \"Test 1\",\n              \"start_time\": 1695817264,\n              \"stop_time\": 1695817264,\n              \"assertion_msg\": \"The test failed because minute 21 is odd! Try again!\",\n              \"measurements\": [],\n              \"msg\": [\n                \"Current minute 21\"\n              ],\n              \"group\": \"MAIN\",\n              \"artifact\": {\n                \"data_str\": \"123DATA\",\n                \"data_int\": 12345,\n                \"data_dict\": {\n                  \"test_key\": \"456DATA\"\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n</code></pre>"},{"location":"documentation/database/#couchdb-instance","title":"CouchDB instance","text":"<p>This section explains how to launch and manage a CouchDB instance. After launching the database, it becomes available at the following address:</p> <p>http://localhost:5984/_utils/</p> <p>The internal settings of the database are contained in the couchDB.ini configuration file. It contains settings that define the behavior and operating parameters of the database.</p>"},{"location":"documentation/database/#running-couchdb-with-docker-compose","title":"Running CouchDB with Docker Compose","text":"<p>An example configuration for running CouchDB via Docker Compose is located in the <code>example/database/couchdb</code> folder. A shortened version of the instructions is described below.</p> <ol> <li>Create a <code>docker</code> directory in the project's root directory.</li> <li>Create a <code>couchdb.ini</code> file in the <code>docker</code> directory.</li> </ol> <p><pre><code>[chttpd]\nenable_cors=true\n\n[cors]\norigins = *\nmethods = GET, PUT, POST, HEAD, DELETE\ncredentials = true\nheaders = accept, authorization, content-type, origin, referer, x-csrf-token\n</code></pre> 3. Create a <code>docker-compose.yaml</code> file in project's root directory.</p> <p><pre><code>services:\n  couchserver:\n    image: couchdb:3.4\n    ports:\n      - \"5984:5984\"\n    environment:\n      COUCHDB_USER: dev\n      COUCHDB_PASSWORD: dev\n    volumes:\n      - couchdb_data:/opt/couchdb/data\n      - ./database/couchdb.ini:/opt/couchdb/etc/local.ini\n\nvolumes:\n  couchdb_data:\n</code></pre> 4. Run docker compose in the root directory.</p> <p><pre><code>docker compose up\n</code></pre> 5. To stop the database, run the command:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"documentation/database/#running-couchdb-with-docker","title":"Running CouchDB with Docker","text":"<ol> <li>Create <code>couchdb.ini</code> file.</li> </ol> <p><pre><code>[chttpd]\nenable_cors=true\n\n[cors]\norigins = *\nmethods = GET, PUT, POST, HEAD, DELETE\ncredentials = true\nheaders = accept, authorization, content-type, origin, referer, x-csrf-token\n</code></pre> 2. The Docker version must be 24.0.0 or higher. Run the Docker container (from the folder with the couchdb.ini file):</p> <pre><code>docker run --rm --name couchdb -p 5984:5984 -e COUCHDB_USER=dev -e COUCHDB_PASSWORD=dev -v ./couchdb.ini:/opt/couchdb/etc/local.ini couchdb:3.4\n</code></pre> <p>Command for Windows:</p> <pre><code>docker run --rm --name couchdb -p 5984:5984 -e COUCHDB_USER=dev -e COUCHDB_PASSWORD=dev -v .\\couchdb.ini:/opt/couchdb/etc/local.ini couchdb:3.4\n</code></pre> <p>The container will be deleted after use.</p>"},{"location":"documentation/database/#running-couchdb-with-binary-packages-in-linux","title":"Running CouchDB with binary packages in Linux","text":"<ol> <li>Use this instruction to install CouchDB.</li> <li>The installer asks you if you want to install CouchDB as a standalone application or in a clustered configuration. Select <code>Standalone</code> and press Enter.</li> <li>You are prompted to enter the Erlang Node Name. You can ask it in Terminal with the command <code>hostname -f</code>.</li> <li>Set the Erlang Magic Cookie. This is a unique identifier, for example, <code>test1234</code>.</li> <li>Configure the network interfaces on which CouchDB will be bound <code>localhost</code> is fine.</li> <li>Enter an admin password of your choice for CouchDB, press <code>Enter</code>, re-type the password and press <code>Enter</code> again to continue the installation.</li> <li>After launching the database, it becomes available at the following address http://localhost:5984/_utils/. Open it.</li> <li>First of all, in the <code>User Management</code> section in the <code>Create Admins</code> tab, create a user with the login <code>dev</code> and password <code>dev</code>.</li> <li>In the <code>Config</code> choose <code>CORS</code> and appoint <code>Enable CORS</code> with <code>All domains</code>.</li> </ol>"},{"location":"documentation/database/#to-disable-the-couchdb-service","title":"To disable the CouchDB service:","text":"<p>Remove packages: <pre><code>sudo apt remove --purge couchdb\n</code></pre> Remove GPG keys and repository: <pre><code>sudo rm /usr/share/keyrings/couchdb-archive-keyring.gpg\nsudo rm /etc/apt/sources.list.d/couchdb.list\n</code></pre> Clean APT cache: <pre><code>sudo apt clean\n</code></pre> Disable service: <pre><code>systemctl stop couchdb.service\nsystemctl disable couchdb.service\nsystemctl daemon-reload\nsystemctl reset-failed\n</code></pre></p>"},{"location":"documentation/database/#running-couchdb-with-binary-packages-in-windows","title":"Running CouchDB with binary packages in Windows","text":"<ol> <li>Use this instruction to install CouchDB.</li> <li>Be sure to install CouchDB to a path with no spaces, such as <code>C:\\CouchDB</code>.</li> <li>Create a user with the login <code>dev</code> and password <code>dev</code> during the installation steps. Validate Credentials.</li> <li>Generate Random Cookie.</li> <li>After launching the database, it becomes available at the following address http://localhost:5984/_utils/. Open it.</li> <li>In the <code>Config</code> choose <code>CORS</code> and appoint <code>Enable CORS</code> with <code>All domains</code>.</li> </ol>"},{"location":"documentation/hardpy_config/","title":"HardPy config","text":"<p>HardPy uses the <code>hardpy.toml</code> file for configuration. The user can change the fields at creation by using hardpy init.</p> Note <p>All HardPy project must have <code>hardpy.toml</code> file.</p>"},{"location":"documentation/hardpy_config/#minimal-configuration-file","title":"Minimal configuration file","text":"<pre><code>title = \"HardPy TOML config\"\n\n[database]\nuser = \"dev\"\npassword = \"dev\"\nhost = \"localhost\"\nport = 5984\n\n[frontend]\nhost = \"localhost\"\nport = 8000\n</code></pre>"},{"location":"documentation/hardpy_config/#full-configuration-file","title":"Full configuration file","text":"<pre><code>title = \"HardPy TOML config\"\ntests_name = \"My tests\"\ncurrent_test_config = \"\"\n\n[database]\nstorage_type = \"couchdb\"\nuser = \"dev\"\npassword = \"dev\"\nhost = \"localhost\"\nport = 5984\n\n[frontend]\nhost = \"localhost\"\nport = 8000\nlanguage = \"en\"\nfull_size_button = false\nsound_on = false\nmeasurement_display = true\nmanual_collect = false\n\n[frontend.modal_result]\nenable = false\nauto_dismiss_pass = true\nauto_dismiss_timeout = 5\n\n[stand_cloud]\naddress = \"standcloud.io\"\nconnection_only = true\nautosync = true\nautosync_timeout = 30\napi-key = \"1234567890\"\n</code></pre>"},{"location":"documentation/hardpy_config/#configuration-fields-description","title":"Configuration fields description","text":""},{"location":"documentation/hardpy_config/#common","title":"common","text":"<p>Common settings.</p>"},{"location":"documentation/hardpy_config/#title","title":"title","text":"<p>Configuration file header. The value is always <code>HardPy TOML config</code>.</p>"},{"location":"documentation/hardpy_config/#tests_name","title":"tests_name","text":"<p>Tests name. The user can change this value with the <code>hardpy init --tests-name</code> argument.</p>"},{"location":"documentation/hardpy_config/#current_test_config","title":"current_test_config","text":"<p>Tests file configuration name.  An example of its use can be found on page Multiple configs.</p>"},{"location":"documentation/hardpy_config/#database","title":"database","text":"<p>Database settings.</p>"},{"location":"documentation/hardpy_config/#storage_type","title":"storage_type","text":"<p>Storage type. The default is <code>couchdb</code>.</p> <ul> <li><code>couchdb</code>: Stores test results and measurements in a CouchDB database. Requires a running CouchDB instance.</li> <li><code>json</code>: Stores test results and measurements in local JSON files. No external database required. </li> </ul> <p>Files are stored in the <code>.hardpy</code> directory in the root of the project. The user can change this value with the <code>hardpy init --storage-type</code> option.</p>"},{"location":"documentation/hardpy_config/#storage_path","title":"storage_path","text":"<p>Path to the storage directory. The default is <code>.hardpy</code> in the root of the project. The user can change this value in the hardpy.toml file using the <code>storage_path</code> option.  Both relative and absolute paths are supported.</p> <pre><code>[database]\nstorage_type = \"json\"\nstorage_path = \"result\"\n</code></pre>"},{"location":"documentation/hardpy_config/#user","title":"user","text":"<p>Database user name. The default is <code>dev</code>. The user can change this value with the <code>hardpy init --database-user</code> option.</p>"},{"location":"documentation/hardpy_config/#password","title":"password","text":"<p>Database password. The default is <code>dev</code>. The user can change this value with the <code>hardpy init --database-password</code> option.</p>"},{"location":"documentation/hardpy_config/#host","title":"host","text":"<p>Database host name. The default is <code>localhost</code>. The user can change this value with the <code>hardpy init --database-host</code> option.</p>"},{"location":"documentation/hardpy_config/#port","title":"port","text":"<p>Database port number. The default is <code>5984</code>. The user can change this value with the <code>hardpy init --database-port</code> option.</p>"},{"location":"documentation/hardpy_config/#frontend","title":"frontend","text":"<p>Frontend (operator panel) settings.</p>"},{"location":"documentation/hardpy_config/#host_1","title":"host","text":"<p>Operator panel host name. The default is <code>localhost</code>. The user can change this value with the <code>hardpy init --frontend-host</code> option.</p>"},{"location":"documentation/hardpy_config/#port_1","title":"port","text":"<p>Operator panel port number. The default is <code>8000</code>. The user can change this value with the <code>hardpy init --frontend-port</code> option.</p>"},{"location":"documentation/hardpy_config/#language","title":"language","text":"<p>Language of operator panel. The default is <code>en</code>. Available languages are there.</p>"},{"location":"documentation/hardpy_config/#full_size_button","title":"full_size_button","text":"<p>Enable full-size start/stop button layout in operator panel. When set to <code>true</code>, the button will be displayed in full-size layout with larger dimensions and centered positioning. Default is <code>false</code>.</p>"},{"location":"documentation/hardpy_config/#sound_on","title":"sound_on","text":"<p>Enable or disable test completion sound notifications. When set to <code>true</code>, sound will play when test execution completes (PASS/FAIL/STOP status). Default is <code>false</code>.</p>"},{"location":"documentation/hardpy_config/#measurement_display","title":"measurement_display","text":"<p>Enable or disable measurement display in the operator panel. When set to <code>true</code>, measurements created using <code>set_case_measurement</code> will be displayed as tags  showing name, value, and unit information. Default is <code>true</code>.</p>"},{"location":"documentation/hardpy_config/#manual_collect","title":"manual_collect","text":"<p>Enable or disable manual test collection mode in the operator panel. When set to <code>true</code>, users can selectively choose which tests to run by checking individual test cases, and only the selected tests will be executed when starting the test run. When set to <code>false</code>, all discovered tests will run automatically as before. Default is <code>false</code>.</p>"},{"location":"documentation/hardpy_config/#modal_result","title":"modal_result","text":"<p>Modal result windows settings for test completion display.</p> <pre><code>[frontend.modal_result]\nenable = false\nauto_dismiss_pass = true\nauto_dismiss_timeout = 5\n</code></pre>"},{"location":"documentation/hardpy_config/#enable","title":"enable","text":"<p>Enable or disable test completion modal result windows. When set to <code>true</code>, modal windows will display test completion status (PASS/FAIL/STOP) at the end of test execution. Default is <code>false</code>.</p>"},{"location":"documentation/hardpy_config/#auto_dismiss_pass","title":"auto_dismiss_pass","text":"<p>Automatically dismiss PASS result modals after the timeout period. When set to <code>true</code>, PASS results will automatically close without requiring user interaction. FAIL and STOP results always require manual dismissal. Default is <code>true</code>.</p>"},{"location":"documentation/hardpy_config/#auto_dismiss_timeout","title":"auto_dismiss_timeout","text":"<p>Timeout in seconds for auto-dismissing PASS result modals. Applies only when <code>auto_dismiss_pass = true</code>. Default is <code>5</code> seconds.</p>"},{"location":"documentation/hardpy_config/#stand_cloud","title":"stand_cloud","text":"<p>StandCloud settings.</p> <pre><code>[stand_cloud]\naddress = \"demo.standcloud.localhost\"\nconnection_only = true\nautosync = true\nautosync_timeout = 30\napi-key = \"1234567890\"\n</code></pre>"},{"location":"documentation/hardpy_config/#address","title":"address","text":"<p>StandCloud service address. To obtain one, contact info@everypin.io.</p>"},{"location":"documentation/hardpy_config/#connection_only","title":"connection_only","text":"<p>Boolean variable, if set to <code>true</code>, HardPy will check the connection to the StandCloud service at each startup before running tests. The default value is <code>false</code>.</p> <p>If the connection fails, the tests will not run.</p>"},{"location":"documentation/hardpy_config/#autosync","title":"autosync","text":"<p>Boolean variable, if set to <code>true</code>, HardPy will automatically send data  to StandCloud upon completion of testing.</p> <p>The default value is <code>false</code>.</p>"},{"location":"documentation/hardpy_config/#autosync_timeout","title":"autosync_timeout","text":"<p>This is an integer variable greater than 1, representing the time interval  in minutes between StandCloud synchronization attempts.</p> <p>The default value is <code>30</code>.</p>"},{"location":"documentation/hardpy_config/#api-key","title":"api-key","text":"<p>StandCloud API key.</p>"},{"location":"documentation/hardpy_config/#test_configs","title":"test_configs","text":"<p>Test configurations describes using the <code>[[test_configs]]</code> table array. </p>"},{"location":"documentation/hardpy_config/#name","title":"name","text":"<p>A user-friendly name for the configuration.</p>"},{"location":"documentation/hardpy_config/#file","title":"file","text":"<p>The path to the <code>pytest.ini</code> file that defines the specific pytest arguments and test selection for this configuration.</p>"},{"location":"documentation/hardpy_config/#description","title":"description","text":"<p>An optional field to provide a more detailed explanation of the configuration.</p>"},{"location":"documentation/hardpy_config/#extra-args","title":"extra args","text":"<p>Users can add their own fields to the hardpy.toml file and use them in test plans.  Here is an example of how to use them:</p> <pre><code>title = \"HardPy TOML config\"\n\n[database]\nuser = \"dev\"\npassword = \"dev\"\nhost = \"localhost\"\nport = 5984\n\n[frontend]\nhost = \"localhost\"\nport = 8000\n\n[extra_args]\nextra_arg_1 = \"extra_arg_1\"\nextra_arg_2 = 2\n</code></pre> <pre><code>import hardpy\n\nconfig = hardpy.get_hardpy_config()\nprint(config.extra_args.[\"extra_arg_1\"])\nprint(config.extra_args.[\"extra_arg_2\"])\n</code></pre>"},{"location":"documentation/hardpy_panel/","title":"HardPy panel","text":"<p>The hardpy panel or operator panel is a web interface that displays and controls the testing process in HardPy.</p>"},{"location":"documentation/hardpy_panel/#capability","title":"Capability","text":"<p>HardPy panel allows you to:</p> <ul> <li>Start and stop testing;</li> <li>Interact with dialog box during testing;</li> <li>Browse:<ul> <li>Test suite name.</li> <li>Last test run status.</li> <li>Test module name.</li> <li>Duration of test modules execution.</li> <li>Test module status.</li> <li>Test case name.</li> <li>Test case message.</li> <li>Test case status.</li> </ul> </li> <li>Browse current statestore state in debug mode.</li> </ul>"},{"location":"documentation/hardpy_panel/#languages","title":"Languages","text":"<p>You can set one of the following operator panel languages \u200b\u200bvia the configuration file:</p> <ul> <li>English (\"en\")</li> <li>German (\"de\")</li> <li>French (\"fr\")</li> <li>Spanish (\"es\")</li> <li>Chinese (\"zh\")</li> <li>Japanese (\"ja\")</li> <li>Russian (\"ru\")</li> <li>Czech (\"cs\")</li> </ul>"},{"location":"documentation/hardpy_panel/#usage","title":"Usage","text":""},{"location":"documentation/hardpy_panel/#launch-operator-panel","title":"Launch operator panel","text":"<p>Use the hardpy run command to start the web server. After this open page http://localhost:8000/ in the browser.</p> <p>When the operator panel is running, you can run tests through the web interface or through the pytest launcher (in a terminal or from another application).</p>"},{"location":"documentation/hardpy_panel/#start-and-stop-tests","title":"Start and stop tests","text":"<p>The operator panel contains a test start/stop button in the lower right corner of the screen. The user can start/stop tests using the space key.</p>"},{"location":"documentation/hardpy_panel/#operator-panel-bar","title":"Operator panel bar","text":"<p>Operator panel bar displays key system status information in a compact tag-based format.</p> <p>Fields:</p> <ul> <li>Stand name - name of the test stand;</li> <li>Status - current system status;</li> <li>Start time - test/operation start timestamp with timezone;</li> <li>Finish time - completion timestamp with timezone;</li> <li>Alert - active warning or error message;</li> <li>Test stand info - additional test stand parameters   from set_dut_info info;</li> </ul> <p>All fields appear as minimal tags and only display when data is available.</p>"},{"location":"documentation/hardpy_panel/#settings","title":"Settings","text":"<p>The operator panel contains a setting button in the top right corner.</p>"},{"location":"documentation/hardpy_panel/#debug-mode","title":"debug mode","text":"<p>The user can view the statestore database online by clicking on the Turn on the debug mode button.</p> <p>Debug mode is disabled by default.</p>"},{"location":"documentation/hardpy_panel/#sound","title":"sound","text":"<p>The user can turn on the sound of the end of the test by clicking on the Turn on the sound button.</p> <p>Sound can also be enabled by setting <code>sound_on = true</code> in the frontend configuration section. Sound is disabled by default.</p>"},{"location":"documentation/hardpy_panel/#full-size-startstop-button","title":"Full-size start/stop button","text":"<p>The operator panel supports a full-size start/stop button layout for improved usability in various scenarios.  This feature is particularly useful for:</p> <ul> <li>Touchscreen interfaces</li> <li>Mobile devices</li> <li>Situations requiring prominent control elements</li> </ul> <p>Configuration: Enable the full-size button in your <code>hardpy.toml</code>:</p> <pre><code>[frontend]\nfull_size_button = true\n</code></pre> Desktop layoutMobile horizontalMobile vertical <p> </p> <p> </p> <p> </p>"},{"location":"documentation/hardpy_panel/#manual-test-selection","title":"Manual test selection","text":"<p>The operator panel supports manual test selection, allowing operators to choose specific test cases  to run instead of executing the entire test plan.</p> <p>Features:</p> <ul> <li>Checkbox Interface: Each test case displays a checkbox for selection in the test suite view.</li> <li>Bulk Selection: Checkboxes at the module level allow selecting/deselecting all tests in a module. </li> <li>Visual Indicators: Selected tests are displayed normally, while non-selected tests are visually muted and marked as <code>Skipped</code>.</li> </ul> <p>Usage:</p> <ol> <li>Enable manual collect mode in your <code>hardpy.toml</code>:</li> </ol> <pre><code>[frontend]\nmanual_collect = true\n</code></pre> <ol> <li>When the operator panel loads, enable Manual collect on in the settings menu - checkboxes will appear next to each test case and module</li> <li>Select the desired tests by checking the corresponding checkboxes</li> <li>Disable manual collect mode by selecting Manual collect off in the settings menu</li> <li>Click the start button to run only the selected tests (non-selected tests will be skipped)</li> </ol> <p>Note: When manual collect mode is disabled (default), all tests are selected and the checkboxes are hidden.</p>"},{"location":"documentation/hardpy_panel/#dialog-box","title":"Dialog box","text":"<p>For user interaction with the test, it is possible to use dialog boxes. An example of usage can be seen in the example dialog box and in dialog box documentation. Currently, there are some types of dialog boxes.</p> <p>Each dialog box can contain an image, HTML page or pass/fail buttons.</p> <ul> <li>Allows the width to be changed using the <code>width</code> parameter.</li> <li>Allows changing the border thickness with the <code>border</code> parameter.</li> <li>Allows the following image types: gif, jpeg, pjpeg, png, svg+xml, tiff, vnd.microsoft.icon, vnd.wap.wbmp, webp.</li> <li>Allows to use one or two buttons (<code>pass/fail</code> button).</li> <li>Allows to set custom button names via the <code>button_text</code> parameter.</li> </ul>"},{"location":"documentation/hardpy_panel/#basic-dialog-box","title":"basic dialog box","text":"<p>Contains an instruction or question and a <code>confirm</code> button for confirmation.</p> WidgetWidget with imageWidget with HTML with linkWidget with HTML with raw codeWidget with pass/fail buttonsWidget with custom pass/fail buttons <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"documentation/hardpy_panel/#text-input-field","title":"text input field","text":"<p>Contains an instruction or question, a text input field, and a <code>confirm</code> button for confirmation. The text is transmitted in UTF-8 encoding.</p> WidgetWidget with imageWidget with HTMLWidget with pass/fail buttons <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"documentation/hardpy_panel/#number-input-field","title":"number input field","text":"<p>Contains an instruction or question, a number input field, and a <code>confirm</code> button for confirmation.</p> <ul> <li>Allows float numbers with a dot separator.</li> <li>Allows negative numbers.</li> <li>Allows numbers to be entered using E notation with <code>e</code>, e.g. <code>2e3</code>.</li> <li>The entered numbers will be converted to float.</li> </ul> WidgetWidget with imageWidget with htmlWidget with pass/fail buttons <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"documentation/hardpy_panel/#radiobutton","title":"radiobutton","text":"<p>Contains radiobutton widget.</p> <ul> <li>The user selects one option from several possible ones.</li> <li>Returns the contents of the selected item as a string.</li> </ul> WidgetWidget with imageWidget with htmlWidget with pass/fail buttons <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"documentation/hardpy_panel/#checkbox","title":"checkbox","text":"<p>Contains checkbox widget.</p> <ul> <li>The user selects several options from several possible ones.</li> <li>Returns a list with the contents of the selected items.</li> </ul> WidgetWidget with imageWidget with htmlWidget with pass/fail buttons <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"documentation/hardpy_panel/#multiple-steps","title":"multiple steps","text":"<p>Contains an instruction with multiple steps and <code>confirm</code> button for confirmation. Allows steps with text and image.</p> Step 1Step 2Step 3Step with pass/fail buttons <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"documentation/hardpy_panel/#operator-message","title":"Operator message","text":"<p>The messages to the operator are similar to dialog boxes, but do not contain a Confirm button and can be called outside the execution of the test plan, for example in case of exception catching in the <code>conftest.py</code> file. For more information, see the example operator message or in the function description set_operator_message.</p>"},{"location":"documentation/hardpy_panel/#warning-window","title":"warning window","text":"<p>If the user clicks <code>confirm</code> without entering anything, a warning window will be displayed.</p>"},{"location":"documentation/hardpy_panel/#error-notification","title":"error notification","text":"<p>If the user closes the dialog box (using the cross in the upper right corner), the tests will be stopped, an error message will be displayed.</p>"},{"location":"documentation/hardpy_panel/#charts","title":"Charts","text":"<p>For visualizing test data, it is possible to use interactive charts. Charts provide a graphical representation of measurement results and allow operators to analyze data trends in real-time.</p> <p>Charts support the following features:</p> <ul> <li>Multiple datasets on a single chart</li> <li>Zooming capabilities</li> <li>Logarithmic scales for both X and Y axes</li> <li>Collapsible/expandable view</li> </ul>"},{"location":"documentation/hardpy_panel/#basic-line-chart","title":"Basic line chart","text":"<p>Displays a single dataset as a line chart with customizable styling.</p>"},{"location":"documentation/hardpy_panel/#multiple-datasets","title":"Multiple datasets","text":"<p>Shows multiple datasets on the same chart for comparative analysis.  Each dataset can have different colors and styles.</p>"},{"location":"documentation/hardpy_panel/#collapsed-view","title":"Collapsed view","text":"<p>Charts can be collapsed to save screen space when not actively being analyzed.</p>"},{"location":"documentation/hardpy_panel/#expanded-view","title":"Expanded view","text":"<p>Charts can be expanded to full view for detailed analysis.</p>"},{"location":"documentation/hardpy_panel/#logarithmic-x-axis","title":"Logarithmic X-axis","text":"<p>When dealing with data that spans multiple orders of magnitude, the X-axis can be set to logarithmic scale.</p>"},{"location":"documentation/hardpy_panel/#logarithmic-y-axis","title":"Logarithmic Y-axis","text":"<p>The Y-axis can be set to logarithmic scale for better visualization of exponential data trends.</p>"},{"location":"documentation/hardpy_panel/#logarithmic-x-and-y-axes","title":"Logarithmic X and Y axes","text":"<p>Both axes can be set to logarithmic scale for data that requires logarithmic representation in both dimensions.</p>"},{"location":"documentation/hardpy_panel/#chart-interaction","title":"Chart interaction","text":"<ul> <li>Zoom: Click and drag to select an area to zoom into</li> <li>Pan: Click and drag to move around the chart when zoomed in</li> <li>Reset zoom: Double-click to reset to the original view</li> <li>Data points: Hover over data points to see exact values</li> <li>Legend: Click on legend items to show/hide specific datasets</li> <li>Download: Downloading plot as a PNG.</li> </ul>"},{"location":"documentation/hardpy_panel/#test-completion-modal-results","title":"Test completion modal results","text":"<p>The operator panel now displays comprehensive test completion results through modal windows that provide immediate visual feedback about test outcomes.</p> <p>Modal types:</p> <ul> <li>PASS Modal: Green background with auto-dismiss countdown timer</li> <li>FAIL Modal: Red background with scrollable list of failed test cases including module names, test case names, and assertion messages</li> <li>STOP Modal: Yellow background showing the stopped test case details</li> </ul> <p>User interaction:</p> <ul> <li>Auto-dismiss: PASS modals automatically close after a configurable timeout (default: 5 seconds)</li> <li>Manual dismissal: Click anywhere on the modal or press any key to close</li> <li>Space key protection: Prevents accidental test start/stop during modal display</li> <li>Cooldown period: Brief delay after modal dismissal before space key actions are enabled</li> </ul> <p>Configuration: Enable and customize modal results in your <code>hardpy.toml</code>:</p> <pre><code>[frontend.modal_result]\nenable = true    \nauto_dismiss_pass = true\nauto_dismiss_timeout = 15\n</code></pre> PASS modal resultFAIL modal resultSTOP modal result <p> </p> <p> </p> <p> </p>"},{"location":"documentation/pytest_hardpy/","title":"pytest-hardpy","text":"<p>pytest-hardpy is a pytest plugin that helps you save test data in a database for test reporting and viewing test data in the web interface.</p>"},{"location":"documentation/pytest_hardpy/#plugin-registration","title":"Plugin registration","text":"<p>To use the pytest-hardpy you need to enable it. You can do this via the <code>pytest.ini</code> file.</p> <p>Example:</p> <pre><code># pytest.ini\n[pytest]\naddopts = --hardpy-pt\n</code></pre> <p>Another way to enable a plugin without <code>pytest.ini</code> file is to run tests with the option <code>--hardpy-pt</code>.</p> <pre><code>pytest --hardpy-pt tests\n</code></pre> <p>If tests are run via hardpy panel, then the pytest-hardpy plugin will be enabled for tests by default.</p>"},{"location":"documentation/pytest_hardpy/#functions","title":"Functions","text":""},{"location":"documentation/pytest_hardpy/#set_user_name","title":"set_user_name","text":"<p>Writes a string with a HardPy operator panel user name. When called again, the exception <code>DuplicateParameterError</code> will be raised.</p> <p>Arguments:</p> <ul> <li><code>name</code> (str): User name</li> </ul> <p>Example:</p> <pre><code>def test_user_name():\n    set_user_name(\"test_operator\")\n    with pytest.raises(DuplicateParameterError):\n        set_user_name(\"another_operator\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_batch_serial_number","title":"set_batch_serial_number","text":"<p>Writes a string with the serial number of the device batch. When called again, the exception <code>DuplicateParameterError</code> will be raised.</p> <p>Arguments:</p> <ul> <li><code>serial_number</code> (str): Batch serial number</li> </ul> <p>Example:</p> <pre><code>def test_batch_number():\n    set_batch_serial_number(\"BATCH-001\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_dut_info","title":"set_dut_info","text":"<p>This function records a dictionary containing information about the test stand. When called again, the information will be added to DB.</p> <p>Arguments:</p> <ul> <li><code>info</code> (Mapping[str, str | int | float  | None]): DUT info</li> </ul> <p>Example:</p> <pre><code>def test_dut_info():\n    set_dut_info({\"sw_version\": \"1.0.0\"})\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_dut_serial_number","title":"set_dut_serial_number","text":"<p>Writes a string with a serial number. When called again, the exception <code>DuplicateParameterError</code> will be caused.</p> <p>Arguments:</p> <ul> <li><code>serial_number</code> (str): DUT serial number</li> </ul> <p>Example:</p> <pre><code>def test_serial_number():\n    set_dut_serial_number(\"1234\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_dut_part_number","title":"set_dut_part_number","text":"<p>Writes a string with a part number. When called again, the exception <code>DuplicateParameterError</code> will be caused.</p> <p>Arguments:</p> <ul> <li><code>part_number</code> (str): DUT part number</li> </ul> <p>Example:</p> <pre><code>def test_part_number():\n    set_dut_part_number(\"part_1\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_dut_name","title":"set_dut_name","text":"<p>Writes a string with a human-readable name of the DUT. When called again, the exception <code>DuplicateParameterError</code> will be raised.</p> <p>Arguments:</p> <ul> <li><code>name</code> (str): DUT name</li> </ul> <p>Example:</p> <pre><code>def test_dut_name():\n    set_dut_name(\"Test Device\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_dut_type","title":"set_dut_type","text":"<p>Writes a string with a type of DUT, f.e \"PCBA\", \"Casing\", etc. When called again, the exception <code>DuplicateParameterError</code> will be raised.</p> <p>Arguments:</p> <ul> <li><code>dut_type</code> (str): DUT type</li> </ul> <p>Example:</p> <pre><code>def test_dut_type():\n    set_dut_type(\"PCBA\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_dut_revision","title":"set_dut_revision","text":"<p>Writes a string with a DUT revision. When called again, the exception <code>DuplicateParameterError</code> will be raised.</p> <p>Arguments:</p> <ul> <li><code>revision</code> (str): DUT revision (e.g. \"REV1.0\")</li> </ul> <p>Example:</p> <pre><code>def test_dut_revision():\n    set_dut_revision(\"HW1.0\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_dut_sub_unit","title":"set_dut_sub_unit","text":"<p>Writes a sub unit of DUT. It accepts a SubUnit object containing all the details of the sub unit.</p> <p>Arguments:</p> <ul> <li><code>sub_unit</code> (SubUnit): SubUnit object</li> </ul> <p>Returns:</p> <ul> <li>(int): sub unit index</li> </ul> <p>Example:</p> <pre><code>def test_dut_sub_unit():\n    sub_unit = SubUnit(\n        serial_number=\"12345\",\n        part_number=\"part_number_1\",\n        name=\"Test Device\",\n        type=\"PCBA\",\n        revision=\"REV1.0\",\n        info={\"sw_version\": \"1.0\"},\n    )\n    hardpy.set_dut_sub_unit(sub_unit)\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_dut_revision_1","title":"set_dut_revision","text":"<p>Writes a string with a DUT revision. When called again, the exception <code>DuplicateParameterError</code> will be raised.</p> <p>Arguments:</p> <ul> <li><code>revision</code> (str): DUT revision (e.g. \"REV1.0\")</li> </ul> <p>Example:</p> <pre><code>def test_dut_revision():\n    set_dut_revision(\"HW1.0\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_stand_name","title":"set_stand_name","text":"<p>Writes a string with a test stand name. When called again, the exception <code>DuplicateParameterError</code> will be caused.</p> <p>Arguments:</p> <ul> <li><code>name</code> (str): test stand name</li> </ul> <p>Example:</p> <pre><code>def test_stand_name():\n    set_stand_name(\"name 1\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_stand_info","title":"set_stand_info","text":"<p>Writes a dictionary with information about the test stand. When called again, the information will be merged with existing data.</p> <p>Arguments:</p> <ul> <li><code>info</code> (Mapping[str, str | int | float ]): Test stand info dictionary</li> </ul> <p>Example:</p> <pre><code>def test_stand_info():\n    set_stand_info({\"calibration_date\": \"2023-01-15\"})\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_stand_location","title":"set_stand_location","text":"<p>Writes a string with a test stand location. When called again, the exception <code>DuplicateParameterError</code> will be caused.</p> <p>Example:</p> <pre><code>def test_stand_info():\n    set_stand_location(\"Moon\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_stand_number","title":"set_stand_number","text":"<p>Writes a integer number with a test stand number. When called again, the exception <code>DuplicateParameterError</code> will be caused. When called with negative or non-integer number, the exception <code>TestStandNumberError</code> will be caused.</p> <p>Arguments:</p> <ul> <li><code>number</code> (int): test stand number</li> </ul> <p>Example:</p> <pre><code>def test_stand_number():\n    set_stand_number(3)\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_stand_revision","title":"set_stand_revision","text":"<p>Writes a string with a test stand revision. When called again, the exception <code>DuplicateParameterError</code> will be raised.</p> <p>Arguments:</p> <ul> <li><code>revision</code> (str): Test stand revision</li> </ul> <p>Example:</p> <pre><code>def test_stand_revision():\n    set_stand_revision(\"HW1.0\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_driver_info","title":"set_driver_info","text":"<p>DEPRECATED, DO NOT USE IT.</p> <p>The function records a dictionary containing information about the test stand driver. The data is updated with new information each time the function is called.</p> <p>Driver data is stored in both statestore and runstore databases.</p> <p>Arguments:</p> <ul> <li><code>drivers</code> (dict): A dictionary of drivers, where keys are driver names and values are driver-specific data.</li> </ul> <p>Example:</p> <pre><code>def test_driver_info():\n    drivers = {\n        \"driver_1\": {\n            \"name\": \"Driver A\",\n            \"type\": \"network\"\n        }\n    }\n    set_driver_info(drivers)\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_instrument","title":"set_instrument","text":"<p>Adds a new information about the instrument (i.e equipment) that form part of the test bench.</p> <p>Can be called multiple times to add multiple instruments. Accepts an Instrument object containing all instrument details.</p> <p>Instrument Parameters:</p> <ul> <li><code>name</code> (str | None): Instrument name</li> <li><code>revision</code> (str | None): Instrument revision</li> <li><code>serial_number</code> (str | None) Instrument serial number</li> <li><code>part_number</code> (str | None)  Instrument part number</li> <li><code>number</code> (int | None): Instrument number  </li> <li><code>comment</code> (str | None): Instrument comment  </li> <li><code>info</code> (Mapping[str, str | int | float ] | None): Additional instrument info </li> </ul> <p>Example:</p> <pre><code>def test_instruments():\n    instrument = Instrument(\n        name=\"Oscilloscope\",\n        revision=\"1.2\",\n        serial_number=\"4235098\",\n        part_number=\"E012\",\n        number=1,\n        info={\"model\": \"DSO-X 2024A\", \"bandwidth\": \"200MHz\"}\n    )\n    set_instrument(instrument)\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_process_name","title":"set_process_name","text":"<p>Writes a string with a process name. When called again, the exception <code>DuplicateParameterError</code> will be raised.</p> <p>Arguments:</p> <ul> <li><code>name</code> (str): Process name (e.g. \"Production Test\")</li> </ul> <p>Example:</p> <pre><code>def test_process():\n    set_process_name(\"Acceptance Test\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_process_number","title":"set_process_number","text":"<p>Writes an integer with a process number. When called again, the exception <code>DuplicateParameterError</code> will be raised.</p> <p>Arguments:</p> <ul> <li><code>number</code> (int): Process number</li> </ul> <p>Example:</p> <pre><code>def test_process():\n    set_process_number(1)\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_process_info","title":"set_process_info","text":"<p>Writes a dictionary with additional information about the process. When called again, the information will be merged with existing data.</p> <p>Arguments:</p> <ul> <li><code>info</code> (Mapping[str, str | int | float | None]): Process info dictionary</li> </ul> <p>Example:</p> <pre><code>def test_process_info():\n    set_process_info({\n        \"stage\": \"production\",\n        \"version\": \"1.0\",\n    })\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_case_measurement","title":"set_case_measurement","text":"<p>Writes measurement information to a database in the form of a case measurement list. When called again, the information will be added to case measurement list.</p> <p>Arguments:</p> <ul> <li><code>measurement</code> NumericMeasurement |   StringMeasurement: measurement data.</li> </ul> <p>Returns:</p> <ul> <li>(int): measurement index</li> </ul> <p>Example:</p> <pre><code>def test_measurement():\n    meas_1 = NumericMeasurement(\n        name=\"Voltage\", \n        value=12.3, \n        unit=\"V\",\n        operation=ComparisonOperation.GELE, \n        lower_limit=10.0, \n        upper_limit=15.0\n    )\n    set_case_measurement(meas_1)\n\n    meas_2 = NumericMeasurement(\n        value=5, \n        unit=\"\u00b0\", \n        operation=ComparisonOperation.EQ, \n        comparison_value=5\n    )\n    set_case_measurement(meas_2)\n\n    meas_3 = NumericMeasurement(\n        name=\"Count\",\n        value=42\n    )\n    set_case_measurement(meas_3)\n\n    meas_4 = StringMeasurement(\n        value=\"1.2.0\", \n        operation=ComparisonOperation.EQ, \n        comparison_value=\"1.2.0\"\n    )\n    set_case_measurement(meas_4)\n\n    meas_5 = StringMeasurement(\n        name=\"Version\",\n        value=\"v2.1.0\", \n        operation=ComparisonOperation.EQ, \n        comparison_value=\"v2.1.0\"\n    )\n    set_case_measurement(meas_5)\n\n    assert meas_1.result\n    assert meas_2.result\n    assert meas_4.result\n    assert meas_5.result\n</code></pre> <p>Operator panel display: The example above would display in the operator panel as:</p> <ul> <li><code>Voltage 12.3 V</code></li> <li><code>5\u00b0</code></li> <li><code>Count 42</code></li> <li><code>1.2.0</code></li> <li><code>Version v2.1.0</code></li> </ul>"},{"location":"documentation/pytest_hardpy/#set_case_chart","title":"set_case_chart","text":"<p>Writes chart (data series) information to a test case in the database. Only one Chart object can be stored per test case in the database. When called again, the exception <code>DuplicateParameterError</code> will be raised.</p> <p>Arguments:</p> <ul> <li><code>chart</code> Chart: chart data.</li> </ul> <p>Example:</p> <pre><code>def test_chart():\n    chart = hardpy.Chart(\n        type=hardpy.ChartType.LINE,\n        title=\"title\",\n        x_label=\"x_label\",\n        y_label=\"y_label\",\n        marker_name=[\"marker_name\", None],\n        x_data=[ [1, 2], [1, 2] ],\n        y_data=[ [3, 4], [3, 4] ]\n    )\n    hardpy.set_case_chart(chart)\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_case_artifact","title":"set_case_artifact","text":"<p>Writes a dictionary with a test case artifact. When called again, the data will be added to DB.</p> <p>Artifacts are saved only in the runstore database because the state in statestore and case artifact must be separated.</p> <p>The <code>set_case_artifact</code> function must be called from a test case.</p> <p>Arguments:</p> <ul> <li><code>data</code> (dict): data</li> </ul> <p>Example:</p> <pre><code>def test_case_artifact():\n    set_case_artifact({\"data_str\": \"123DATA\"})\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_module_artifact","title":"set_module_artifact","text":"<p>Writes a dictionary with a module test artifact. When called again, the data will be added to DB.</p> <p>Artifacts are saved only in the runstore database because the state in statestore and module artifact must be separated.</p> <p>The <code>set_module_artifact</code> function must be called from a test case.</p> <p>Arguments:</p> <ul> <li><code>data</code> (dict): data</li> </ul> <p>Example:</p> <pre><code>def test_module_artifact():\n    set_module_artifact({\"data_str\": \"456DATA\"})\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_run_artifact","title":"set_run_artifact","text":"<p>Writes a dictionary with a test run artifact. When called again, the data will be added to DB.</p> <p>Artifacts are saved only in the runstore database because the state in statestore and run artifact must be separated.</p> <p>Arguments:</p> <ul> <li><code>data</code> (dict): data</li> </ul> <p>Example:</p> <pre><code>def test_run_artifact():\n    set_run_artifact({\"data_str\": \"789DATA\"})\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_message","title":"set_message","text":"<p>Writes a string with a message. If a message is sent without a key, the key will be generated automatically and the messages will be appended. If the message is sent with a known key, it will be updated.</p> <p>The <code>set_message</code> function must be called from a test case.</p> <p>Arguments:</p> <ul> <li><code>msg</code> (str): Message content.</li> <li><code>msg_key</code> (Optional[str]): Message ID. If not specified, a random ID will be generated.</li> </ul> <p>Example:</p> <pre><code>def test_message():\n    set_message(\"Test message\")\n    set_message(\"Update message 1\", \"msg_upd\")\n    set_message(\"Update message 2\", \"msg_upd\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#set_operator_message","title":"set_operator_message","text":"<p>Sets an operator message in the statestore database and updates the database. Does not provide user interaction unlike the run_dialog_box function.</p> <p>Arguments:</p> <ul> <li><code>msg</code> (str): The message to be displayed.</li> <li><code>title</code> (str | None): The optional title for the message.</li> <li><code>block</code> (bool=True): If True, the function will block until the message is closed.</li> <li><code>image</code> (ImageComponent | None): Image information.</li> <li><code>html</code> (HTMLComponent | None): HTML information.</li> <li><code>font_size</code>: (int=14): Text font size.</li> </ul> <p>Example:</p> <pre><code>from hardpy import set_operator_message\n\ndef test_set_operator_msg():\n    set_operator_message(msg=\"This is a sample operator message.\", title=\"Important Notice\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#clear_operator_message","title":"clear_operator_message","text":"<p>Clears the current message to the operator if it exists, otherwise does nothing.</p> <p>Example:</p> <pre><code>from time import sleep\nfrom hardpy import set_operator_message, clear_operator_message\n\ndef test_clear_operator_msg():\n    hardpy.set_operator_message(msg=\"Clearing operator message.\", title=\"Operator message\", block=False)\n    sleep(2)\n    clear_operator_message()\n</code></pre>"},{"location":"documentation/pytest_hardpy/#run_dialog_box","title":"run_dialog_box","text":"<p>Displays a dialog box and updates the <code>dialog_box</code> field in the statestore database.</p> <p>The <code>run_dialog_box</code> function must be called from a test case.</p> <p>Arguments:</p> <ul> <li><code>dialog_box_data</code> (DialogBox): Data for the dialog box.</li> </ul> <p>Returns:</p> <ul> <li>(Any): An object containing the user's response.</li> </ul> <p>The type of the return value depends on the widget type:</p> <ul> <li>Without widget (BASE): bool.</li> <li>NUMERIC_INPUT: float.</li> <li>TEXT_INPUT: str.</li> <li>RADIOBUTTON: str.</li> <li>CHECKBOX: List(str).</li> <li>MULTISTEP: bool.</li> </ul> <p>Raises</p> <ul> <li><code>ValueError</code>: If the <code>message</code> argument is empty.</li> </ul> <p>Example:</p> <pre><code>from hardpy import dialog_box\ndef test_text_input():\n    dbx = DialogBox(\n        dialog_text=\"Type 'ok' and press the Confirm button\",\n        title_bar=\"Example of text input\",\n        widget=TextInputWidget(),\n        image=ImageComponent(address=\"assets/test.png\", width=50),\n    )\n    response = run_dialog_box(dbx)\n    set_message(f\"Entered text {response}\")\n    assert response == \"ok\", \"The entered text is not correct\"\n</code></pre>"},{"location":"documentation/pytest_hardpy/#get_current_report","title":"get_current_report","text":"<p>Returns the current report from the database runstore.</p> <p>Returns:</p> <ul> <li>(ResultRunStore | None): report, or None if not found or invalid</li> </ul> <p>Example:</p> <pre><code>def test_current_report():\n    report = get_current_report()\n</code></pre>"},{"location":"documentation/pytest_hardpy/#get_current_attempt","title":"get_current_attempt","text":"<p>Returns the num of current attempt.</p> <p>The <code>get_current_attempt</code> function must be called from a test case.</p> <p>Returns:</p> <ul> <li>(int): num of current attempt</li> </ul> <p>Example:</p> <pre><code>@pytest.mark.attempt(5)\ndef test_attempt_message():\n    attempt = hardpy.get_current_attempt()\n    hardpy.set_message(f\"Current attempt {attempt}\")\n    if attempt &lt; 5:\n        assert False\n</code></pre>"},{"location":"documentation/pytest_hardpy/#get_hardpy_config","title":"get_hardpy_config","text":"<p>Returns the actual HardPy project configuration from the hardpy.toml file.</p> <p>Returns:</p> <ul> <li>HardpyConfig: HardPy project configuration.</li> </ul> <p>Example:</p> <pre><code>config = hardpy.get_hardpy_config()\nprint(config.database.storage_type)\n</code></pre>"},{"location":"documentation/pytest_hardpy/#class","title":"Class","text":""},{"location":"documentation/pytest_hardpy/#hardpyconfig","title":"HardpyConfig","text":"<p>This class defines the configuration of the HardPy project for the hardpy.toml file. Users can obtain an instance of HardpyConfig with the current values from the  hardpy.toml file using the get_hardpy_config() function.</p>"},{"location":"documentation/pytest_hardpy/#dialogbox","title":"DialogBox","text":"<p>The class is used to configure the dialogue box and is used with the run_dialog_box function.</p> <p>Arguments:</p> <ul> <li><code>dialog_text</code> (str): The text of the dialog box.</li> <li><code>title_bar</code> (str | None): The title bar of the dialog box. If the <code>title_bar</code> field is missing, it is the case name.</li> <li><code>widget</code> (IWidget | None): Widget information.</li> <li><code>image</code> (ImageComponent | None): Image information.</li> <li><code>html</code> (HTMLComponent | None): HTML information.</li> <li><code>font_size</code>: (int=14): Text font size.</li> <li><code>pass_fail</code>: (bool): enable pass/fail buttons instead of confirm button.</li> <li><code>button_text</code>: (list | None): user text for buttons.</li> </ul> <p>Widget list:</p> <ul> <li>Base, only dialog text;</li> <li>Text input, TextInputWidget;</li> <li>Numeric input, NumericInputWidget;</li> <li>Radiobutton, RadiobuttonWidget;</li> <li>Checkbox, CheckboxWidget;</li> <li>Multistep, MultistepWidget.</li> </ul> <p>Example:</p> <pre><code>    DialogBox(title_bar=\"Example title\", dialog_text=\"Example text\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#textinputwidget","title":"TextInputWidget","text":"<p>The class is used to configure text input widget in dialog box. Further information can be found in section text input field. Widget returns a string when using run_dialog_box.</p> <p>Example:</p> <pre><code>    dbx = DialogBox(\n        dialog_text=\"Type 'ok' and press the Confirm button\",\n        title_bar=\"Example of text input\",\n        widget=TextInputWidget(),\n    )\n    response = run_dialog_box(dbx)\n</code></pre>"},{"location":"documentation/pytest_hardpy/#numericinputwidget","title":"NumericInputWidget","text":"<p>The class is used to configure numeric input widget in dialog box. Further information can be found in section numeric input field. Widget returns a float when using run_dialog_box.</p> <p>Example:</p> <pre><code>    dbx = DialogBox(\n        dialog_text=f\"Enter the number {test_num} and press the Confirm button\",\n        title_bar=\"Example of entering a number\",\n        widget=NumericInputWidget(),\n    )\n    response = int(run_dialog_box(dbx))\n</code></pre>"},{"location":"documentation/pytest_hardpy/#radiobuttonwidget","title":"RadiobuttonWidget","text":"<p>The class is used to configure radiobutton widget in dialog box. Further information can be found in section radiobutton. Widget returns a string with the selected radiobutton value run_dialog_box.</p> <p>Arguments:</p> <ul> <li><code>fields</code> (list[str]): Radiobutton fields.</li> </ul> <p>Example:</p> <pre><code>    dbx = DialogBox(\n        dialog_text='Select item \"one\" out of several and click Confirm.',\n        title_bar=\"Radiobutton example\",\n        widget=RadiobuttonWidget(fields=[\"one\", \"two\", \"three\"]),\n    )\n    response = run_dialog_box(dbx)\n</code></pre>"},{"location":"documentation/pytest_hardpy/#checkboxwidget","title":"CheckboxWidget","text":"<p>The class is used to configure checkbox widget in dialog box. Further information can be found in section checkbox. Widget returns a list of string with the selected checkbox value run_dialog_box.</p> <p>Arguments:</p> <ul> <li><code>fields</code> (list[str]): Checkbox fields.</li> </ul> <p>Example:</p> <pre><code>    dbx = DialogBox(\n        dialog_text='Select items \"one\" and \"two\" and click the Confirm button',\n        title_bar=\"Checkbox example\",\n        widget=CheckboxWidget(fields=[\"one\", \"two\", \"three\"]),\n    )\n    response = run_dialog_box(dbx)\n</code></pre>"},{"location":"documentation/pytest_hardpy/#stepwidget","title":"StepWidget","text":"<p>The class is used to configure the step for the multistep widget in dialog box.</p> <p>Arguments:</p> <ul> <li><code>title</code> (str): Step title.</li> <li><code>text</code> (str | None): Step text.</li> <li><code>image</code> (ImageComponent | None): Step image.</li> <li><code>html</code> (HTMLComponent | None): Step HTML.</li> </ul> <p>Example:</p> <pre><code>    StepWidget(\"Step 1\", text=\"Content for step\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#multistepwidget","title":"MultistepWidget","text":"<p>The class is used to configure multistep widget in dialog box. Further information can be found in section multiple steps.</p> <p>Arguments:</p> <ul> <li><code>steps</code> (list[StepWidget]): A list with info about the steps.</li> </ul> <p>Example:</p> <pre><code>    steps = [\n        StepWidget(\"Step 1\", text=\"Content for step\"),\n        StepWidget(\"Step 2\", text=\"Content for step 2\", image=ImageComponent(address=\"assets/test.png\", width=100)),\n        StepWidget(\"Step 3\", text=\"Content for step 3\", html=HTMLComponent(html=\"https://everypinio.github.io/hardpy/\", width=50)),\n    ]\n    dbx = DialogBox(dialog_text=\"Follow the steps and click Confirm\", widget=MultistepWidget(steps))\n    response = run_dialog_box(dbx)\n</code></pre>"},{"location":"documentation/pytest_hardpy/#imagecomponent","title":"ImageComponent","text":"<p>A class for configuring an image for a dialogue box or operator message box and is used with the run_dialog_box and set_operator_message functions.</p> <p>Arguments:</p> <ul> <li><code>address</code> (str): Image address.</li> <li><code>width</code> (int | None): Image width in %.</li> <li><code>border</code> (int | None): Image border width.</li> </ul> <p>Example:</p> <pre><code>    ImageComponent(address=\"assets/test.png\", width=100)\n</code></pre>"},{"location":"documentation/pytest_hardpy/#htmlcomponent","title":"HTMLComponent","text":"<p>A class for configurating HTML for a dialogue box or operator message box and is used with the run_dialog_box and set_operator_message functions.</p> <p>Arguments:</p> <ul> <li><code>code_or_url</code> (str): HTML code or link.</li> <li><code>width</code> (int | None): HTML width in %.</li> <li><code>border</code> (int | None): HTML border width.</li> <li><code>is_raw_html</code> (bool): Is HTML code is raw.</li> </ul> <p>Example:</p> <pre><code>    HTMLComponent(code_or_url=\"https://everypinio.github.io/hardpy/\", width=100, is_raw_html=False)\n</code></pre>"},{"location":"documentation/pytest_hardpy/#couchdbloader","title":"CouchdbLoader","text":"<p>Used to write reports to the database CouchDB.</p> <p>Report names (revision id) are automatically generated based on the test completion date and the device serial number. If the serial number dut is empty, a random identifier with prefix <code>no_serial</code> is used. The random identifier is a unique string generated using the <code>uuid4()</code> function from the <code>uuid</code> module in Python. This allows for easy identification and sorting of reports.</p> <ul> <li>Valid report name: <code>report_1726496218_1234567890</code></li> <li>Valid report name (no serial number): <code>report_1726496218_no_serial_808007</code></li> </ul> <p>Functions:</p> <ul> <li><code>load</code> (ResultRunStore): Load report to the CouchDB report database.</li> </ul> <p>Example:</p> <pre><code># conftest\ndef finish_executing():\n    report = get_current_report()\n    if report:\n        loader = CouchdbLoader(CouchdbConfig())\n        loader.load(report)\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef fill_actions_after_test(post_run_functions: list):\n    post_run_functions.append(finish_executing)\n    yield\n</code></pre>"},{"location":"documentation/pytest_hardpy/#jsonloader","title":"JsonLoader","text":"<p>Used to write reports to the JSON.</p> <p>Arguments:</p> <ul> <li><code>storage_dir</code> (Path | None): JSON file directory.</li> </ul> <p>Functions:</p> <ul> <li><code>load</code> (ResultRunStore, new_report_id): Load report to the JSON file.</li> </ul> <p>Example:</p> <pre><code># conftest\ndef save_report_to_dir():\n    report = get_current_report()\n    if report:\n        loader = JsonLoader(Path.cwd() / \"reports\")\n        loader.load(report)\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef fill_actions_after_test(post_run_functions: list):\n    post_run_functions.append(save_report_to_dir)\n    yield\n</code></pre>"},{"location":"documentation/pytest_hardpy/#standcloudloader","title":"StandCloudLoader","text":"<p>Used to write reports to the StandCloud. A login to StandCloud is required to work.</p> <p>Arguments:</p> <ul> <li><code>address</code> (str | None): StandCloud address. Defaults to None   (the value is taken from hardpy.toml). Can be used outside of HardPy applications.</li> </ul> <p>Functions:</p> <ul> <li><code>healthcheck</code>: Healthcheck of StandCloud API.   Returns the <code>requests.Response</code> object.</li> <li><code>load</code> (ResultRunStore): Load report to the StandCloud.   Returns the <code>requests.Response</code> object.   Status code 201 is considered a successful status.</li> </ul> <p>Example:</p> <pre><code># conftest\ndef finish_executing():\n    report = get_current_report()\n    if report:\n        try:\n            loader = StandCloudLoader()\n            response = loader.load(report)\n            if response.status_code != HTTPStatus.CREATED:\n                set_operator_message(\n                    \"Report not uploaded to StandCloud, \"\n                    f\"status code: {response.status_code}, text: {response.text}\",\n                )\n        except StandCloudError as exc:\n            set_operator_message(f\"{exc}\")\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef fill_actions_after_test(post_run_functions: list):\n    post_run_functions.append(finish_executing)\n    yield\n</code></pre>"},{"location":"documentation/pytest_hardpy/#standcloudconnector","title":"StandCloudConnector","text":"<p>Used to create the StandCloud connection addresses.</p> <p>Arguments:</p> <ul> <li><code>addr</code> (str): StandCloud service name.   For example: demo.standcloud.io</li> <li><code>api_mode</code> (StandCloudAPIMode): StandCloud API mode:   hardpy for test stand, integration for third-party service.   Default: <code>StandCloudAPIMode.HARDPY</code></li> <li><code>api_version</code> (int): StandCloud API version.   Default: 1.</li> </ul>"},{"location":"documentation/pytest_hardpy/#standcloudreader","title":"StandCloudReader","text":"<p>Used to read data from the StandCloud. A login to StandCloud is required to work. For more information, see the example StandCloud reader</p> <p>Arguments:</p> <ul> <li><code>sc_connector</code> (StandCloudConnector): StandCloud connection data.</li> </ul> <p>Functions:</p> <ul> <li><code>test_run</code> (run_id: str, params: dict[str, Any]) - get run data from <code>/test_run</code> endpoint.   All test run filters can view in REST documentation.   Return <code>requests.Response</code> class with test run data.</li> <li><code>tested_dut</code> (params: dict[str, Any]) - get last tested DUT's data from <code>/tested_dut</code> endpoint.   All tested dut's filters can view in REST documentation.   Return <code>requests.Response</code> class with tested DUT's data.</li> </ul> <p>In terms of filters, the difference between <code>test_run</code> and <code>tested_dut</code> in terms of filters is that in test_run allows you to request a filter for the number of runs - <code>number_of_attempt</code>, while tested_dut allows you to request a filter for the number of runs - <code>attempt_count</code>. In other words, <code>tested_dut</code> will return the last run with the specified number of runs specified in <code>attempt_count</code>, and <code>test_run</code> will return the runs whose start number is equal to the <code>number_of_attempt</code> specified in the filter.</p> <p>Examples:</p> <pre><code>    reader = StandCloudReader(StandCloudConnector(addr=\"demo.standcloud.io\"))\n\n    response = reader.test_run(run_id=\"0196434d-e8f7-7ce1-81f7-e16f20487494\")\n    status_code = response.status_code\n    response_data = response.json()\n    print(response_data)\n\n    param = {\"part_number\": \"part_number_1\", \"status\": \"pass\", \"firmware_version\": \"1.2.3\"}\n    response = reader.test_run(params=param)\n    status_code = response.status_code\n    response_data = response.json()\n    print(response_data)\n\n    response = reader.tested_dut(param)\n    status_code = response.status_code\n    response_data = response.json()\n    print(response_data)\n</code></pre>"},{"location":"documentation/pytest_hardpy/#instrument","title":"Instrument","text":"<p>The class is used to store information about test equipment that forms part of the test bench.  It is used with the set_instrument function.</p> <p>Arguments:</p> <ul> <li><code>name</code> (str | None): Instrument name  </li> <li><code>revision</code> (str | None): Instrument revision  </li> <li><code>serial_number</code> (str | None): Instrument serial number  </li> <li><code>part_number</code> (str | None): Instrument part number  </li> <li><code>number</code> (int | None): Instrument number  </li> <li><code>comment</code> (str | None): Instrument comment  </li> <li><code>info</code> (Mapping[str, str | int | float | None]): Additional instrument info as key-value pairs  </li> </ul> <p>Returns:</p> <ul> <li>(int): instrument index</li> </ul> <p>Validation Rules:</p> <ul> <li><code>number</code> must be positive if specified (\u2265 0)</li> </ul> <p>Example:</p> <pre><code>oscilloscope = Instrument(\n    name=\"Oscilloscope\",\n    revision=\"1.2.3\",\n    serial_number=\"1325\",\n    part_number=\"EOSC_23\",\n    number=1,\n    info={\n        \"model\": \"DSO-X 2024A\",\n        \"bandwidth\": \"200MHz\",\n    }\n)\n\nset_instrument(oscilloscope)\n</code></pre>"},{"location":"documentation/pytest_hardpy/#subunit","title":"SubUnit","text":"<p>This class contains information about the sub-unit of the DUT.  It is used with the set_dut_sub_unit function.</p> <p>Arguments:</p> <ul> <li><code>serial_number</code> (str | None): unit serial number</li> <li><code>part_number</code> (str | None): unit part number</li> <li><code>name</code> (str | None): unit name  </li> <li><code>type</code> (str | None): unit type</li> <li><code>revision</code> (str | None): unit revision  </li> <li><code>info</code> (Mapping[str, str | int | float  | None] | None): additional unit info as key-value pairs  </li> </ul> <p>Example:</p> <pre><code>def test_dut_sub_unit():\n    sub_unit = SubUnit(\n        serial_number=\"12345\",\n        part_number=\"part_number_1\",\n        name=\"Test Device\",\n        type=\"PCBA\",\n        revision=\"REV1.0\",\n        info={\"sw_version\": \"1.0\"},\n    )\n    hardpy.set_dut_sub_unit(sub_unit)\n</code></pre>"},{"location":"documentation/pytest_hardpy/#numericmeasurement","title":"NumericMeasurement","text":"<p>This class contains information about numeric measurement.  It is used with the set_case_measurement function.</p> <p>Arguments:</p> <ul> <li><code>value</code> (float | int): numeric measure value.</li> <li><code>name</code> (str | None): numeric measure name.</li> <li><code>unit</code> (str | None): unit of numeric measure.</li> <li><code>operation</code>: comparison operators of numeric measure.</li> <li><code>comparison_value</code> (ComparisonOperation | None): value to compare against.</li> <li><code>lower_limit</code>: (float | int): lower limit for range operations.</li> <li><code>upper_limit</code> (float | int):upper limit for range operations.</li> </ul> <p>Returns:</p> <ul> <li>(int): measurement index</li> </ul> <p>Example:</p> <pre><code>def test_measurement():\n    meas_1 = NumericMeasurement(value=10, operation=ComparisonOperation.EQ, comparison_value=10)\n    set_case_measurement(meas_1)\n    meas_2 = NumericMeasurement(value=3, unit=\"V\", operation=ComparisonOperation.GTLT, lower_limit=2.9, upper_limit=3.5)\n    set_case_measurement(meas_2)\n    meas_3 = NumericMeasurement(value=1.0)\n    set_case_measurement(meas_3)\n\n    assert meas_1.result\n    assert meas_2.result\n</code></pre>"},{"location":"documentation/pytest_hardpy/#stringmeasurement","title":"StringMeasurement","text":"<p>This class contains information about string measurement.  It is used with the set_case_measurement function.</p> <p>Arguments:</p> <ul> <li><code>value</code> (str): string measure value.</li> <li><code>name</code> (str | None): string measure name.</li> <li><code>operation</code>: comparison operators of string measure (EQ and NE).</li> <li><code>comparison_value</code>: (ComparisonOperation | None): value to compare against.</li> <li><code>casesensitive</code>: bool=True: case sensitivity.</li> </ul> <p>Example:</p> <pre><code>def test_measurement():\n    meas_1 = StringMeasurement(value=\"1.2.0\", operation=ComparisonOperation.EQ, comparison_value=\"1.2.0\")\n    set_case_measurement(meas_1)\n\n    meas_2 = StringMeasurement(value=\"abc\", operation=ComparisonOperation.EQ, casesensitive=False, comparison_value=\"ABC\")\n    set_case_measurement(meas_2)\n\n    assert meas_1.result\n    assert meas_2.result\n</code></pre>"},{"location":"documentation/pytest_hardpy/#chart","title":"Chart","text":"<p>This class contains information about chart (data series).  It is used with the set_case_chart function.</p> <p>A <code>ValidationError</code> will result if the lengths of x_data, y_data, and marker_name differ.</p> <p>Arguments:</p> <ul> <li><code>type</code> (ChartType): chart type, LINE by default.</li> <li><code>title</code> (str | None): chart title.</li> <li><code>x_label</code> (str | None): x label name.</li> <li><code>y_label</code> (str | None): y label name.</li> <li><code>marker_name</code> (list[str | None]): data series marker name.</li> <li><code>x_data</code> (list[list[int | float]]): x data series. Each data series is its own list.</li> <li><code>y_data</code> (list[list[int | float]]): y data series. Each data series is its own list.</li> </ul> <p>Functions:</p> <ul> <li><code>add_series</code> (x_data: list[int | float], y_data: list[int | float], marker_name: str | None = None): add data series to current <code>Chart</code>. </li> </ul> <p>Example:</p> <pre><code>def test_chart():\n    chart = hardpy.Chart(\n        type=hardpy.ChartType.LINE,\n        title=\"title\",\n        x_label=\"x_label\",\n        y_label=\"y_label\",\n        marker_name=[\"marker_name\", None],\n        x_data=[ [1, 2], [1, 2] ],\n        y_data=[ [3, 4], [3, 4] ]\n    )\n    chart.add_series(x_data=[0, 3], y_data=[2, 4], marker_name=\"A\")\n    hardpy.set_case_chart(chart)\n</code></pre>"},{"location":"documentation/pytest_hardpy/#errorcode","title":"ErrorCode","text":"<p>This class saves the error code of the test plan to the database. Only the first error code from this class is saved.</p> <p>The <code>ErrorCode</code> class must be called from the <code>assert</code>.  Otherwise, an extra error code may be saved.</p> <p>Arguments:</p> <ul> <li><code>error_code</code> (int): error code (non-negative integer).</li> <li><code>message</code> (str | None): error code message.</li> </ul> <p>Example:</p> <pre><code>def test_error_code():\n    assert False, hardpy.ErrorCode(1, \"error code message\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#enum","title":"Enum","text":""},{"location":"documentation/pytest_hardpy/#comparisonoperation","title":"ComparisonOperation","text":"<p>A comparison operation for the measurements.</p> <p>Values:</p> <ul> <li>EQ: equal.</li> <li>NE: not equal.</li> <li>GT: greater than.</li> <li>GE: greater or equal.</li> <li>LT: less than.</li> <li>LE: less or equal.</li> <li>GTLT: greater than lower limit, less than upper limit.</li> <li>GELE: greater or equal than lower limit, less or equal than upper limit.</li> <li>GELT: greater or equal than lower limit, less than upper limit.</li> <li>GTLE: greater than lower limit, less or equal than upper limit.</li> <li>LTGT: less than lower limit or greater than upper limit.</li> <li>LEGE: less or equal than lower limit or greater or equal than upper limit.</li> <li>LEGT: less or equal than lower limit or greater than upper limit.</li> <li>LTGE: less than lower limit or greater or equal than upper limit.</li> </ul>"},{"location":"documentation/pytest_hardpy/#charttype","title":"ChartType","text":"<p>This is a chart type for the Chart class.</p> <p>Values:</p> <ul> <li>LINE: line.</li> <li>LINE_LOG_X: line_log_x.</li> <li>LINE_LOG_Y: line_log_y.</li> <li>LOG_X_Y: log_x_y.</li> </ul>"},{"location":"documentation/pytest_hardpy/#fixture","title":"Fixture","text":""},{"location":"documentation/pytest_hardpy/#post_run_functions","title":"post_run_functions","text":"<p>To execute actions at the end of testing, you can use the fixture post_run_functions. This fixture is a <code>list[Callable]</code> and you can write functions into it that must be executed at the end of testing.</p> <p>Fill this list in conftest.py and functions from this list will be called after tests run (at the end of pytest_sessionfinish).</p> <p>Returns:</p> <ul> <li>(list[Callable]): list of post run methods</li> </ul> <p>Example:</p> <pre><code># conftest.py file\ndef finish_executing():\n    print(\"Pytest finished\")\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef fill_actions_after_test(post_run_functions: list):\n    post_run_functions.append(finish_executing)\n    yield\n</code></pre>"},{"location":"documentation/pytest_hardpy/#hardpy_start_args","title":"hardpy_start_args","text":"<p>To access HardPy start arguments passed via command line, you can use the <code>hardpy_start_args</code> fixture. This fixture returns a <code>dict</code> containing parsed key-value pairs from <code>--hardpy-start-arg</code> options.</p> <p>Returns:</p> <ul> <li>(dict): dictionary of start arguments (key-value pairs)</li> </ul> <p>Example:</p> <pre><code>def test_with_start_args(hardpy_start_args):\n\n    if hardpy_start_args.get(\"test_mode\") == \"debug\":\n        hardpy.set_message(\"Running in debug mode\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#marker","title":"Marker","text":""},{"location":"documentation/pytest_hardpy/#case_name","title":"case_name","text":"<p>Sets a text name for the test case (default: function name)</p> <p>Example:</p> <pre><code>@pytest.mark.case_name(\"Simple case 1\")\ndef test_one():\n    assert True\n</code></pre>"},{"location":"documentation/pytest_hardpy/#module_name","title":"module_name","text":"<p>Sets a text name for the test module (file) (default: module name)</p> <p>Example:</p> <pre><code>pytestmark = pytest.mark.module_name(\"Module 1\")\n</code></pre>"},{"location":"documentation/pytest_hardpy/#case_group","title":"case_group","text":"<p>Sets the group for a test case. Valid groups: <code>setup</code>, <code>main</code>, <code>teardown</code> (default: <code>main</code>)</p> <p>Example:</p> <pre><code>from hardpy import Group\n\n@pytest.mark.case_group(Group.SETUP)\ndef test_setup_case():\n    assert True\n\n@pytest.mark.case_group(\"teardown\")  \ndef test_teardown_case():\n    assert True\n</code></pre>"},{"location":"documentation/pytest_hardpy/#module_group","title":"module_group","text":"<p>Sets the group for all test cases in a module. Valid groups: <code>setup</code>, <code>main</code>, <code>teardown</code> (default: <code>main</code>)</p> <p>Example:</p> <pre><code>import pytest\nfrom hardpy import Group\n\npytestmark = pytest.mark.module_group(Group.TEARDOWN)\n\ndef test_cleanup1():\n    assert True\n\ndef test_cleanup2():\n    assert True\n</code></pre>"},{"location":"documentation/pytest_hardpy/#dependency","title":"dependency","text":"<p>Skips the test case/module if the main test fails/skipped/errored. For more information, see the example skip test and skip feature description.</p> <p>Example:</p> <pre><code>#test_1.py\ndef test_one():\n    assert False\n\n@pytest.mark.dependency(\"test_1::test_one\")\ndef test_two():\n    assert True\n</code></pre>"},{"location":"documentation/pytest_hardpy/#attempt","title":"attempt","text":"<p>If a test is marked <code>attempt</code>, it will be repeated if it fails the number of attempts specified by the mark. The test will be repeated until it is passed. There is a 1 second pause between attempts. Each attempt clears the case data, including the message,  the assertion message, the chart, the measurements, and the artifact. For more information, see the example attempts.</p> <p>Example:</p> <pre><code>@pytest.mark.attempt(5)\ndef test_attempts():\n    assert False\n</code></pre>"},{"location":"documentation/pytest_hardpy/#critical","title":"critical","text":"<p>Marks test or module as critical. Failing/skipped critical tests skip all subsequent tests. For implementation details see critical tests example.</p> <p>Example (test level):</p> <pre><code>@pytest.mark.critical\ndef test_core_feature():\n    assert check_core_functionality()\n</code></pre> <p>Example (module level):</p> <pre><code>pytestmark = pytest.mark.critical\n\ndef test_db_connection():\n    assert connect_to_database()\n</code></pre> <p>Behavior:</p> <ul> <li>Critical test passes - Continue normally</li> <li>Critical test fails/skips - Skip all remaining tests</li> <li>Any test fails in critical module - Skip all remaining tests</li> </ul>"},{"location":"documentation/pytest_hardpy/#options","title":"Options","text":"<p>pytest-hardpy has several options to run:</p>"},{"location":"documentation/pytest_hardpy/#hardpy-pt","title":"hardpy-pt","text":"<p>Option to enable the pytest-hardpy plugin.</p> <pre><code>--hardpy-pt\n</code></pre>"},{"location":"documentation/pytest_hardpy/#hardpy-db-url","title":"hardpy-db-url","text":"<p>The CouchDB instance url for the statestore and runstore databases. The default is <code>http://dev:dev@localhost:5984/</code>.</p> <pre><code>--hardpy-db-url\n</code></pre>"},{"location":"documentation/pytest_hardpy/#hardpy-tests-name","title":"hardpy-tests-name","text":"<p>The HardPy tests name. The default value is Tests.</p> <pre><code>--hardpy-tests-name\n</code></pre>"},{"location":"documentation/pytest_hardpy/#hardpy-clear-database","title":"hardpy-clear-database","text":"<p>Option to clean statestore and runstore databases before running pytest.</p> <pre><code>--hardpy-clear-database\n</code></pre>"},{"location":"documentation/pytest_hardpy/#sc-address","title":"sc-address","text":"<p>StandCloud address. The default is empty string.</p> <pre><code>--sc-address\n</code></pre>"},{"location":"documentation/pytest_hardpy/#sc-connection-only","title":"sc-connection-only","text":"<p>Check StandCloud service availability. The default is False.</p> <pre><code>--sc-connection-only\n</code></pre>"},{"location":"documentation/pytest_hardpy/#sc-autosync","title":"sc-autosync","text":"<p>Enable HardPy to StandCloud test report data auto synchroniztion. The default is False.</p> <pre><code>--sc-autosync\n</code></pre>"},{"location":"documentation/pytest_hardpy/#hardpy-config-file","title":"hardpy-config-file","text":"<p>The HardPy configuration file path (hardpy.toml). The default is the tests path.</p> <pre><code>--hardpy-config-file path/to/file/\n</code></pre>"},{"location":"documentation/pytest_hardpy/#hardpy-start-arg","title":"hardpy-start-arg","text":"<p>Dynamic arguments for test execution in key=value format. Can be specified multiple times.</p> <pre><code>--hardpy-start-arg key=value\n</code></pre>"},{"location":"documentation/stand_cloud/","title":"StandCloud","text":""},{"location":"documentation/stand_cloud/#about-standcloud","title":"About StandCloud","text":"<p>StandCloud is a cloud management tool for electronics manufacturing. Test data is crucial for evaluating performance. StandCloud allows you to explore data patterns, create visualizations, and gain insights. This helps in identifying potential trends and opportunities. It allows you to identify potential limitations.</p> <p>For more information, visit the StandCloud website.</p>"},{"location":"documentation/stand_cloud/#standcloud-and-hardpy-integration","title":"StandCloud and HardPy integration","text":"<p>HardPy allows test result data to be stored in the StandCloud and read them from the StandCloud. For an example of StandCloud and HardPy integration, see StandCloud example and StandCloud readed example.</p>"},{"location":"documentation/stand_cloud/#hardpy-rules","title":"HardPy rules","text":""},{"location":"documentation/stand_cloud/#general-guidelines","title":"General guidelines","text":"<p>Some tips for getting the best analytics in StandCloud.</p> <ol> <li>Use the set_dut_serial_number    to store DUT serial number.    The serial number allows you to distinguish between units with    the same part number. It also allows you to analyze the    number of attempts to test a device.</li> <li>Use the set_dut_part_number    to store DUT part number.    The part number is an identifier that allows you to clearly     identify the product.   </li> <li>Use the set_case_artifact,    set_module_artifact,    and set_run_artifact    to store important information, that cannot be accommodated in other structured fields    such as set_case_measurement    and set_case_chart.</li> <li>Use the case_name    and module_name    markers for human-readable names.    They make it easier to analyze the tests.</li> </ol>"},{"location":"examples/","title":"Examples","text":"<p>HardPy includes working examples directly in the package in the examples folder. However, the documentation contains more examples, which you can explore in this section.</p>"},{"location":"examples/attempts/","title":"Attempts","text":"<p>In HardPy library, the @pytest.mark.attempt(n) marker allows you to configure a test to be retried a specified number of times <code>(n)</code> if it initially fails. This functionality is particularly useful for tests that might encounter transient errors or require multiple attempts to succeed due to external factors. The code for this example can be seen inside the hardpy package Attempts.</p> <p>Contains some examples of valid tests with attempts.</p>"},{"location":"examples/attempts/#how-to-start","title":"how to start","text":"<ol> <li>Launch <code>hardpy init attempts</code>.</li> <li>Launch CouchDB instance.</li> <li>Modify the files described below.</li> <li>Launch <code>hardpy run attempts</code>.</li> </ol>"},{"location":"examples/attempts/#conftestpy","title":"conftest.py","text":"<pre><code>import datetime\nimport pytest\n\nclass CurrentMinute:\n    def get_minute(self):\n        current_time = datetime.datetime.now()\n        return int(current_time.strftime(\"%M\"))\n\n@pytest.fixture\ndef current_minute():\n    current_time = CurrentMinute()\n    yield current_time\n</code></pre>"},{"location":"examples/attempts/#test_1py","title":"test_1.py","text":"<pre><code>from time import sleep\nimport pytest\nimport hardpy\nfrom hardpy import DialogBox, TextInputWidget, run_dialog_box\n\n@pytest.mark.attempt(5)\ndef test_minute_parity(current_minute):\n    attempt = hardpy.get_current_attempt()\n    hardpy.set_message(f\"Current attempt {attempt}\", \"updated_status\")\n    if attempt &gt; 1:\n        sleep(15)\n\n    minute = current_minute.get_minute()\n    hardpy.set_message(f\"Current minute {minute}\")\n    hardpy.set_case_artifact({\"minute\": minute})\n\n    assert minute % 2 == 0, f\"The test failed because {minute} is odd! Try again!\"\n\n@pytest.mark.attempt(3)\ndef test_dialog_box():\n    dbx = DialogBox(\n        dialog_text=\"Print 'ok', if you want to pass attempt.\",\n        title_bar=\"Example of text input\",\n        widget=TextInputWidget(),\n    )\n    response = run_dialog_box(dbx)\n    if response != \"ok\":\n        dbx = DialogBox(\n            dialog_text=f\"Test attempt {hardpy.get_current_attempt()}\",\n            title_bar=\"Attempt message\",\n        )\n        run_dialog_box(dbx)\n\n    assert response == \"ok\", \"The entered text is not correct\"\n</code></pre>"},{"location":"examples/chart/","title":"Chart","text":"<p>This is an example of how charts can be saved in tests.</p> <p>The set_case_chart function allows  to save chart (data series) result to database.</p>"},{"location":"examples/chart/#how-to-start","title":"how to start","text":"<ol> <li>Launch <code>hardpy init charts</code>.</li> <li>Launch CouchDB instance.</li> <li>Modify the files described below.</li> <li>Launch <code>hardpy run charts</code>.</li> </ol>"},{"location":"examples/chart/#test_1py","title":"test_1.py","text":"<pre><code>import pytest\n\nfrom hardpy import Chart, ChartType, set_case_chart\n\npytestmark = pytest.mark.module_name(\"Charts\")\n\n@pytest.mark.case_name(\"Line chart\")\ndef test_line_chart():\n    chart = Chart(\n        type=ChartType.LINE,\n        title=\"title\",\n        x_label=\"x_label\",\n        y_label=\"y_label\",\n        marker_name=[None, None],\n        x_data=[ [1, 2], [1, 2] ],\n        y_data=[ [3, 4], [3, 4] ],\n    )\n    set_case_chart(chart)\n\n@pytest.mark.case_name(\"Multiple charts\")\ndef test_multiple_charts():\n    chart = Chart()\n    chart.add_series(x_data=[1, 2, 3], y_data=[1, 2, 3], marker_name=\"a\")\n    chart.add_series(x_data=[1, 2, 4], y_data=[5, 4, 3], marker_name=\"b\")\n    set_case_chart(chart)\n\n@pytest.mark.case_name(\"Line chart with logarithmic X axis\")\ndef test_line_log_x():\n    chart = Chart(\n        type=ChartType.LINE_LOG_X,\n        title=\"Logarithmic X Axis\",\n        x_label=\"Log X\",\n        y_label=\"Linear Y\",\n    )\n    x_data = [10**i for i in range(5)]\n    y_data = [i**2 for i in range(5)]\n    chart.add_series(x_data=x_data, y_data=y_data, marker_name=\"Quadratic\")\n    set_case_chart(chart)\n\n\n@pytest.mark.case_name(\"Line chart with logarithmic Y axis\")\ndef test_line_log_y():\n    chart = Chart(\n        type=ChartType.LINE_LOG_Y,\n        title=\"Logarithmic Y Axis\",\n        x_label=\"Linear X\",\n        y_label=\"Log Y\",\n    )\n    x_data = list(range(1, 6))\n    y_data = [10**i for i in range(5)]\n    chart.add_series(x_data=x_data, y_data=y_data, marker_name=\"Exponential\")\n    set_case_chart(chart)\n\n\n@pytest.mark.case_name(\"Double logarithmic chart\")\ndef test_log_x_y():\n    chart = Chart(\n        type=ChartType.LOG_X_Y,\n        title=\"Double Logarithmic Plot\",\n        x_label=\"Log X\",\n        y_label=\"Log Y\",\n    )\n    x_data = [10**i for i in range(1, 6)]\n    y_data = [10**i for i in range(1, 6)]\n    chart.add_series(x_data=x_data, y_data=y_data, marker_name=\"Linear in log-log\")\n    set_case_chart(chart)    \n</code></pre>"},{"location":"examples/couchdb_load/","title":"Couchdb load","text":"<p>This is an example of storing the test result in CouchDB. Test reports are written to the report database at the end of  the testing process via CouchdbLoader. The code for this example can be seen inside the hardpy package  CouchDB Load.</p>"},{"location":"examples/couchdb_load/#how-to-start","title":"how to start","text":"<ol> <li>Launch <code>hardpy init couchdb_load</code>.</li> <li>Launch CouchDB instance.</li> <li>Modify the files described below.</li> <li>Launch <code>hardpy run couchdb_load</code>.</li> </ol>"},{"location":"examples/couchdb_load/#conftestpy","title":"conftest.py","text":"<p>Contains settings and fixtures for all tests:</p> <ul> <li>The function of generating a report and recording it in the database <code>save_report_to_couchdb</code>;</li> <li>The list of actions that will be performed after testing is filled in function <code>fill_actions_after_test</code>;</li> </ul> <pre><code>import pytest\nfrom hardpy import (\n    CouchdbLoader,\n    CouchdbConfig,\n    get_current_report,\n)\n\ndef save_report_to_couchdb():\n    report = get_current_report()\n    if report:\n        loader = CouchdbLoader(CouchdbConfig())\n        loader.load(report)\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef fill_actions_after_test(post_run_functions: list):\n    post_run_functions.append(save_report_to_couchdb)\n    yield\n</code></pre>"},{"location":"examples/couchdb_load/#test_1py","title":"test_1.py","text":"<p>Contains two simple examples of a valid test.</p> <pre><code>def test_one():\n    assert True\n\ndef test_two():\n    assert True\n</code></pre>"},{"location":"examples/couchdb_load/#test_2py","title":"test_2.py","text":"<p>Contains two simple examples: a valid test and an invalid test.</p> <pre><code>def test_three():\n    assert True\n\ndef test_four():\n    raise AssertionError\n</code></pre>"},{"location":"examples/critical_test/","title":"Critical test marker","text":"<p>This is example of using the critical marker in pytest-hardpy to control test execution flow.</p> <p>The @pytest.mark.critical  marker allows to designate tests or entire modules as critical. If a critical test fails or is skipped, all subsequent tests in the current and following modules will be skipped.</p>"},{"location":"examples/critical_test/#how-to-start","title":"how to start","text":"<ol> <li>Launch <code>hardpy init critical_test</code>.</li> <li>Launch CouchDB instance.</li> <li>Modify the files described below.</li> <li>Launch <code>hardpy run critical_test</code>.</li> </ol>"},{"location":"examples/critical_test/#test_criticalpy","title":"test_critical.py","text":"<pre><code>import pytest\n\n@pytest.mark.critical\ndef test_core_feature():\n    assert False  # This will fail\n\ndef test_secondary_feature():\n    assert True  # This will be skipped\n</code></pre> <p>Output:</p> <ul> <li><code>test_core_feature</code>: Failed</li> <li><code>test_secondary_feature</code>: Skipped</li> </ul>"},{"location":"examples/critical_test/#test_module_apy","title":"test_module_a.py","text":"<pre><code>import pytest\n\npytestmark = pytest.mark.critical\n\ndef test_a1():\n    assert False  # Fails\n\ndef test_a2():\n    assert True  # Skipped\n</code></pre>"},{"location":"examples/critical_test/#test_module_bpy","title":"test_module_b.py","text":"<pre><code>def test_b1():\n    assert True  # Skipped because module_a failed\n</code></pre> <p>Output:</p> <ul> <li><code>test_a1</code>: Failed</li> <li><code>test_a2</code>: Skipped</li> <li><code>test_b1</code>: Skipped</li> </ul>"},{"location":"examples/db_in_ram/","title":"Storing CouchDB database in RAM","text":"<p>When using one of the methods below, the database data will be saved to a  folder in temporary storage, and after the computer is turned off, the data will be deleted.</p> <p>Here are instructions for saving the database in RAM for different  methods of launching the database:</p> <ul> <li>in linux with binary packages</li> <li>in linux with docker-compose.yaml</li> <li>in windows with binary packages</li> <li>in windows with docker-compose.yaml</li> </ul>"},{"location":"examples/db_in_ram/#linux","title":"linux","text":""},{"location":"examples/db_in_ram/#binary-packages","title":"binary packages","text":"<p>Start the database according to the  instructions</p> <ul> <li>Create a CouchDB folder in the temporary file storage:</li> </ul> <pre><code>sudo mkdir /dev/shm/couchdb\n</code></pre> <ul> <li>Add editing rights to this folder:</li> </ul> <pre><code>sudo chmod 777 /dev/shm/couchdb\n</code></pre> <ul> <li>Edit the database_dir parameter in the database configuration  file <code>/opt/couchdb/etc/default.ini</code>:</li> </ul> <pre><code>[couchdb]\ndatabase_dir = ./../../dev/shm/couchdb\n</code></pre> <ul> <li>Restart the couchdb service:</li> </ul> <pre><code>sudo service couchdb restart\n</code></pre>"},{"location":"examples/db_in_ram/#docker","title":"docker","text":"<p>Start the database according to the  instructions (steps 1-3)</p> <p>Example of <code>docker-compose.yaml</code> file with saving files in RAM:</p> <pre><code>version: \"3.8\"\n\nservices:\n  couchserver:\n    image: couchdb:3.4\n    ports:\n      - \"5984:5984\"\n    environment:\n      COUCHDB_USER: dev\n      COUCHDB_PASSWORD: dev\n    volumes:\n      - /dev/shm/couchdb:/opt/couchdb/data\n      - ./docker/couchdb.ini:/opt/couchdb/etc/local.ini\n</code></pre> <p>Run docker compose in the root directory to launch DB.</p> <pre><code>docker compose up\n</code></pre> <p>To stop the database, run the command:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"examples/db_in_ram/#windows","title":"windows","text":""},{"location":"examples/db_in_ram/#virtual-hard-disk","title":"virtual hard disk","text":"<p>You need to create a virtual hard disk on which the database will be saved.</p> <ul> <li>Open the command prompt using <code>Win+R</code>, type <code>diskmgmt.msc</code>, and press <code>Enter</code>.</li> <li>Click Action &gt; Create Virtual Hard Disk.</li> <li>Specify any Location, set Size (e.g., 30 MB), choose VHD type and Fixed size.</li> <li>Right-click the created disk (left pane), select Initialize Disk, choose GUID Partition Table.</li> <li>Right-click the created disk (right pane), select New Simple Volume, Assign drive  letter (e.g., <code>K</code>), click Next twice, then Finish.</li> <li>Create a folder named <code>couchdb</code> in the created drive (<code>K</code> in our case) using File Explorer.</li> </ul>"},{"location":"examples/db_in_ram/#binary-packages_1","title":"binary packages","text":"<p>Start the database according to the  instructions</p> <ul> <li>Open the file located at <code>C:/CouchDB/etc/default.ini</code> as administrator.</li> <li>Set the value of the database_dir parameter to <code>K:/couchdb</code>.</li> <li>Save and close the file.</li> <li>Open the Services console using <code>Win+R</code>, type <code>services.msc</code>, and press <code>Enter</code>.</li> <li>Locate the Apache CouchDB service and restart it.</li> </ul>"},{"location":"examples/db_in_ram/#docker_1","title":"docker","text":"<p>Start the database according to the  instructions (steps 1-3)</p> <p>In <code>docker-compose.yaml</code> file in <code>volumes</code> find the string:</p> <pre><code>./docker/dbdata:/opt/couchdb/data\n</code></pre> <p>Replace this string with next value:</p> <pre><code>K:/couchdb:/opt/couchdb/data\n</code></pre> <p>Run docker compose in the root directory to launch DB.</p> <pre><code>docker compose up\n</code></pre> <p>To stop the database, run the command:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"examples/dialog_box/","title":"Dialog box","text":"<p>This is an example of testing dialog boxes using the HardPy library. The code for this example can be seen inside the hardpy package Dialog Box.</p> <p>Contains some examples of valid tests for dialog boxes. To test images, create an <code>assets</code> folder in the <code>dialog_box</code> folder with the image <code>test.png</code>.</p>"},{"location":"examples/dialog_box/#how-to-start","title":"how to start","text":"<ol> <li>Launch <code>hardpy init dialog_box</code>.</li> <li>Launch CouchDB instance.</li> <li>Modify the files described below.</li> <li>Launch <code>hardpy run dialog_box</code>.</li> </ol>"},{"location":"examples/dialog_box/#test_1_base_boxpy","title":"test_1_base_box.py","text":"<pre><code>import pytest\nfrom hardpy import DialogBox, HTMLComponent, ImageComponent, run_dialog_box\n\npytestmark = pytest.mark.module_name(\"Base dialog box\")\n\n@pytest.mark.case_name(\"Empty test before\")\ndef test_before():\n    assert True\n\n@pytest.mark.case_name(\"Base dialog box with image\")\ndef test_base_dialog_box_with_image():\n    dbx = DialogBox(\n        title_bar=\"Operator check\",\n        dialog_text=\"Press the Confirm button\",\n        image=ImageComponent(address=\"assets/test.png\", width=100, border=1),\n        html=HTMLComponent(html=\"https://everypinio.github.io/hardpy/\", width=50, is_raw_html=False),\n        pass_fail=True,\n    )\n    response = run_dialog_box(dbx)\n    assert response.result\n    assert response.data\n\n@pytest.mark.case_name(\"Base dialog box custom button text\")\ndef test_base_dialog_box_custom_button_text():\n    dbx = DialogBox(\n        title_bar=\"Custom button text\",\n        dialog_text=\"Press the 'Some text' button\",\n        button_text=[\"Some text\"],\n    )\n    response = run_dialog_box(dbx)\n    assert response\n\n@pytest.mark.case_name(\"Base dialog box with pass_fail\")\ndef test_base_dialog_box_with_pass_fail():\n    dbx = DialogBox(\n        title_bar=\"Operator check\",\n        dialog_text=\"Press Pass or Fail button\",\n        pass_fail=True,\n    )\n    response = run_dialog_box(dbx)\n    assert response.result\n    assert response.data\n\n\n@pytest.mark.case_name(\"Base dialog box with pass_fail custom buttons text\")\ndef test_base_dialog_box_with_pass_fail_custom_button_text():\n    dbx = DialogBox(\n        title_bar=\"Custom buttons text\",\n        dialog_text=\"Press 'Some text' or 'Some other text' button\",\n        pass_fail=True,\n        button_text=[\"Some text\", \"Some other text\"],\n    )\n    response = run_dialog_box(dbx)\n    assert response.result\n    assert response.data\n</code></pre>"},{"location":"examples/dialog_box/#test_2_input_fieldpy","title":"test_2_input_field.py","text":"<pre><code>import pytest\nfrom hardpy import (\n    DialogBox,\n    ImageComponent,\n    NumericInputWidget,\n    TextInputWidget,\n    run_dialog_box,\n    set_message,\n)\n\npytestmark = pytest.mark.module_name(\"Input field dialog boxes\")\n\n@pytest.mark.case_name(\"Text input with image\")\ndef test_text_input_with_image():\n    dbx = DialogBox(\n        dialog_text=\"Type 'ok' and press the Confirm button\",\n        title_bar=\"Example of text input\",\n        widget=TextInputWidget(),\n        image=ImageComponent(address=\"assets/test.png\", width=50),\n        font_size=18,\n    )\n    response = run_dialog_box(dbx)\n    set_message(f\"Entered text {response}\")\n    assert response == \"ok\", \"The entered text is not correct\"\n\n@pytest.mark.case_name(\"Numeric input\")\ndef test_num_input():\n    test_num = 123\n    dbx = DialogBox(\n        dialog_text=f\"Enter the number {test_num} and press the Confirm button\",\n        title_bar=\"Example of entering a number\",\n        widget=NumericInputWidget(),\n    )\n    response = int(run_dialog_box(dbx))\n    set_message(f\"Entered number {response}\")\n    assert response == test_num, \"The entered number is not correct\"\n</code></pre>"},{"location":"examples/dialog_box/#test_3_choice_controlpy","title":"test_3_choice_control.py","text":"<pre><code>import pytest\nfrom hardpy import (\n    CheckboxWidget,\n    DialogBox,\n    RadiobuttonWidget,\n    run_dialog_box,\n    set_message,\n)\n\npytestmark = pytest.mark.module_name(\"Choice control dialog boxes\")\n\n@pytest.mark.case_name(\"Test dialog box with radiobutton\")\ndef test_radiobutton():\n    dbx = DialogBox(\n        dialog_text='Select item \"one\" out of several and click Confirm.',\n        title_bar=\"Radiobutton example\",\n        widget=RadiobuttonWidget(fields=[\"one\", \"two\", \"three\"]),\n    )\n    response = run_dialog_box(dbx)\n    set_message(f\"Selected item {response}\")\n    assert response == \"one\", \"The answer is not correct\"\n\n@pytest.mark.case_name(\"Test dialog box with checkbox\")\ndef test_checkbox():\n    dbx = DialogBox(\n        dialog_text='Select items \"one\" and \"two\" and click the Confirm button',\n        title_bar=\"Checkbox example\",\n        widget=CheckboxWidget(fields=[\"one\", \"two\", \"three\"]),\n    )\n    response = run_dialog_box(dbx)\n    set_message(f\"Selected item {response}\")\n    correct_answer = {\"one\", \"two\"}\n    assert set(response) == correct_answer, \"The answer is not correct\"\n</code></pre>"},{"location":"examples/dialog_box/#test_4_multiple_stepspy","title":"test_4_multiple_steps.py","text":"<pre><code>import pytest\nfrom hardpy import DialogBox, HTMLComponent, ImageComponent, MultistepWidget, StepWidget, run_dialog_box\n\npytestmark = pytest.mark.module_name(\"Multiple steps dialog box\")\n\n@pytest.mark.case_name(\"Multistep\")\ndef test_multiple_steps():\n    steps = [\n        StepWidget(\"Step 1\", text=\"Content for step\"),\n        StepWidget(\n            \"Step 2\",\n            text=\"Content for step 2\",\n            image=ImageComponent(address=\"assets/test.png\", width=50),\n        ),\n        StepWidget(\n            \"Step 3\",\n            text=None,\n            html=HTMLComponent(html=\"https://everypinio.github.io/hardpy/\", width=50, is_raw_html=False),\n        ),\n    ]\n    dbx = DialogBox(\n        dialog_text=\"Follow the steps and click Confirm\",\n        widget=MultistepWidget(steps),\n    )\n    response = run_dialog_box(dbx)\n    assert response\n</code></pre>"},{"location":"examples/dialog_box/#test_5_htmlpy","title":"test_5_html.py","text":"<pre><code>import pytest\n\nfrom hardpy import DialogBox, HTMLComponent, run_dialog_box\n\npytestmark = pytest.mark.module_name(\"Dialog box with HTML\")\n\n@pytest.mark.case_name(\"Base dialog box with html code\")\ndef test_base_dialog_box_with_html_code():\n    test_html = \"\"\"\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;body&gt;\n\n    &lt;h1&gt;Test HTML Page&lt;/h1&gt;\n\n    &lt;p&gt;It is testing page.&lt;/p&gt;\n    &lt;p&gt;You can put anything on it.&lt;/p&gt;\n\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \"\"\"\n    dbx = DialogBox(\n        title_bar=\"Operator check\",\n        dialog_text=\"Press the Confirm button\",\n        html=HTMLComponent(html=test_html, is_raw_html=True, width=50),\n    )\n    response = run_dialog_box(dbx)\n    assert response\n\n@pytest.mark.case_name(\"Base dialog box with html link\")\ndef test_base_dialog_box_with_html_link():\n    dbx = DialogBox(\n        title_bar=\"Operator check\",\n        dialog_text=\"Press the Confirm button\",\n        html=HTMLComponent(\n            html=\"https://everypinio.github.io/hardpy/\",\n            is_raw_html=False,\n            border=2,\n        ),\n    )\n    response = run_dialog_box(dbx)\n    assert response\n</code></pre>"},{"location":"examples/frontend_remote_access/","title":"Enabling mobile device access to the frontend","text":"<p>To allow other devices to connect to the frontend remotely, follow these steps:</p>"},{"location":"examples/frontend_remote_access/#1-determine-ip-address","title":"1. Determine IP address","text":"<p>Determine your computer's IP address. For Linux, you can use the <code>ifconfig</code> command:</p> <pre><code>ifconfig | grep \"inet \"\n</code></pre> <p>Look for an address in the format <code>192.168.x.x</code> or <code>10.x.x.x</code> (this is your local network IP).</p>"},{"location":"examples/frontend_remote_access/#2-configure-ports-in-project-files","title":"2. Configure ports in project files","text":""},{"location":"examples/frontend_remote_access/#hardpytoml-configuration","title":"<code>hardpy.toml</code> configuration","text":"<p>Edit the file to include your computer's network address:</p> <pre><code>[database]\nuser = \"dev\"\npassword = \"dev\"\nhost = \"{YOUR_COMPUTER_NETWORK_ADDRESS}\"  # Replace with address from step 2\nport = 5984\n\n[frontend]\nhost = \"{YOUR_COMPUTER_NETWORK_ADDRESS}\"  # Replace with the address from step 2 or insert \"0.0.0.0\"\nport = 8000\nlanguage = \"en\"\n</code></pre>"},{"location":"examples/frontend_remote_access/#3-launch-the-frontend","title":"3. Launch the frontend","text":"<p>You can start the frontend using <code>hardpy run</code>.</p>"},{"location":"examples/frontend_remote_access/#verification-steps","title":"Verification steps","text":"<ol> <li>On your mobile device, ensure it's connected to the same network as your computer</li> <li>Open a browser and navigate to: <code>http://[YOUR_COMPUTER_ADDRESS]:8000</code></li> <li>Verify you can see the frontend interface</li> </ol>"},{"location":"examples/frontend_remote_access/#troubleshooting","title":"Troubleshooting","text":"<p>If connection fails:</p> <ul> <li>Open required ports on your computer.</li> <li>Check all services are running (CouchDB, frontend).</li> <li>Ensure no other devices are using the same ports.</li> <li>Verify your mobile device and computer are on the same network.</li> </ul>"},{"location":"examples/frontend_remote_access/#open-required-ports-on-your-computer","title":"Open required ports on your computer","text":""},{"location":"examples/frontend_remote_access/#for-linux-using-ufw-firewall","title":"For Linux (using <code>ufw</code> firewall)","text":"<p>Run the following commands in your terminal:</p> <pre><code>sudo ufw allow 5984  # Allows CouchDB connections\nsudo ufw allow 8000  # Allows frontend access\nsudo ufw enable     # Enable the firewall if not already active\nsudo ufw status     # Verify the ports are open\n</code></pre>"},{"location":"examples/hardpy_launch/","title":"HardPy launch","text":"<p>HardPy can be started from either the operator panel or the terminal. Below we will look at all the launch options.</p>"},{"location":"examples/hardpy_launch/#how-to-start","title":"how to start","text":"<p>Initialize the HardPy project:</p> <ol> <li>Launch <code>hardpy init test_project</code>.</li> <li>Launch CouchDB instance.</li> </ol>"},{"location":"examples/hardpy_launch/#launch-options","title":"launch options","text":""},{"location":"examples/hardpy_launch/#1-operator-panel","title":"1. Operator panel","text":"<ol> <li> <p>Launch operator panel:    <pre><code>hardpy run test_project\n</code></pre></p> </li> <li> <p>Open the operator panel in your browser at: http://localhost:8000/.</p> </li> <li>Click the Start button.</li> </ol>"},{"location":"examples/hardpy_launch/#2-start-command","title":"2. Start command","text":"<ol> <li>Launch operator panel:    <pre><code>hardpy run test_project\n</code></pre></li> <li>Run the tests by executing the following command in the terminal:     <pre><code>hardpy start test_project\n</code></pre></li> </ol>"},{"location":"examples/hardpy_launch/#3-pytest-in-console","title":"3. Pytest in console","text":"<p>Run the tests by executing the following command in the terminal:</p> <pre><code>pytest test_project\n</code></pre>"},{"location":"examples/hardpy_launch/#31-pytest-in-console-with-launching-operator-panel","title":"3.1. Pytest in console with launching operator panel","text":"<p>If the operator panel is running, the tests will start after the command in the terminal in the same way as by clicking on the <code>Start</code> button.</p>"},{"location":"examples/hardpy_launch/#32-pytest-in-console-without-launching-operator-panel","title":"3.2. Pytest in console without launching operator panel","text":"<p>If the operator panel has not been started, the tests will also run, but without visualization.</p> Warning <p>This method is only appropriate if you are not using any operator panel features such as operator messages and dialog boxes.</p>"},{"location":"examples/hardpy_launch/#4-ide","title":"4. IDE","text":"<p>An example of starting in vscode is as follows. If you use other IDEs, explore the possibility of running tests in your environment.</p> <ol> <li>Open the Testing extension in VS Code.</li> <li>Run the tests through the extension interface.</li> </ol>"},{"location":"examples/hardpy_launch/#41-pytest-in-testing-with-launching-operator-panel","title":"4.1. Pytest in Testing with launching operator panel","text":"<p>If the operator panel is running, the tests will start after launching in Testing in the same way as by clicking on the <code>Start</code> button.</p>"},{"location":"examples/hardpy_launch/#42-pytest-in-testing-without-launching-operator-panel","title":"4.2. Pytest in Testing without launching operator panel","text":"<p>If the operator panel has not been started, the tests will also run, but without visualization.</p> Warning <p>This method is only appropriate if you are not using any operator panel features such as operator messages and dialog boxes.</p>"},{"location":"examples/hello_hardpy/","title":"Hello hardpy","text":"<p>This is the simplest example of using HardPy with CouchDB. The code for this example can be seen inside the hardpy package Hello Hardpy.</p>"},{"location":"examples/hello_hardpy/#how-to-start","title":"how to start","text":"<ol> <li>Launch <code>hardpy init hello_hardpy</code>.</li> <li>Launch CouchDB instance.</li> <li>Launch <code>hardpy run hello_hardpy</code>.</li> </ol>"},{"location":"examples/hello_hardpy/#test_1py","title":"test_1.py","text":"<p>Contains the simplest example of a valid test.</p> <pre><code>def test_one():\n    assert True\n</code></pre>"},{"location":"examples/json_storage/","title":"JSON storage","text":"<p>This is an example of using pytest-hardpy functions, storing the result to JSON file. The code for this example can be seen inside the hardpy package JSON storage.</p>"},{"location":"examples/json_storage/#how-to-start","title":"how to start","text":"<ol> <li>Launch <code>hardpy init --no-create-database --storage-type json json_storage</code>.</li> <li>Modify the files described below.</li> <li>Launch <code>hardpy run json_storage</code>.</li> </ol>"},{"location":"examples/json_storage/#hardpytoml","title":"hardpy.toml","text":"<p>Replace the settings in the <code>[frontend]</code> and <code>[frontend.modal_result]</code> sections  with those shown in the hardpy.toml example file below.</p> <pre><code>title = \"HardPy JSON Storage Demo\"\ntests_name = \"Device Test Suite\"\n\n[database]\nstorage_type = \"json\"\nstorage_path = \"result\"\n\n[frontend]\nhost = \"localhost\"\nport = 8000\nlanguage = \"en\"\nsound_on = false\nfull_size_button = false\nmanual_collect = false\nmeasurement_display = true\n\n[frontend.modal_result]\nenable = true\nauto_dismiss_pass = true\nauto_dismiss_timeout = 5\n</code></pre>"},{"location":"examples/json_storage/#conftestpy","title":"conftest.py","text":"<pre><code>from pathlib import Path\nimport pytest\n\nfrom hardpy import JsonLoader, get_current_report\n\n\n@pytest.fixture(scope=\"session\")\ndef setup_test_environment():\n    \"\"\"Set up test environment before all tests.\"\"\"\n    print(\"\\n=== Setting up test environment ===\")\n    # Add any global setup here\n    yield\n    print(\"\\n=== Tearing down test environment ===\")\n    # Add any global cleanup here\n\n\n@pytest.fixture(scope=\"function\")\ndef test_device():\n    \"\"\"Fixture providing simulated test device.\"\"\"\n\n    class TestDevice:\n        def __init__(self):\n            self.connected = False\n            self.voltage = 5.0\n            self.current = 0.0\n\n        def connect(self):\n            self.connected = True\n            return True\n\n        def disconnect(self):\n            self.connected = False\n\n        def measure_voltage(self):\n            return self.voltage\n\n        def measure_current(self):\n            return self.current\n\n    device = TestDevice()\n    device.connect()\n\n    yield device\n\n    device.disconnect()\n\n\ndef save_report_to_dir():\n    report = get_current_report()\n    if report:\n        loader = JsonLoader(Path.cwd() / \"reports\")\n        loader.load(report)\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef fill_actions_after_test(post_run_functions: list):\n    post_run_functions.append(save_report_to_dir)\n    yield\n</code></pre>"},{"location":"examples/json_storage/#test_chart_demopy","title":"test_chart_demo.py","text":"<pre><code>import hardpy\nimport pytest\nimport math\n\n@pytest.mark.case_name(\"Sine Wave Analysis\")\n@pytest.mark.module_name(\"Chart Demonstrations\")\ndef test_sine_wave():\n    \"\"\"Test generating and analyzing a sine wave.\"\"\"\n    hardpy.set_message(\"Generating sine wave data...\")\n\n    # Generate sine wave data\n    x_data = []\n    y_data = []\n\n    for i in range(100):\n        x = i / 10.0  # 0 to 10\n        y = math.sin(x)\n        x_data.append(x)\n        y_data.append(y)\n\n    # Create chart\n    chart = hardpy.Chart(\n        title=\"Sine Wave\",\n        x_label=\"Time\",\n        y_label=\"Amplitude\",\n        type=hardpy.ChartType.LINE,\n    )\n    chart.add_series(x_data, y_data, \"Sine Wave\")\n\n    hardpy.set_case_chart(chart)\n\n    # Verify amplitude\n    max_amplitude = max(y_data)\n    min_amplitude = min(y_data)\n    peak_to_peak = max_amplitude - min_amplitude\n\n    hardpy.set_case_measurement(\n        hardpy.NumericMeasurement(\n            name=\"Peak-to-Peak Amplitude\",\n            value=peak_to_peak,\n            unit=\"V\",\n            lower_limit=1.9,\n            upper_limit=2.1,\n        )\n    )\n\n    hardpy.set_message(f\"Peak-to-peak amplitude: {peak_to_peak:.3f}V\")\n\n    assert 1.9 &lt;= peak_to_peak &lt;= 2.1, \"Amplitude out of range\"\n\n@pytest.mark.case_name(\"Temperature Curve\")\n@pytest.mark.module_name(\"Chart Demonstrations\")\ndef test_temperature_curve():\n    \"\"\"Test temperature rise curve.\"\"\"\n    hardpy.set_message(\"Recording temperature curve...\")\n\n    # Simulate temperature rise\n    time_data = []\n    temp_data = []\n\n    for i in range(50):\n        time = i * 2  # seconds\n        # Exponential rise to 80\u00b0C\n        temp = 25 + 55 * (1 - math.exp(-i / 20))\n        time_data.append(time)\n        temp_data.append(temp)\n\n    # Create chart\n    chart = hardpy.Chart(\n        title=\"Temperature Rise Curve\",\n        x_label=\"Time (seconds)\",\n        y_label=\"Temperature (\u00b0C)\",\n        type=hardpy.ChartType.LINE,\n    )\n    chart.add_series(time_data, temp_data, \"Temperature\")\n\n    hardpy.set_case_chart(chart)\n\n    # Check final temperature\n    final_temp = temp_data[-1]\n\n    hardpy.set_case_measurement(\n        hardpy.NumericMeasurement(\n            name=\"Final Temperature\",\n            value=final_temp,\n            unit=\"\u00b0C\",\n            upper_limit=85,\n        )\n    )\n\n    hardpy.set_message(f\"Final temperature: {final_temp:.1f}\u00b0C\")\n\n    assert final_temp &lt; 85, f\"Temperature too high: {final_temp}\u00b0C\"\n</code></pre>"},{"location":"examples/json_storage/#test_communicationpy","title":"test_communication.py","text":"<pre><code>import hardpy\nimport pytest\nfrom time import sleep\n\n@pytest.mark.case_name(\"Serial Port Connection\")\n@pytest.mark.module_name(\"Communication Tests\")\ndef test_serial_connection():\n    \"\"\"Test serial port connection.\"\"\"\n    hardpy.set_message(\"Testing serial port connection...\")\n\n    # Simulate connection\n    port = \"/dev/ttyUSB0\"\n    baudrate = 115200\n\n    hardpy.set_instrument(\n        hardpy.Instrument(\n            name=\"Serial Port\",\n            comment=f\"{port} @ {baudrate} baud\"\n        )\n    )\n\n    # Simulate successful connection\n    connection_ok = True\n\n    hardpy.set_message(f\"Connected to {port} at {baudrate} baud\")\n\n    assert connection_ok, \"Failed to establish serial connection\"\n\n@pytest.mark.case_name(\"Data Transfer Test\")\n@pytest.mark.module_name(\"Communication Tests\")\n@pytest.mark.attempt(3)  # Allow 2 retries\ndef test_data_transfer():\n    \"\"\"Test data transfer over serial.\"\"\"\n    hardpy.set_message(\"Testing data transfer...\")\n\n    # Simulate sending and receiving data\n    sent_bytes = 1024\n    received_bytes = 1024\n    transfer_time = 0.5  # seconds\n\n    # Calculate transfer rate\n    transfer_rate = (sent_bytes + received_bytes) / transfer_time\n\n    hardpy.set_case_measurement(\n        hardpy.NumericMeasurement(\n            name=\"Transfer Rate\",\n            value=transfer_rate,\n            unit=\"bytes/s\",\n            lower_limit=1000,\n        )\n    )\n\n    hardpy.set_message(f\"Transfer rate: {transfer_rate:.0f} bytes/s\")\n\n    assert received_bytes == sent_bytes, \"Data integrity error\"\n    assert transfer_rate &gt; 1000, \"Transfer rate too slow\"\n\n@pytest.mark.case_name(\"Protocol Validation\")\n@pytest.mark.module_name(\"Communication Tests\")\n@pytest.mark.critical  # Critical test - stops all if fails\ndef test_protocol_validation():\n    \"\"\"Test communication protocol validation.\"\"\"\n    hardpy.set_message(\"Validating communication protocol...\")\n\n    # Simulate protocol check\n    protocol_version = \"v2.1\"\n    expected_version = \"v2.1\"\n\n    hardpy.set_case_measurement(\n        hardpy.StringMeasurement(\n            name=\"Protocol Version\",\n            value=protocol_version,\n            comparison_value=expected_version,\n        )\n    )\n\n    hardpy.set_message(f\"Protocol version: {protocol_version}\")\n\n    assert protocol_version == expected_version, \\\n        f\"Protocol mismatch: got {protocol_version}, expected {expected_version}\"\n\n@pytest.mark.case_name(\"Error Handling Test\")\n@pytest.mark.module_name(\"Communication Tests\")\n@pytest.mark.dependency(\"test_communication::test_protocol_validation\")\ndef test_error_handling():\n    \"\"\"Test error handling (depends on protocol validation).\"\"\"\n    hardpy.set_message(\"Testing error handling...\")\n\n    # Simulate error injection and recovery\n    errors_injected = 5\n    errors_handled = 5\n\n    hardpy.set_case_measurement(\n        hardpy.NumericMeasurement(\n            name=\"Errors Handled\",\n            value=errors_handled,\n            unit=\"count\",\n            comparison_value=errors_injected,\n        )\n    )\n\n    hardpy.set_message(f\"Handled {errors_handled}/{errors_injected} errors\")\n\n    assert errors_handled == errors_injected, \"Some errors not handled correctly\"\n</code></pre>"},{"location":"examples/json_storage/#test_voltagepy","title":"test_voltage.py","text":"<pre><code>import hardpy\nimport pytest\nfrom time import sleep\n\n@pytest.mark.case_name(\"Check Power Supply Voltage\")\n@pytest.mark.module_name(\"Power Supply Tests\")\ndef test_power_supply_voltage():\n    \"\"\"Test that power supply outputs correct voltage.\"\"\"\n    # Set test stand information\n    hardpy.set_stand_name(\"Test Bench #1\")\n    hardpy.set_stand_location(\"Lab A\")\n\n    # Set device under test information\n    hardpy.set_dut_serial_number(\"PSU-12345\")\n    hardpy.set_dut_name(\"Power Supply Unit\")\n    hardpy.set_dut_type(\"DC Power Supply\")\n\n    # Simulate voltage measurement\n    expected_voltage = 5.0\n    measured_voltage = 5.02  # Simulated measurement\n    tolerance = 0.1\n\n    # Record measurement\n    hardpy.set_case_measurement(\n        hardpy.NumericMeasurement(\n            name=\"Output Voltage\",\n            value=measured_voltage,\n            unit=\"V\",\n            lower_limit=expected_voltage - tolerance,\n            upper_limit=expected_voltage + tolerance,\n        )\n    )\n\n    # Add message\n    hardpy.set_message(f\"Measured voltage: {measured_voltage}V (expected: {expected_voltage}V)\")\n\n    # Verify voltage is within tolerance\n    assert abs(measured_voltage - expected_voltage) &lt;= tolerance, \\\n        f\"Voltage out of tolerance: {measured_voltage}V\"\n\n@pytest.mark.case_name(\"Check Current Limit\")\n@pytest.mark.module_name(\"Power Supply Tests\")\ndef test_current_limit():\n    \"\"\"Test that power supply has correct current limit.\"\"\"\n    # Simulate current limit test\n    expected_limit = 3.0\n    measured_limit = 3.05  # Simulated measurement\n    tolerance = 0.2\n\n    # Record measurement\n    hardpy.set_case_measurement(\n        hardpy.NumericMeasurement(\n            name=\"Current Limit\",\n            value=measured_limit,\n            unit=\"A\",\n            lower_limit=expected_limit - tolerance,\n            upper_limit=expected_limit + tolerance,\n        )\n    )\n\n    hardpy.set_message(f\"Current limit: {measured_limit}A\")\n\n    assert abs(measured_limit - expected_limit) &lt;= tolerance\n\n@pytest.mark.case_name(\"Voltage Stability Test\")\n@pytest.mark.module_name(\"Power Supply Tests\")\n@pytest.mark.attempt(2)  # Retry once if fails\ndef test_voltage_stability():\n    \"\"\"Test voltage stability over time.\"\"\"\n    hardpy.set_message(\"Testing voltage stability over 5 seconds...\")\n\n    voltage_readings = []\n    for i in range(5):\n        # Simulate reading voltage\n        voltage = 5.0 + (i * 0.01)  # Slight increase\n        voltage_readings.append(voltage)\n        sleep(0.1)  # Simulate measurement delay\n\n    max_variation = max(voltage_readings) - min(voltage_readings)\n\n    # Record measurement\n    hardpy.set_case_measurement(\n        hardpy.NumericMeasurement(\n            name=\"Voltage Variation\",\n            value=max_variation,\n            unit=\"V\",\n            upper_limit=0.1,\n        )\n    )\n\n    hardpy.set_message(f\"Max voltage variation: {max_variation:.3f}V\")\n\n    assert max_variation &lt; 0.1, f\"Voltage not stable: {max_variation}V variation\"\n</code></pre>"},{"location":"examples/launch_arguments/","title":"Launch arguments","text":"<p>HardPy launches pytest tests. HardPy supports two methods for passing parameters to tests.  The recommended method is the start arguments functionality. The other method is writing your own addoption pytest approach.</p>"},{"location":"examples/launch_arguments/#start-arguments","title":"Start arguments","text":"<p>The hardpy_start_args  fixture allows you to pass dynamic parameters to your test runs using the <code>--arg</code>  option (CLI) or --hardpy-start-arg option (pytest). This is particularly useful for configuring tests at runtime without changing the code.</p> <p>Contains examples of how to use dynamic start arguments in tests.</p>"},{"location":"examples/launch_arguments/#how-to-start","title":"how to start","text":"<ol> <li>Launch <code>hardpy init start_arguments</code>.</li> <li>Launch CouchDB instance.</li> <li>Modify the files described below.</li> <li> <p>Launch tests with dynamic arguments:</p> <pre><code>hardpy run start_arguments\n\n# Using hardpy command\nhardpy start start_arguments --arg test_mode=debug --arg device_id=DUT-007 --arg retry_count=3\n\n# Or using pytest directly\npytest --hardpy-start-arg test_mode=debug --hardpy-start-arg device_id=DUT-007 --hardpy-start-arg retry_count=3\n</code></pre> <p>Alternatively, you can specify start arguments in the <code>pytest.ini</code> file. This is useful for setting default arguments that are always used when running tests.</p> <pre><code>[pytest]\naddopts = --hardpy-pt\n        --hardpy-db-url http://dev:dev@localhost:5984/\n        --hardpy-start-arg test_mode=debug\n        --hardpy-start-arg device_id=DUT-007\n</code></pre> </li> </ol>"},{"location":"examples/launch_arguments/#test_1py","title":"test_1.py","text":"<pre><code>import pytest\nimport hardpy\n\ndef test_with_start_args(hardpy_start_args):\n\n    if hardpy_start_args.get(\"test_mode\") == \"debug\":\n        hardpy.set_message(\"Running in debug mode\")\n\n    device_id = hardpy_start_args.get(\"device_id\")\n    if device_id:\n        hardpy.set_message(f\"Testing device: {device_id}\")\n        hardpy.set_case_artifact({\"device_id\": device_id})\n    else:\n        hardpy.set_message(\"No device ID provided\")\n</code></pre>"},{"location":"examples/launch_arguments/#addoption","title":"Addoption","text":"<p>Alternative approach using pytest's built-in <code>addoption</code> method for adding custom options. You can read more about it in the  pytest documentation.</p>"},{"location":"examples/launch_arguments/#how-to-start_1","title":"how to start","text":"<ol> <li>Launch <code>hardpy init launch_arg</code>.</li> <li>Launch CouchDB instance.</li> <li>Modify the files described below.</li> <li>Launch <code>hardpy run launch_arg</code>.</li> </ol>"},{"location":"examples/launch_arguments/#conftestpy","title":"conftest.py","text":"<pre><code>def pytest_addoption(parser):\n    parser.addoption(\"--my-opt\", action=\"store\", help=\"add my opt\")\n\n@pytest.fixture(scope=\"session\")\ndef my_opt(request):\n    return request.config.getoption(\"--my-opt\")\n</code></pre>"},{"location":"examples/launch_arguments/#test_1py_1","title":"test_1.py","text":"<pre><code>def test_custom_option_1(request):\n    custom_value = request.config.getoption(\"--my-opt\")\n    print(f\"Custom option value: {custom_value}\")\n    assert custom_value == \"hello\"\n\ndef test_custom_option_2(my_opt):\n    print(f\"Custom option value: {my_opt}\")\n    assert my_opt == \"hello\"\n</code></pre>"},{"location":"examples/launch_arguments/#launch-test-with-your-parameter","title":"launch test with your parameter","text":"<p>You can launch tests with this command:</p> <pre><code>pytest --my-opt hello\n</code></pre> <p>Alternatively, you can add the parameter <code>--my-opt hello</code> to the <code>pytest.ini</code> file.</p>"},{"location":"examples/logging/","title":"Logging in tests","text":"<p>How to add logging to your tests using Python's built-in <code>logging</code> package.</p> <p>Python's built-in <code>logging</code> package provides a flexible way to add logging to your tests, which can help with debugging and monitoring test execution.</p> <p>Logging in tests is valuable because it:</p> <ul> <li>Provides visibility into test execution flow</li> <li>Helps diagnose failures by capturing context</li> <li>Creates an audit trail of test operations</li> <li>Enables performance monitoring</li> <li>Facilitates troubleshooting in CI/CD pipelines</li> </ul> <p>The logs will be written to:</p> <ul> <li>Console output</li> <li>CI/CD system logs if running in a pipeline</li> </ul> <p>While Python's <code>logging</code> package is versatile, HardPy offers additional specialized logging methods:</p> <ul> <li>Database Logging with set_message</li> <li>Interactive Dialogs with run_dialog_box </li> <li>Operator Messages with set_operator_message</li> </ul>"},{"location":"examples/logging/#how-to-implement-logging","title":"How to implement logging","text":""},{"location":"examples/logging/#basic-setup","title":"Basic setup","text":"<ol> <li> <p>Import the logging module where needed:</p> <pre><code>import logging\n</code></pre> </li> <li> <p>Add a fixture to <code>conftest.py</code>:</p> <pre><code>@pytest.fixture(scope=\"module\")\ndef module_log(request: pytest.FixtureRequest):\n    log_name = request.module.__name__\n    yield logging.getLogger(log_name)\n</code></pre> </li> </ol>"},{"location":"examples/logging/#using-the-logger","title":"Using the logger","text":"<p>You can call logging commands anywhere in your tests:</p> <pre><code>self._log.warning(\"Saving user credentials for registration test case\")\n</code></pre> <p>Example of logging in a test function:</p> <pre><code>def test_log(module_log: logging.Logger):\n    module_log.info(\"DUT serial number logged\")\n    assert True\n</code></pre>"},{"location":"examples/logging/#configuration","title":"Configuration","text":"<p>To configure logging parameters, add these settings to your <code>pytest.ini</code> file:</p> <pre><code>[pytest]\nlog_cli = true\nlog_cli_level = INFO\nlog_cli_format = %(asctime)s [%(levelname)s] %(message)s\nlog_cli_date_format = %H:%M:%S\n</code></pre> <p>You can find the description in How to manage logging</p>"},{"location":"examples/logging/#explanation-of-configuration-options","title":"Explanation of configuration options","text":"<ul> <li><code>log_cli = true</code>: Enables logging output during test execution</li> <li><code>log_cli_level = INFO</code>: Sets the minimum logging level to display</li> <li><code>log_cli_format</code>: Defines the format of log messages:</li> <li><code>%(asctime)s</code>: Timestamp</li> <li><code>[%(levelname)s]</code>: Log level (INFO, WARNING, etc.)</li> <li><code>%(message)s</code>: The actual log message</li> <li><code>log_cli_date_format = %H:%M:%S</code>: Sets the timestamp format to hours:minutes:seconds</li> </ul>"},{"location":"examples/measurement/","title":"Measurement","text":"<p>This is an example of how measurements can be saved in tests.</p> <p>The set_case_measurement function allows  to save measurement result to database. The code for this example can be seen inside the hardpy package Measurement.</p>"},{"location":"examples/measurement/#how-to-start","title":"how to start","text":"<ol> <li>Launch <code>hardpy init measurement</code>.</li> <li>Launch CouchDB instance.</li> <li>Modify the files described below.</li> <li>Launch <code>hardpy run measurement</code>.</li> </ol>"},{"location":"examples/measurement/#test_1py","title":"test_1.py","text":"<pre><code>import pytest\n\nfrom hardpy import ComparisonOperation as CompOp, NumericMeasurement, set_case_measurement\n\npytestmark = pytest.mark.module_name(\"Numeric measurement\")\n\n\n@pytest.mark.case_name(\"Basic numeric operations\")\ndef test_basic_numeric_operations():\n    meas0 = NumericMeasurement(value=1.0)\n    set_case_measurement(meas0)\n\n    meas1 = NumericMeasurement(value=10, operation=CompOp.EQ, comparison_value=10)\n    set_case_measurement(meas1)\n\n    meas2 = NumericMeasurement(value=9, operation=CompOp.EQ, comparison_value=10)\n    set_case_measurement(meas2)\n\n    meas3 = NumericMeasurement(value=5, operation=CompOp.NE, comparison_value=3)\n    set_case_measurement(meas3)\n\n    meas4 = NumericMeasurement(value=7, operation=CompOp.GELE, lower_limit=5, upper_limit=10)\n    set_case_measurement(meas4)\n\n    assert meas1.result\n    assert meas2.result, \"value is not equal to 10\"\n    assert meas3.result\n    assert meas4.result\n\n\n@pytest.mark.case_name(\"Advanced numeric measurements\")\ndef test_advanced_numeric_measurements():\n    voltage_meas = NumericMeasurement(\n        name=\"Voltage\",\n        value=1.2,\n        unit=\"V\",\n        operation=CompOp.GELE,\n        lower_limit=1.0,\n        upper_limit=1.5,\n    )\n    set_case_measurement(voltage_meas)\n\n    temperature_meas = NumericMeasurement(\n        name=\"Temperature\",\n        value=23.5,\n        unit=\"\u00b0C\",\n        operation=CompOp.GELE,\n        lower_limit=20,\n        upper_limit=25,\n    )\n    set_case_measurement(temperature_meas)\n\n    percentage_meas = NumericMeasurement(\n        value=98.6,\n        unit=\"%\",\n        operation=CompOp.GE,\n        comparison_value=95,\n    )\n    set_case_measurement(percentage_meas)\n\n    minutes_meas = NumericMeasurement(\n        name=\"Time\",\n        value=30,\n        unit=\"\u2032\",\n        operation=CompOp.EQ,\n        comparison_value=30,\n    )\n    set_case_measurement(minutes_meas)\n\n    assert all(\n        [\n            voltage_meas.result,\n            temperature_meas.result,\n            percentage_meas.result,\n            minutes_meas.result,\n        ],\n    )\n\n\n@pytest.mark.case_name(\"Failed numeric measurements\")\ndef test_failed_numeric_measurements():\n    high_voltage_meas = NumericMeasurement(\n        name=\"High Voltage\",\n        value=15.0,\n        unit=\"V\",\n        operation=CompOp.LE,\n        comparison_value=12.0,\n    )\n    set_case_measurement(high_voltage_meas)\n\n    low_temp_meas = NumericMeasurement(\n        name=\"Low Temp\",\n        value=-10,\n        unit=\"\u00b0C\",\n        operation=CompOp.GELE,\n        lower_limit=0,\n        upper_limit=50,\n    )\n    set_case_measurement(low_temp_meas)\n\n    raw_meas = NumericMeasurement(name=\"Raw Value\", value=42.0)\n    set_case_measurement(raw_meas)\n\n    rad_meas = NumericMeasurement(value=3.14, unit=\"rad\")\n    set_case_measurement(rad_meas)\n\n    assert not high_voltage_meas.result, \"Voltage too high\"\n    assert not low_temp_meas.result, \"Temperature too low\"\n</code></pre>"},{"location":"examples/measurement/#test_2py","title":"test_2.py","text":"<pre><code>import pytest\n\nfrom hardpy import (\n    ComparisonOperation as CompOp,\n    StringMeasurement,\n    set_case_measurement,\n)\n\npytestmark = pytest.mark.module_name(\"String measurement\")\n\n\n@pytest.mark.case_name(\"Basic string operations\")\ndef test_basic_string_operations():\n    meas1 = StringMeasurement(\n        value=\"1.2.1\",\n        operation=CompOp.EQ,\n        comparison_value=\"1.2.1\",\n    )\n    set_case_measurement(meas1)\n\n    meas2 = StringMeasurement(\n        value=\"ABC\",\n        operation=CompOp.EQ,\n        comparison_value=\"abc\",\n    )\n    set_case_measurement(meas2)\n\n    meas3 = StringMeasurement(\n        value=\"ABC\",\n        operation=CompOp.EQ,\n        casesensitive=False,\n        comparison_value=\"abc\",\n    )\n    set_case_measurement(meas3)\n\n    assert meas1.result\n    assert not meas2.result\n    assert meas3.result\n\n\n@pytest.mark.case_name(\"Advanced string measurements\")\ndef test_advanced_string_measurements():\n    simple_str_meas = StringMeasurement(\n        value=\"abc\", operation=CompOp.EQ, comparison_value=\"abc\"\n    )\n    set_case_measurement(simple_str_meas)\n\n    serial_meas = StringMeasurement(\n        name=\"Serial Number\",\n        value=\"SN123456\",\n        operation=CompOp.NE,\n        comparison_value=\"SN000000\",\n    )\n    set_case_measurement(serial_meas)\n\n    failed_str_meas = StringMeasurement(\n        name=\"Wrong Code\", value=\"FAIL\", operation=CompOp.EQ, comparison_value=\"PASS\"\n    )\n    set_case_measurement(failed_str_meas)\n\n    info_meas = StringMeasurement(value=\"INFO\")\n    set_case_measurement(info_meas)\n\n    assert simple_str_meas.result\n    assert serial_meas.result\n    assert not failed_str_meas.result, \"Wrong status code\"\n</code></pre>"},{"location":"examples/measurement/#test_3py","title":"test_3.py","text":"<pre><code>import pytest\n\nfrom hardpy import (\n    ComparisonOperation as CompOp,\n    NumericMeasurement,\n    set_case_measurement,\n)\n\npytestmark = pytest.mark.module_name(\"All comparison operations test\")\n\n\n@pytest.mark.case_name(\"Operation types with brackets\")\ndef test_all_operation_types_with_brackets():\n    meas_lt = NumericMeasurement(value=4, operation=CompOp.LT, comparison_value=5)\n    set_case_measurement(meas_lt)\n\n    meas_gelt = NumericMeasurement(\n        value=3,\n        operation=CompOp.GELT,\n        lower_limit=2,\n        upper_limit=4,\n    )\n    set_case_measurement(meas_gelt)\n\n    meas_legt = NumericMeasurement(\n        value=1,\n        operation=CompOp.LEGT,\n        lower_limit=2,\n        upper_limit=4,\n    )\n    set_case_measurement(meas_legt)\n\n    assert meas_lt.result\n    assert meas_gelt.result\n    assert meas_legt.result\n\n\n@pytest.mark.case_name(\"Edge cases with units\")\ndef test_edge_cases_with_units():\n    meas_percent = NumericMeasurement(\n        name=\"Efficiency\",\n        value=95.5,\n        unit=\"%\",\n        operation=CompOp.GELE,\n        lower_limit=90,\n        upper_limit=100,\n    )\n    set_case_measurement(meas_percent)\n\n    meas_degrees = NumericMeasurement(\n        name=\"Angle\",\n        value=45,\n        unit=\"\u00b0\",\n        operation=CompOp.GTLT,\n        lower_limit=0,\n        upper_limit=90,\n    )\n    set_case_measurement(meas_degrees)\n\n    meas_minutes = NumericMeasurement(\n        name=\"Duration\",\n        value=30,\n        unit=\"\u2032\",\n        operation=CompOp.LE,\n        comparison_value=60,\n    )\n    set_case_measurement(meas_minutes)\n\n    meas_voltage = NumericMeasurement(\n        name=\"Voltage\",\n        value=3.3,\n        unit=\"V\",\n        operation=CompOp.GELT,\n        lower_limit=3.0,\n        upper_limit=3.6,\n    )\n    set_case_measurement(meas_voltage)\n\n    assert meas_percent.result\n    assert meas_degrees.result\n    assert meas_minutes.result\n    assert meas_voltage.result\n\n\n@pytest.mark.case_name(\"Boundary value tests\")\ndef test_boundary_value_tests():\n    meas_gt_boundary = NumericMeasurement(\n        value=2.1,\n        operation=CompOp.GTLT,\n        lower_limit=2,\n        upper_limit=4,\n    )\n    set_case_measurement(meas_gt_boundary)\n\n    meas_ge_boundary = NumericMeasurement(\n        value=2,\n        operation=CompOp.GELE,\n        lower_limit=2,\n        upper_limit=4,\n    )\n    set_case_measurement(meas_ge_boundary)\n\n    meas_mixed_boundary = NumericMeasurement(\n        value=4,\n        operation=CompOp.GTLE,\n        lower_limit=2,\n        upper_limit=4,\n    )\n    set_case_measurement(meas_mixed_boundary)\n\n    assert meas_gt_boundary.result\n    assert meas_ge_boundary.result\n    assert meas_mixed_boundary.result\n</code></pre>"},{"location":"examples/minimal_example/","title":"Minimal example","text":"<p>This is the simplest example of using HardPy. This code shows how to convert any pytest tests into a HardPy project.  To achieve this, a <code>hardpy.toml</code> file containing the minimum required information must be added.  Data from this project will be saved in a JSON document in the .hardpy/runstore directory.</p> <p>The code for this example can be seen inside the hardpy package Minimal example.</p>"},{"location":"examples/minimal_example/#how-to-start","title":"how to start","text":"<ol> <li>Install the hardpy from pypi:    <pre><code>pip install hardpy\n</code></pre></li> <li>Copy the hardpy.toml file to the tests directory.</li> <li>Launch HardPy:    <pre><code>hardpy run &lt;tests_directory&gt;\n</code></pre></li> <li>Open <code>http://localhost:8000/</code> </li> </ol>"},{"location":"examples/minimal_example/#test_1py","title":"test_1.py","text":"<pre><code>def test_one():\n    assert True\n</code></pre>"},{"location":"examples/minimal_example/#hardpytoml","title":"hardpy.toml","text":"<pre><code>[database]\nstorage_type = \"json\"\n\n[frontend]\nhost = \"localhost\"\nport = 8000\n</code></pre>"},{"location":"examples/minute_parity/","title":"Minute parity","text":"<p>This is an example of using pytest-hardpy functions, storing the result to CouchDB and writing a simple driver. The code for this example can be seen inside the hardpy package Minute parity.</p> <p>This example includes the results of the test completion modal,  sound notifications and  manual test selection features.</p>"},{"location":"examples/minute_parity/#how-to-start","title":"how to start","text":"<ol> <li>Launch <code>hardpy init minute_parity</code>.</li> <li>Launch CouchDB instance.</li> <li>Modify the files described below.</li> <li>Launch <code>hardpy run minute_parity</code>.</li> </ol>"},{"location":"examples/minute_parity/#hardpytoml","title":"hardpy.toml","text":"<p>Replace the settings in the <code>[frontend]</code> and <code>[frontend.modal_result]</code> sections  with those shown in the hardpy.toml example file below.</p> <pre><code>title = \"HardPy TOML config\"\ntests_name = \"Minute Parity\"\n\n[database]\nuser = \"dev\"\npassword = \"dev\"\nhost = \"localhost\"\nport = 5984\n\n[frontend]\nhost = \"localhost\"\nport = 8000\nlanguage = \"en\"\nfull_size_button = false\nsound_on = true\nmeasurement_display = true\nmanual_collect = true\n\n[frontend.modal_result]\nenable = true\nauto_dismiss_pass = true\nauto_dismiss_timeout = 5\n</code></pre>"},{"location":"examples/minute_parity/#conftestpy","title":"conftest.py","text":"<p>Contains settings and fixtures for all tests:</p> <ul> <li>The function of generating a report and recording it in the database <code>finish_executing</code>;</li> <li>The example of devices used as a fixture in <code>driver_example</code>;</li> <li>The list of actions that will be performed after testing is filled in function <code>fill_actions_after_test</code>;</li> </ul> <pre><code>import pytest\nfrom driver_example import DriverExample\nfrom hardpy import (\n    CouchdbConfig,\n    CouchdbLoader,\n    get_current_report,\n)\n\n@pytest.fixture(scope=\"session\")\ndef driver_example():\n    example = DriverExample()\n    yield example\n    example.random_method()\n\ndef finish_executing():\n    report = get_current_report()\n    if report:\n        loader = CouchdbLoader(CouchdbConfig())\n        loader.load(report)\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef fill_actions_after_test(post_run_functions: list):\n    post_run_functions.append(finish_executing)\n    yield\n</code></pre>"},{"location":"examples/minute_parity/#driver_examplepy","title":"driver_example.py","text":"<p>An example of writing a simple device driver. The driver returns the current minute in the OS.</p> <pre><code>import datetime\nfrom hardpy import set_operator_message\n\nclass DriverExample:\n    @property\n    def current_minute(self):\n        current_time = datetime.datetime.now()\n        return int(current_time.strftime(\"%M\"))\n\n    def random_method(self):\n        pass\n</code></pre>"},{"location":"examples/minute_parity/#test_1py","title":"test_1.py","text":"<p>Contains tests related to preparation for the testing process:</p> <ul> <li>The name of the test module for the web interface is set to <code>pytest.mark.module_name</code>;</li> <li>The name of the test cases for the web interface is set to <code>pytest.mark.case_name</code>;</li> <li>The device under test (DUT) info is stored in the database in <code>test_dut_info</code>;</li> <li>The test stand info is store in the database in <code>test_stand_info</code>;</li> </ul> <pre><code>from uuid import uuid4\nimport pytest\nimport hardpy\n\npytestmark = pytest.mark.module_name(\"Testing preparation\")\n\n@pytest.mark.case_name(\"Process info\")\ndef test_process_info():\n    hardpy.set_process_name(\"Acceptance Test\")\n    hardpy.set_process_number(1)\n\n    process_info = {\"stage\": \"production\", \"version\": \"1.0\"}\n    hardpy.set_process_info(process_info)\n\n\n@pytest.mark.case_name(\"Batch info\")\ndef test_batch_info():\n    hardpy.set_batch_serial_number(\"batch_1\")\n\n\n@pytest.mark.case_name(\"DUT info\")\ndef test_dut_info():\n    serial_number = str(uuid4())[:6]\n    hardpy.set_dut_serial_number(serial_number)\n    hardpy.set_message(f\"Serial number: {serial_number}\")\n    hardpy.set_dut_part_number(\"part_number_1\")\n    hardpy.set_dut_name(\"Test Device\")\n    hardpy.set_dut_type(\"PCBA\")\n    hardpy.set_dut_revision(\"REV1.0\")\n\n    info = {\"sw_version\": \"1.0.0\"}\n    hardpy.set_dut_info(info)\n\n@pytest.mark.case_name(\"Sub unit info\")\ndef test_sub_unit_info():\n    hardpy.set_dut_sub_unit(\n        hardpy.SubUnit(\n            serial_number=str(uuid4())[:6],\n            part_number=\"part_number_1\",\n            type=\"PCBA\",\n            revision=\"REV2.0\"\n        )\n    )\n\n@pytest.mark.case_name(\"Test stand info\")\ndef test_stand_info():\n    test_stand_name = \"Stand 1\"\n    hardpy.set_stand_name(test_stand_name)\n    hardpy.set_stand_location(\"Moon\")\n    hardpy.set_stand_number(2)\n    hardpy.set_stand_revision(\"HW1.0\")\n\n    stand_info = {\"some_info\": \"123\", \"release\": \"1.0.0\", \"calibration_due\": \"2023-12-31\"}\n    hardpy.set_stand_info(stand_info)\n    hardpy.set_message(f\"Stand name: {test_stand_name}\")\n    assert True\n</code></pre>"},{"location":"examples/minute_parity/#test_2py","title":"test_2.py","text":"<p>Contains basic tests:</p> <ul> <li>The name of the test module for the web interface is set to <code>pytest.mark.module_name</code>;</li> <li>The name of the test cases for the web interface is set to <code>pytest.mark.case_name</code>;</li> <li>An example of using a driver to get the current minute in <code>test_minute_parity</code>;</li> <li>An example of saving a test case measurement to the database using <code>set_case_measurement</code>    and <code>NumericMeasurement</code>;</li> <li>An example of using <code>ErrorCode</code>.</li> </ul> <pre><code>import pytest\nfrom driver_example import DriverExample\n\nimport hardpy\n\npytestmark = pytest.mark.module_name(\"Main tests\")\n\n@pytest.mark.case_name(\"Minute check\")\ndef test_minute_parity(driver_example: DriverExample):\n    minute = driver_example.current_minute\n    result = minute % 2\n    hardpy.set_case_measurement(hardpy.NumericMeasurement(value=minute, name=\"Current minute\"))\n    error_code = 1\n    error_msg = f\"The test failed because {minute} is odd! Try again!\"\n    assert result == 0, hardpy.ErrorCode(error_code, error_msg)\n</code></pre>"},{"location":"examples/minute_parity/#test_3py","title":"test_3.py","text":"<p>Contains the final tests of the testing process:</p> <ul> <li>The name of the test module for the web interface is set to <code>pytest.mark.module_name</code>;</li> <li>The name of the test cases for the web interface is set to <code>pytest.mark.case_name</code>;</li> <li>An example of setting and updating a message for a web interface using <code>set_message</code>;</li> <li><code>test_3</code> depends on <code>test_minute_parity</code> from <code>test_2</code>. Dependency is set to <code>pytest.mark.dependency</code>. If <code>test_2::test_minute_parity</code> fails, <code>test_3</code> will be skipped</li> </ul> <pre><code>from time import sleep\nimport pytest\nimport hardpy\n\npytestmark = [\n    pytest.mark.module_name(\"End of testing\"),\n    pytest.mark.dependency(\"test_2::test_minute_parity\"),\n]\n\n@pytest.mark.case_name(\"Final case\")\ndef test_one():\n    for i in range(5, 0, -1):\n        hardpy.set_message(f\"Time left until testing ends {i} s\", \"updated_status\")\n        sleep(1)\n    hardpy.set_message(\"Testing ended\", \"updated_status\")\n    assert True\n</code></pre>"},{"location":"examples/multiple_configs/","title":"Multiple Configs","text":"<p>This example demonstrates how to manage multiple test configurations within a single HardPy project.  This is useful when you need to run different sets of tests,  or the same tests with different parameters, without modifying the main <code>hardpy.toml</code> or <code>pytest.ini</code> files for each run. The code for this example can be seen inside the hardpy package Multiple Configs.</p>"},{"location":"examples/multiple_configs/#how-to-start","title":"how to start","text":"<ol> <li>Launch CouchDB instance.</li> <li>Launch <code>hardpy run examples/multiple_configs</code>.</li> </ol>"},{"location":"examples/multiple_configs/#project-structure","title":"Project Structure","text":"<p>The <code>examples/multiple_configs</code> directory contains:</p> <ul> <li><code>hardpy.toml</code>: The main HardPy configuration file, which defines the available test configurations.</li> <li><code>config1_pytest.ini</code>, <code>config2_pytest.ini</code>, <code>config3_pytest.ini</code>: Individual pytest configuration files,    each specifying a unique set of tests or pytest arguments.</li> <li><code>test_1.py</code>, <code>test_2.py</code>, <code>test_3.py</code>: Example test files that are executed based on the selected configuration.</li> </ul>"},{"location":"examples/multiple_configs/#hardpytoml-configuration","title":"<code>hardpy.toml</code> Configuration","text":"<p>The <code>hardpy.toml</code> file defines the different test configurations using the <code>[[test_configs]]</code> table array. Each entry has:</p> <ul> <li><code>name</code>: A user-friendly name for the configuration.</li> <li><code>file</code>: The path to the <code>pytest.ini</code> file that defines the specific pytest arguments and test selection for this configuration.</li> <li><code>description</code>: An optional field to provide a more detailed explanation of the configuration.</li> </ul> <pre><code># examples/multiple_configs/hardpy.toml\n[[test_configs]]\nname = \"Config 1\"\nfile = \"config1_pytest.ini\"\n\n[[test_configs]]\nname = \"Config 2\"\nfile = \"config2_pytest.ini\"\n\n[[test_configs]]\nname = \"Config 3\"\ndescription = \"Example description\"\nfile = \"config3_pytest.ini\"\n</code></pre>"},{"location":"examples/multiple_configs/#pytestini-files","title":"<code>pytest.ini</code> Files","text":"<p>Each <code>pytest.ini</code> file specifies the <code>python_files</code> that pytest should collect and run for that specific configuration.</p> <p>For example, <code>config1_pytest.ini</code> will run <code>test_1.py</code>:</p> <pre><code># examples/multiple_configs/config1_pytest.ini\n[pytest]\naddopts = --hardpy-pt\n          --hardpy-db-url http://dev:dev@localhost:5984/\n\n; This prevents pytest from trying to collect tests from this directory\ntestpaths = \n\n; Instead we specify the test files directly\npython_files = test_1.py\n</code></pre>"},{"location":"examples/multiple_configs/#running-multiple-configurations","title":"Running Multiple Configurations","text":"<p>When launching HardPy, you can select which configuration to use.  This allows you to easily switch between different testing scenarios.</p>"},{"location":"examples/multiple_stands/","title":"Multiple stands","text":"<p>This is an example of running multiple stands on one PC.</p> <p>Each stand will record reports on the testing it conducts in the CouchDB  document assigned to it in the runstore and statestore databases.</p>"},{"location":"examples/multiple_stands/#how-to-start","title":"how to start","text":"<ol> <li> <p>Create three separate projects, each with a different frontend port.    Launch:      <pre><code>hardpy init tests_1 \n</code></pre></p> <pre><code>hardpy init tests_2 --frontend-port 8001\n</code></pre> <p><pre><code>hardpy init tests_3 --frontend-port 8002\n</code></pre> 2. Launch any database CouchDB instance from any project. 3. Modify the files described below. 4. Launch all projects:</p> </li> </ol> <pre><code>hardpy run tests_1\n</code></pre> <pre><code>hardpy run tests_2\n</code></pre> <pre><code>hardpy run tests_3\n</code></pre>"},{"location":"examples/multiple_stands/#result","title":"result","text":"<p>Three instances of HardPy will launch at the following addresses:  localhost:8000, localhost:8001, and localhost:8002.  Documents named localhost_8000, localhost_8001, and localhost_8002 will  appear immediately in the statestore database.  Documents named localhost_8000, localhost_8001, and localhost_8002 will  appear after the test is launched in the runstore database.  Each document is synchronized with its corresponding HardPy server address. If the host addresses in the hardpy.toml file change, the document addresses will change as well.</p>"},{"location":"examples/operator_msg/","title":"Operator message","text":"<p>The set_operator_message function is intended for sending messages to the operator. Operator messages can be used before, after, and during tests.</p> <p>set_operator_message can be used in conjunction with images and html pages. To do this, the user can set the argument <code>image</code> with the ImageComponent class  and the argument <code>html</code> with the HTMLComponent class.</p> <p>The default message to the operator blocks further execution of the code, but the user can set the argument <code>block=False</code> and the function will display the message and continue execution of the test. In this case, the user can clear the operator message with the clear_operator_message function.</p> <p>The clear_operator_message is intended for clearing current operator message.</p>"},{"location":"examples/operator_msg/#how-to-start","title":"how to start","text":"<ol> <li>Launch CouchDB instance.</li> <li>Create a directory <code>&lt;dir_name&gt;</code> with the files described below.</li> <li>Launch <code>hardpy run &lt;dir_name&gt;</code>.</li> </ol>"},{"location":"examples/operator_msg/#conftestpy","title":"conftest.py","text":"<p>Contains settings and fixtures for all tests:</p> <ul> <li>The <code>finish_executing</code> function generates a report and saves it to the database;</li> <li>The <code>test_end_message</code> function shows message about completing of testing;</li> <li>The <code>fill_list_functions_after_test</code> function populates a list of actions to be performed post-test. You may rename this function as you want;</li> </ul> <p>If the report database doesn't exist, the report won't be saved, and an error message will be displayed to the operator. Otherwise, a success message will be shown indicating successful report saving.</p> <pre><code>import pytest\nfrom hardpy import CouchdbConfig, CouchdbLoader, get_current_report, set_operator_message\n\ndef finish_executing():\n    report = get_current_report()\n    try:\n        if report:\n            loader = CouchdbLoader(CouchdbConfig(port=5986))\n            loader.load(report)\n            set_operator_message(msg=\"Saving report was successful\", title=\"Operator message\")\n    except RuntimeError as e:\n        set_operator_message(msg='The report was not recorded with error: \"' + str(e) + '\"', title=\"Operator message\")\n\ndef test_end_message():\n    set_operator_message(msg=\"Testing completed\", title=\"Operator message\")\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef fill_list_functions_after_test(post_run_functions: list):\n    post_run_functions.append(test_end_message)\n    post_run_functions.append(finish_executing)\n    yield\n</code></pre>"},{"location":"examples/operator_msg/#test_1py","title":"test_1.py","text":"<p>Contains examples of how to use operator messages.</p> <pre><code>from time import sleep\nfrom hardpy import clear_operator_message, set_message, set_operator_message\n\ndef test_block_operator_message():\n    set_operator_message(msg=\"Test blocking operator message\", title=\"Operator message\")\n    for i in range(3, 0, -1):\n        set_message(f\"Time left to complete test case {i} s\", \"updated_status\")\n        sleep(1)\n    set_message(\"Test case finished\", \"updated_status\")\n    assert True\n\ndef test_not_block_operator_message():\n    set_operator_message(msg=\"Test not blocking operator message\", title=\"Operator message\", block=False, font_size=18)\n    for i in range(3, 0, -1):\n        set_message(f\"Time left to complete test case {i} s\", \"updated_status\")\n        sleep(1)\n    set_message(\"Test case finished\", \"updated_status\")\n    sleep(2)\n    assert True\n\ndef test_clear_operator_message():\n    set_operator_message(msg=\"Test clearing operator message\", title=\"Operator message\", block=False)\n    for i in range(3, 0, -1):\n        set_message(f\"Time left to complete test case {i} s\", \"updated_status\")\n        sleep(1)\n    clear_operator_message()\n    set_message(\"Test case finished\", \"updated_status\")\n    sleep(2)\n    assert True\n</code></pre>"},{"location":"examples/operator_msg/#test_2py","title":"test_2.py","text":"<pre><code>from time import sleep\nfrom hardpy import ImageComponent, clear_operator_message, set_message, set_operator_message\n\ndef test_block_operator_message():\n    set_operator_message(\n        msg=\"Test blocking operator message\",\n        title=\"Operator message\",\n        image=ImageComponent(address=\"assets/test.png\", width=100),\n    )\n    for i in range(3, 0, -1):\n        set_message(f\"Time left to complete test case {i} s\", \"updated_status\")\n        sleep(1)\n    set_message(\"Test case finished\", \"updated_status\")\n    assert True\n\ndef test_not_block_operator_message():\n    set_operator_message(\n        msg=\"Test not blocking operator message\",\n        title=\"Operator message\",\n        image=ImageComponent(address=\"assets/test.png\", width=100),\n        block=False,\n    )\n    for i in range(3, 0, -1):\n        set_message(f\"Time left to complete test case {i} s\", \"updated_status\")\n        sleep(1)\n    set_message(\"Test case finished\", \"updated_status\")\n    sleep(2)\n    assert True\n\ndef test_clear_operator_message():\n    set_operator_message(\n        msg=\"Test clearing operator message\",\n        title=\"Operator message\",\n        image=ImageComponent(address=\"assets/test.png\", width=100),\n        block=False,\n    )\n    for i in range(3, 0, -1):\n        set_message(f\"Time left to complete test case {i} s\", \"updated_status\")\n        sleep(1)\n    clear_operator_message()\n    set_message(\"Test case finished\", \"updated_status\")\n    sleep(2)\n    assert True\n</code></pre>"},{"location":"examples/operator_msg/#test_3py","title":"test_3.py","text":"<pre><code>from time import sleep\nfrom hardpy import HTMLComponent, set_message, set_operator_message\n\ntest_html = \"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n\n&lt;h1&gt;Test HTML Page&lt;/h1&gt;\n\n&lt;p&gt;It is testing page.&lt;/p&gt;\n&lt;p&gt;You can put anything on it.&lt;/p&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n\"\"\"\n\ndef test_operator_message_with_html():\n    set_operator_message(\n        msg=\"Test operator message with html\",\n        title=\"Operator message\",\n        html=HTMLComponent(html=test_html, is_raw_html=True),\n    )\n    for i in range(3, 0, -1):\n        set_message(f\"Time left to complete test case {i} s\", \"updated_status\")\n        sleep(1)\n    set_message(\"Test case finished\", \"updated_status\")\n    assert True\n\ndef test_operator_message_with_html_and_border():\n    set_operator_message(\n        msg=\"Test operator message with html\",\n        title=\"Operator message\",\n        html=HTMLComponent(html=test_html, is_raw_html=True, border=10, width=20),\n    )\n    for i in range(3, 0, -1):\n        set_message(f\"Time left to complete test case {i} s\", \"updated_status\")\n        sleep(1)\n    set_message(\"Test case finished\", \"updated_status\")\n    assert True\n</code></pre>"},{"location":"examples/skip_test/","title":"Skip test","text":"<p>This is an example of using the pytest-hardpy functions with a test dependency on another test and skipping tests.</p>"},{"location":"examples/skip_test/#how-to-start","title":"how to start","text":"<ol> <li>Launch <code>hardpy init skip_test</code>.</li> <li>Launch CouchDB instance.</li> <li>Modify the files described below.</li> <li>Launch <code>hardpy run skip_test</code>.</li> </ol>"},{"location":"examples/skip_test/#description","title":"description","text":"<p>If a test case/module that a test case/module depends on fails, errors or is skipped, the dependent test case/module will also be skipped. A module is considered passed only if all module tests passed. If these dependencies are incorrect, the tests will not run.</p> <p>To use:</p> <ul> <li>Add the line <code>@pytest.mark.dependency()</code> before independent tests.</li> <li>Add the line <code>@pytest.mark.dependency(test_1::test_a)</code> before the dependent test, if a test that a test depends on is in the same file.</li> <li>Add the line <code>@pytest.mark.dependency(test_1)</code> before the dependent test, if the test depends on the module.</li> </ul> <p>Test/module name formats:</p> <ul> <li><code>test_1</code> - if depends on the test module</li> <li><code>test_1::test_a</code> - if depends on the test case</li> </ul>"},{"location":"examples/skip_test/#case-by-case-dependence","title":"case by case dependence","text":"<pre><code>import pytest\n\ndef test_a():\n    assert False\n\n@pytest.mark.dependency(\"test_1::test_a\")\ndef test_b():\n    assert False\n</code></pre> <p><code>test_a</code> is marked as a dependency for <code>test_b</code> using <code>@pytest.mark.dependency(\"test_1::test_a\")</code>. If <code>test_a</code>, then <code>test_b</code> will be skipped.</p>"},{"location":"examples/skip_test/#module-by-module-dependence","title":"module by module dependence","text":""},{"location":"examples/skip_test/#test_1py","title":"test_1.py","text":"<pre><code>def test_a():\n    assert False\n</code></pre>"},{"location":"examples/skip_test/#test_2py","title":"test_2.py","text":"<pre><code>import pytest\n\npytestmark = pytest.mark.dependency(\"test_1\")\n\ndef test_a():\n    assert True\n</code></pre> <p>Module <code>test_2</code> depends on module <code>test_1</code>. If an error occurs in module <code>test_1</code>, all tests in module <code>test_2</code> will be skipped.</p>"},{"location":"examples/skip_test/#multiple-test-dependencies-example","title":"multiple test dependencies example","text":"<p>You can specify multiple dependencies for a single test or module. The test will only run if ALL specified dependencies are successful. If any dependency fails, the test will be skipped.</p> <pre><code>import pytest\n\ndef test_a():\n    assert True\n\ndef test_b():\n    assert False\n\n@pytest.mark.dependency(\"test_1::test_a\")\n@pytest.mark.dependency(\"test_1::test_b\")\ndef test_c():\n    assert True\n</code></pre> <p>In this case, <code>test_c</code> depends on two other tests. Since <code>test_b</code> fails, <code>test_c</code> will be skipped.</p>"},{"location":"examples/skip_test/#multiple-module-dependencies-example","title":"multiple module dependencies example","text":""},{"location":"examples/skip_test/#test_1py_1","title":"test_1.py","text":"<pre><code>def test_a():\n    assert True\n</code></pre>"},{"location":"examples/skip_test/#test_2py_1","title":"test_2.py","text":"<pre><code>def test_b():\n    assert False\n</code></pre>"},{"location":"examples/skip_test/#test_3py","title":"test_3.py","text":"<pre><code>import pytest\n\npytestmark = [\n    pytest.mark.dependency(\"test_1\"),\n    pytest.mark.dependency(\"test_2\"),\n]\n\ndef test_c():\n    assert True\n</code></pre> <p>Here, the entire <code>test_3</code> module depends on both <code>test_1</code> and <code>test_2</code> modules. Since <code>test_2</code> fails, all tests in <code>test_3</code> will be skipped.</p>"},{"location":"examples/skip_test/#incorrect-dependency-names","title":"incorrect dependency names","text":""},{"location":"examples/skip_test/#test_1py_2","title":"test_1.py","text":"<pre><code>import pytest\n\ndef test_a():\n    assert True\n\n@pytest.mark.dependency(\"test_1::a\")\ndef test_b():\n    assert True\n</code></pre> <p>In this case, the test <code>test_b</code> has no dependencies because the  <code>test_1::a</code> dependency is incorrect.</p>"},{"location":"examples/stand_cloud/","title":"StandCloud","text":"<p>This is an example of using pytest-hardpy functions, storing the result to StandCloud. This example is similar to Minute parity, but the test results are stored in StandCloud instead of CouchDB. The code for this example can be seen inside the hardpy package StandCloud.</p> <p>After testing is complete, data is sent to StandCloud by setting the  <code>autosync</code> to <code>true</code> in the [stand_cloud] section of the hardpy.toml file,  and by setting the value of the <code>api_key</code> variable. If there is no connection to StandCloud, HardPy will attempt to send data  to StandCloud at the specified <code>autosync_timeout</code> interval (in minutes).</p>"},{"location":"examples/stand_cloud/#how-to-start","title":"how to start","text":"<ol> <li>Login to standcloud.io.</li> <li>Create a company or login to an existing one.</li> <li>Create and copy the API key.</li> <li>Launch <code>hardpy init stand_cloud --sc-api-key &lt;your_api_key&gt; --sc-autosync</code>     or <code>hardpy init stand_cloud</code> and manually modify the     [stand_cloud] section in hardpy.toml.</li> <li>Launch CouchDB instance.</li> <li>Modify the files described below. Copying the hardpy.toml file will     suffice for sending data to StandCloud after testing.</li> <li>Launch <code>hardpy run stand_cloud</code>.</li> </ol>"},{"location":"examples/stand_cloud/#hardpytoml","title":"hardpy.toml","text":"<pre><code>title = \"HardPy TOML config\"\ntests_name = \"StandCloud\"\n\n[database]\nuser = \"dev\"\npassword = \"dev\"\nhost = \"localhost\"\nport = 5984\n\n[frontend]\nhost = \"localhost\"\nport = 8000\nlanguage = \"en\"\nfull_size_button = false\nsound_on = true\nmeasurement_display = true\nmanual_collect = false\n\n[frontend.modal_result]\nenable = true\nauto_dismiss_pass = true\nauto_dismiss_timeout = 5\n\n[stand_cloud]\naddress = \"standcloud.io\"\nconnection_only = false\nautosync = false\nautosync_timeout = 30\napi_key = \"&lt;your_api_key&gt;\"\n</code></pre>"},{"location":"examples/stand_cloud/#conftestpy","title":"conftest.py","text":"<p>Contains settings and fixtures for all tests:</p> <ul> <li>The example of devices used as a fixture in <code>driver_example</code>;</li> </ul> <pre><code>import pytest\nfrom driver_example import DriverExample\n\n@pytest.fixture(scope=\"session\")\ndef driver_example():\n    example = DriverExample()\n    yield example\n</code></pre>"},{"location":"examples/stand_cloud/#driver_examplepy","title":"driver_example.py","text":"<p>An example of writing a simple device driver. The driver returns the current minute in the OS.</p> <pre><code>import datetime\n\nclass DriverExample:\n    @property\n    def current_minute(self):\n        current_time = datetime.datetime.now()\n        return int(current_time.strftime(\"%M\"))\n</code></pre>"},{"location":"examples/stand_cloud/#test_1py","title":"test_1.py","text":"<p>Contains tests related to preparation for the testing process:</p> <ul> <li>The name of the test module for the web interface is set to <code>pytest.mark.module_name</code>;</li> <li>The name of the test cases for the web interface is set to <code>pytest.mark.case_name</code>;</li> <li>The device under test (DUT) info is stored in the database in <code>test_dut_info</code>;</li> <li>The test stand info is store in the database in <code>test_stand_info</code>;</li> </ul> <pre><code>from uuid import uuid4\nimport pytest\nimport hardpy\n\npytestmark = pytest.mark.module_name(\"Testing preparation\")\n\n@pytest.mark.case_name(\"Process info\")\ndef test_process_info():\n    hardpy.set_process_name(\"Acceptance Test\")\n    hardpy.set_process_number(1)\n\n    process_info = {\"stage\": \"production\", \"version\": \"1.0\"}\n    hardpy.set_process_info(process_info)\n\n\n@pytest.mark.case_name(\"Batch info\")\ndef test_batch_info():\n    hardpy.set_batch_serial_number(\"batch_1\")\n\n\n@pytest.mark.case_name(\"DUT info\")\ndef test_dut_info():\n    serial_number = str(uuid4())[:6]\n    hardpy.set_dut_serial_number(serial_number)\n    hardpy.set_message(f\"Serial number: {serial_number}\")\n    hardpy.set_dut_part_number(\"part_number_1\")\n    hardpy.set_dut_name(\"Test Device\")\n    hardpy.set_dut_type(\"PCBA\")\n    hardpy.set_dut_revision(\"REV1.0\")\n\n    info = {\"sw_version\": \"1.0.0\"}\n    hardpy.set_dut_info(info)\n\n@pytest.mark.case_name(\"Sub unit info\")\ndef test_sub_unit_info():\n    hardpy.set_dut_sub_unit(\n        hardpy.SubUnit(\n            serial_number=str(uuid4())[:6],\n            part_number=\"part_number_1\",\n            type=\"PCBA\",\n            revision=\"REV2.0\"\n        )\n    )\n\n@pytest.mark.case_name(\"Test stand info\")\ndef test_stand_info():\n    test_stand_name = \"Stand 1\"\n    hardpy.set_stand_name(test_stand_name)\n    hardpy.set_stand_location(\"Moon\")\n    hardpy.set_stand_number(2)\n    hardpy.set_stand_revision(\"HW1.0\")\n\n    stand_info = {\"some_info\": \"123\", \"release\": \"1.0.0\", \"calibration_due\": \"2023-12-31\"}\n    hardpy.set_stand_info(stand_info)\n    hardpy.set_message(f\"Stand name: {test_stand_name}\")\n    assert True\n</code></pre>"},{"location":"examples/stand_cloud/#test_2py","title":"test_2.py","text":"<p>Contains basic tests:</p> <ul> <li>The name of the test module for the web interface is set to <code>pytest.mark.module_name</code>;</li> <li>The name of the test cases for the web interface is set to <code>pytest.mark.case_name</code>;</li> <li>An example of using a driver to get the current minute in <code>test_minute_parity</code>;</li> <li>An example of saving a test case measurement to the database using <code>set_case_measurement</code>    and <code>NumericMeasurement</code>;</li> <li>An example of using <code>ErrorCode</code>.</li> </ul> <pre><code>import pytest\nfrom driver_example import DriverExample\n\nimport hardpy\n\npytestmark = pytest.mark.module_name(\"Main tests\")\n\n@pytest.mark.case_name(\"Minute check\")\ndef test_minute_parity(driver_example: DriverExample):\n    minute = driver_example.current_minute\n    result = minute % 2\n    hardpy.set_case_measurement(hardpy.NumericMeasurement(value=minute, name=\"Current minute\"))\n    error_code = 1\n    error_msg = f\"The test failed because {minute} is odd! Try again!\"\n    assert result == 0, hardpy.ErrorCode(error_code, error_msg)\n</code></pre>"},{"location":"examples/stand_cloud/#test_3py","title":"test_3.py","text":"<p>Contains the final tests of the testing process:</p> <ul> <li>The name of the test module for the web interface is set to <code>pytest.mark.module_name</code>;</li> <li>The name of the test cases for the web interface is set to <code>pytest.mark.case_name</code>;</li> <li>An example of setting and updating a message for a web interface using <code>set_message</code>;</li> <li><code>test_3</code> depends on <code>test_minute_parity</code> from <code>test_2</code>. Dependency is set to <code>pytest.mark.dependency</code>. If <code>test_2::test_minute_parity</code> fails, <code>test_3</code> will be skipped</li> </ul> <pre><code>from time import sleep\nimport pytest\nimport hardpy\n\npytestmark = [\n    pytest.mark.module_name(\"End of testing\"),\n    pytest.mark.dependency(\"test_2::test_minute_parity\"),\n]\n\n@pytest.mark.case_name(\"Final case\")\ndef test_one():\n    for i in range(5, 0, -1):\n        hardpy.set_message(f\"Time left until testing ends {i} s\", \"updated_status\")\n        sleep(1)\n    hardpy.set_message(\"Testing ended\", \"updated_status\")\n    assert True\n</code></pre>"},{"location":"examples/stand_cloud/#manual-standcloud-syncronization","title":"manual StandCloud syncronization","text":"<p>If you want to control the sending of reports to StandCloud yourself,  the following code, which you can add to <code>conftest.py</code>, will allow you to do so.</p> <p>Remember to also set <code>autosync=false</code> in the stand_cloud section of  the hardpy.toml file.</p> <pre><code># conftest.py\nfrom http import HTTPStatus\n\nimport pytest\nfrom driver_example import DriverExample\n\nfrom hardpy import (\n    StandCloudError,\n    StandCloudLoader,\n    get_current_report,\n    set_operator_message,\n)\n\n@pytest.fixture(scope=\"session\")\ndef driver_example():\n    example = DriverExample()\n    yield example\n\ndef finish_executing():\n    report = get_current_report()\n    if report:\n        try:\n            loader = StandCloudLoader()\n            response = loader.load(report)\n            if response.status_code != HTTPStatus.CREATED:\n                set_operator_message(\n                    \"Report not uploaded to StandCloud, \"\n                    f\"status code: {response.status_code}, text: {response.text}\",\n                )\n        except StandCloudError as exc:\n            set_operator_message(f\"{exc}\")\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef fill_actions_after_test(post_run_functions: list):\n    post_run_functions.append(finish_executing)\n    yield\n</code></pre>"},{"location":"examples/stand_cloud_reader/","title":"StandCloud reader","text":"Warning <p>The information on this page is currently out of date!</p> <p>HardPy allows you to read test data from the StandCloud. For this purpose, the StandCloudReader class is available in HardPy, which provides access to the REST API of the StandCloud service. To read data from the StandCloud, the user must log in to the StandCloud using the hardpy sc-login.</p> <p>To view the REST API documentation, the user can navigate to the format page https://service_name/integration/api/v1/docs, where <code>service_name</code> is the address of the StandCloud client.</p> <p>Example of documentation page address: https://demo.standcloud.io/integration/api/v1/docs</p> <p>To access StandCloud, contact info@everypin.io.</p>"},{"location":"examples/stand_cloud_reader/#functions","title":"Functions","text":""},{"location":"examples/stand_cloud_reader/#test_run","title":"test_run","text":"<p>Provides access to 2 endpoints: <code>/test_run/{test_run_id}</code> and <code>/test_run</code>. This function does not allow two arguments to be used at the same time. The argument must be either <code>run_id</code> or the filter <code>params</code>.</p> <p>run_id</p> <p>Allows the user to retrieve data about a specific test run by its id from <code>/test_run</code> URL. User can get id from StandCloudLoader.load() function.</p> <p>Example:</p> <pre><code>import hardpy\n\nsc_connector = hardpy.StandCloudConnector(api_key=\"your_api_key\")\nreader = hardpy.StandCloudReader(sc_connector)\n\nresponse = reader.test_run(run_id=\"0196434d-e8f7-7ce1-81f7-e16f20487494\")\nprint(response.json())\n</code></pre> <p>Request URL of this example:</p> <pre><code>https://demo.standcloud.io/hardpy/api/v1/test_run/0196434d-e8f7-7ce1-81f7-e16f20487494\n</code></pre> <p>REST API documentation page of this example:</p> <pre><code>https://demo.standcloud.io/integration/api/v1/docs\n</code></pre> <p>Response data example:</p> <pre><code>{\n  \"test_run_id\": \"0196434d-e8f7-7ce1-81f7-e16f20487494\",\n  \"test_plan_name\": \"Test Plan A\",\n  \"serial_number\": \"SN-98765\",\n  \"part_number\": \"PN-54321AB\",\n  \"status\": \"FAIL\",\n  \"status_icon\": \"\u274c\",\n  \"number_of_attempt\": 1,\n  \"start_time\": \"2024-02-20T14:23:45Z\",\n  \"stop_time\": \"2024-02-20T14:23:45Z\",\n  \"duration\": \"00:12:34\",\n  \"test_stand_name\": \"EMC Chamber #2\",\n  \"test_stand_hw_id\": \"TS-CH45\",\n  \"test_run_artifact\": {\n    \"test_run_parameter\": \"test_run_value\"\n  },\n  \"test_stand_info\": {\n    \"calibration_date\": \"2025-01-15\"\n  },\n  \"dut_info\": {\n    \"manufacturer\": \"ABC Corp\"\n  }\n}\n</code></pre> <p>params</p> <p>Allows the user to retrieve data about a test run by filters from <code>/test_run</code> URL. Filters are specified as parameters. A special place is occupied by the filter by field <code>dut.info</code>, which allows to add fields <code>dut.info</code> as keys for the filter in the parameters.</p> <p>The difference between the test run with filters and the tested DUT is described in the StandCloudReader documentation.</p> <p>Example:</p> <pre><code>import hardpy\n\nsc_connector = hardpy.StandCloudConnector(addr=\"demo.standcloud.io\")\nreader = hardpy.StandCloudReader(sc_connector)\n\nparam = {\n    \"part_number\": \"PN-54321AB\",\n    \"status\": \"pass\",\n    \"manufacturer\": \"ABC_Corp\",\n    \"number_of_attempt\": 2,\n}\nresponse = reader.test_run(params=param)\nprint(response.json())\n</code></pre> <p>Request URL of this example:</p> <pre><code>https://demo.standcloud.io/hardpy/api/v1/test_run?part_number=PN-54321AB&amp;status=pass&amp;manufacturer=ABC_Corp&amp;number_of_attempt=2\n</code></pre> <p>REST API documentation page of this example:</p> <pre><code>https://demo.standcloud.io/integration/api/v1/docs\n</code></pre> <p>Response data example:</p> <pre><code>{\n  \"test_run_id\": \"0196434d-e8f7-7ce1-81f7-e16f20487494\",\n  \"test_plan_name\": \"Test Plan A\",\n  \"serial_number\": \"SN-98765\",\n  \"part_number\": \"PN-54321AB\",\n  \"status\": \"FAIL\",\n  \"status_icon\": \"\u274c\",\n  \"number_of_attempt\": 1,\n  \"start_time\": \"2024-02-20T14:23:45Z\",\n  \"stop_time\": \"2024-02-20T14:23:45Z\",\n  \"duration\": \"00:12:34\",\n  \"test_stand_name\": \"EMC Chamber #2\",\n  \"test_stand_hw_id\": \"TS-CH45\",\n  \"test_run_artifact\": {\n    \"test_run_parameter\": \"test_run_value\"\n  },\n  \"test_stand_info\": {\n    \"calibration_date\": \"2025-01-15\"\n  },\n  \"dut_info\": {\n    \"serial_number\": \"SN-98765\",\n    \"part_number\": \"PN-54321AB\",\n    \"info\": {\n      \"manufacturer\": \"ABC Corp\"\n    }\n  }\n}\n</code></pre>"},{"location":"examples/stand_cloud_reader/#tested_dut","title":"tested_dut","text":"<p>Allows the user to retrieve data about the last tested dut's by filters from <code>/tested_dut</code> URL. Filters are specified as parameters. A special place is occupied by the filter by field <code>dut.info</code>, which allows to add fields <code>dut.info</code> as keys for the filter in the parameters.</p> <p>The difference between the test run with filters and the tested DUT is described in the StandCloudReader documentation.</p> <p>Example:</p> <pre><code>import hardpy\n\nsc_connector = hardpy.StandCloudConnector(addr=\"demo.standcloud.io\")\nreader = hardpy.StandCloudReader(sc_connector)\n\nparam = {\n    \"part_number\": \"PN-54321AB\",\n    \"status\": \"pass\",\n    \"manufacturer\": \"ABC_Corp\",\n    \"attempt_count\": 3,\n}\nresponse = reader.tested_dut(param)\nprint(response.json())\n</code></pre> <p>Request URL of this example:</p> <pre><code>https://demo.standcloud.io/hardpy/api/v1/tested_dut?part_number=PN-54321AB&amp;status=pass&amp;manufacturer=ABC_Corp&amp;attempt_count=3\n</code></pre> <p>REST API documentation page of this example:</p> <pre><code>https://demo.standcloud.io/integration/api/v1/docs\n</code></pre> <p>Response data example:</p> <pre><code>[\n  {\n    \"test_plan_name\": \"Test run #A\",\n    \"serial_number\": \"SN-12345\",\n    \"part_number\": \"PN-54321AB\",\n    \"status\": \"pass\",\n    \"status_icon\": \"\u2705\",\n    \"attempt_count\": 1,\n    \"start_time\": \"2025-03-28T09:34:05Z\",\n    \"finish_time\": \"2025-03-28T09:34:55Z\",\n    \"duration\": \"PT50S\",\n    \"test_stand_name\": \"EMC Chamber #2\",\n    \"test_stand_hw_id\": \"TS-CH45\",\n    \"dut_info\": {\n      \"manufacturer\": \"ABC_Corp\"\n    },\n    \"test_stand_info\": {\n      \"calibration_date\": \"2025-01-15\"\n    },\n    \"test_run_artifact\": {\n      \"test_run_parameter\": \"test_run_value\"\n    }\n  },\n  {\n    \"test_plan_name\": \"Test run #B\",\n    \"serial_number\": \"SN-98766\",\n    \"part_number\": \"PN-54321AB\",\n    \"status\": \"pass\",\n    \"status_icon\": \"\u2705\",\n    \"attempt_count\": 1,\n    \"start_time\": \"2025-03-28T09:35:05Z\",\n    \"finish_time\": \"2025-03-28T09:35:55Z\",\n    \"duration\": \"PT50S\",\n    \"test_stand_name\": \"EMC Chamber #2\",\n    \"test_stand_hw_id\": \"TS-CH45\",\n    \"dut_info\": {\n      \"manufacturer\": \"ABC_Corp\"\n    },\n    \"test_stand_info\": {\n      \"calibration_date\": \"2025-01-15\"\n    },\n    \"test_run_artifact\": {\n      \"test_run_parameter\": \"test_run_value\"\n    }\n  }\n]\n</code></pre>"},{"location":"examples/stand_cloud_thirdparty_integration/","title":"StandCloud third party integration","text":"<p>This documentation explains how to use the StandCloud integration service via an API key. Information on integration options with StandCloud can be found at  https://standcloud.io/integration/api/v1/docs.</p>"},{"location":"examples/stand_cloud_thirdparty_integration/#code-example","title":"Code example","text":"<p>The code samples are in Python, but they use simple constructs that are available in most programming languages.</p>"},{"location":"examples/stand_cloud_thirdparty_integration/#integration-example","title":"Integration example","text":"<p>integration_flow.py is an example of a simple integration script written in Python. The script requires the installation of the <code>requests</code> package. The only thing that needs to be changed to make it work is the API key in the <code>API_KEY</code> variable. This script first checks access to the integration service,  and then requests a list of completed test runs by sending a request to <code>test_run</code>.</p> <pre><code>import sys\n\nimport requests\n\nSSL_VERIFY = True\n\n########################################\n# Change API_KEY to your api key\nAPI_KEY = \"your_api_key\"\n########################################\n\nHEALTHCHECK_URL = \"https://standcloud.io/integration/api/v1/healthcheck\"\nresponse = requests.get(HEALTHCHECK_URL, verify=SSL_VERIFY)\n\nif response.status_code != 200:\n    print(response.text)\n    sys.exit(1)\nprint(\"StandCloud is up and running\")\n\n# Test API call\nheader = {\n    \"Authorization\": f\"Bearer {API_KEY}\",\n    \"Content-type\": \"application/json\",\n    \"Accept\": \"text/plain\",\n}\n\nUSER_INFO_URL = \"https://standcloud.io/integration/api/v1/test_run\"\nresponse = requests.get(USER_INFO_URL, headers=header, verify=SSL_VERIFY)\n\nif response.status_code != 200:\n    print(response.text)\n    sys.exit(1)\nprint(\"\\nTest run list: \", response.text)\n</code></pre>"},{"location":"examples/stand_equipment/","title":"Stand equipment example","text":"<p>This example demonstrates how to document test bench equipment using the <code>set_instrument()</code> function in pytest-hardpy. The example shows how to record instruments that are part of the test setup.</p>"},{"location":"examples/stand_equipment/#how-to-start","title":"How to start","text":"<ol> <li>Launch <code>hardpy init test_stand_equipment</code>.</li> <li>Launch CouchDB instance.</li> <li>Launch <code>hardpy run test_stand_equipment</code>.</li> </ol>"},{"location":"examples/stand_equipment/#description","title":"Description","text":"<p>The <code>Instrument</code> class and <code>set_instrument()</code> function allow documenting all equipment that forms part of the test bench setup. This information is stored in the database. Information about using of the function set_instrument and class Instrument</p>"},{"location":"examples/stand_equipment/#example-implementation","title":"Example implementation","text":"<pre><code>import pytest\nimport hardpy\nfrom datetime import datetime\n\npytestmark = pytest.mark.module_name(\"Stand equipment\")\n\n@pytest.mark.case_name(\"Power Supply Setup\")\ndef test_power_supply():\n    \"\"\"Document the power supply used in testing.\"\"\"\n    psu = hardpy.Instrument(\n        name=\"DC Power Supply\",\n        revision=\"2.1\",\n        serial_number=\"809184\",\n        part_number=\"pwr_blck_01\",\n        number=1,\n        comment=\"Main system power source\",\n        info={\n            \"voltage_range\": \"0-30V\",\n            \"current_range\": \"0-5A\",\n            \"calibration_date\": datetime(2023, 6, 15).isoformat()\n        }\n    )\n    hardpy.set_instrument(psu)\n    assert True\n\n@pytest.mark.case_name(\"Measurement Equipment\")\ndef test_measurement_devices():\n    \"\"\"Document measurement equipment on the test bench.\"\"\"\n    # Multimeter\n    dmm = hardpy.Instrument(\n        name=\"Digital Multimeter\",\n        serial_number=\"235446\",\n        part_number=\"epin_mlmtr_05\",\n        revision=\"1.3\",\n        number=2,\n        info={\n            \"accuracy\": \"0.1%\",\n            \"channels\": 4\n        }\n    )\n    hardpy.set_instrument(dmm)\n\n    # Oscilloscope\n    scope = hardpy.Instrument(\n        name=\"Oscilloscope\",\n        revision=\"3.2\",\n        serial_number=\"456312\",\n        part_number=\"epin_osc_07\",\n        number=3,\n        info={\n            \"model\": \"DSO-X 2024A\",\n            \"bandwidth\": \"200MHz\",\n            \"sample_rate\": \"2GSa/s\"\n        }\n    )\n    hardpy.set_instrument(scope)\n    assert True\n\n@pytest.mark.case_name(\"Environmental Controls\")\ndef test_environmental_controls():\n    \"\"\"Document environmental control equipment.\"\"\"\n    chamber = hardpy.Instrument(\n        name=\"Temperature Chamber\",\n        revision=\"4.0\",\n        serial_number=\"5468653\",\n        part_number=\"epin_temp_12\",\n        number=4,\n        comment=\"Used for thermal testing\",\n        info={\n            \"temperature_range\": \"-40\u00b0C to +150\u00b0C\",\n            \"humidity_range\": \"10% to 98% RH\"\n        }\n    )\n    hardpy.set_instrument(chamber)\n    assert True\n</code></pre>"},{"location":"features/features/","title":"Features","text":""},{"location":"features/features/#operator-panel","title":"Operator panel","text":""},{"location":"features/features/#viewing-tests-in-the-operator-panel","title":"Viewing tests in the operator panel","text":"<p>The operator panel allows you to view the test hierarchy with test folder/test file/function levels. An example of its use can be found on page HardPy panel. User can launch operator panel using hardpy run command.</p>"},{"location":"features/features/#interacting-with-tests-in-the-operator-panel","title":"Interacting with tests in the operator panel","text":"<p>The user can interact with the tests through dialog boxes in the operator panel. HardPy provides the ability to enter text or numbers, make selections using checkboxes or radio buttons, guide the user through multiple steps in a single window, display images to the user, and insert HTML components into the dialog box using iframes.</p> <p>In dialog boxes, the user must confirm their action by clicking the Confirm button. Closing the dialog box prematurely will abort the test.</p> <p>If the user only needs to provide information, an operator message can be used. Unlike dialog boxes, the user cannot enter any information, only close the window. User can also close the operator message from within the test code and run it in non-blocking mode, so that displaying the window does not stop the test.</p>"},{"location":"features/features/#operator-panel-language","title":"Operator panel language","text":"<p>The user can set one of the following operator panel languages \u200b\u200bvia the hardpy.toml. Available languages are there.</p> <p>The example of file:</p> <pre><code>[frontend]\nlanguage = \"zh\"\n</code></pre>"},{"location":"features/features/#full-size-startstop-button","title":"Full-size start/stop button","text":"<p>HardPy provides a full-size start/stop button layout option for improved usability across different devices and use cases.</p> <p>Configuration: Enable the full-size button in your <code>hardpy.toml</code>:</p> <pre><code>[frontend]\nfull_size_button = true\n</code></pre>"},{"location":"features/features/#test-completion-modal-results","title":"Test completion modal results","text":"<p>HardPy provides configurable modal result windows that display test completion status with detailed information:</p> <ul> <li>PASS results: Green modal with auto-dismiss functionality</li> <li>FAIL results: Red modal showing detailed failed test cases list</li> <li>STOP results: Yellow modal displaying stopped test case information</li> </ul> <p>Configuration Options in <code>hardpy.toml</code>: <pre><code>[frontend.modal_result]\nenable = true\nauto_dismiss_pass = true\nauto_dismiss_timeout = 5\n</code></pre></p> <p>Features:</p> <ul> <li>Auto-dismiss: PASS results automatically close after configurable timeout.</li> <li>Manual dismissal: Click anywhere or press any key to close modal.</li> <li>Detailed reporting: Failed test cases show module names, case names, and assertion messages.</li> <li>Keyboard integration: Space key handling respects modal visibility states.</li> <li>Responsive design: Adapts to different screen sizes with optimal readability.</li> </ul>"},{"location":"features/features/#sound-notifications","title":"Sound notifications","text":"<p>HardPy provides configurable sound notifications for test completion events. Users can enable audio feedback when tests reach completion status (PASS/FAIL/STOP).</p> <p>Configuration: Enable sound notifications in your <code>hardpy.toml</code>:</p> <pre><code>[frontend]\nsound_on = true\n</code></pre>"},{"location":"features/features/#manual-test-selection","title":"Manual test selection","text":"<p>HardPy supports manual test selection, allowing operators to choose specific test cases to run instead of executing the entire test plan.</p> <p>Key capabilities:</p> <ul> <li>Checkbox-based selection: Each test case and module displays checkboxes for selection</li> <li>Bulk operations: Select or deselect all tests in a module with a single checkbox</li> <li>Visual feedback: Selected tests are marked for execution, non-selected tests are skipped</li> </ul> <p>Workflow:</p> <ol> <li>Enable manual collect mode in settings menu</li> <li>Select desired tests using checkboxes</li> <li>Disable manual collect mode</li> <li>Start test execution - only selected tests will run</li> </ol> <p>Configuration: Enable manual test selection in your <code>hardpy.toml</code>:</p> <pre><code>[frontend]\nmanual_collect = true\n</code></pre>"},{"location":"features/features/#cli","title":"CLI","text":""},{"location":"features/features/#creating-template-project","title":"Creating template project","text":"<p>The HardPy can create template project using the hardpy init command. An example of usage can be found in the how to init section among the examples, e.g. in minute parity.</p> <pre><code>hardpy init minute_parity\n</code></pre>"},{"location":"features/features/#running-the-hardpy-server","title":"Running the HardPy server","text":"<p>Based on pytest HardPy allows you to run pytest tests in the browser or from the command line. To run the HardPy server and view tests in the browser, execute the  hardpy run command.</p> <pre><code>hardpy run &lt;project_path&gt;\n</code></pre> <p>For example:</p> <pre><code>hardpy run examples/minute_parity \n</code></pre>"},{"location":"features/features/#starting-the-tests","title":"Starting the tests","text":"<p>The user can start the operator panel with the Start button. As an alternative, the user can start the tests using the  hardpy start command.</p> <pre><code>hardpy start &lt;project_path&gt;\n</code></pre> <p>For example:</p> <pre><code>hardpy start examples/minute_parity \n</code></pre> <p>Through the CLI, the user can start HardPy with arguments that will be used during testing.</p> <pre><code>hardpy start --arg device_id=123 --arg operator_name=user examples/minute_parity \n</code></pre> <p>The user can also start the tests using pytest.</p> <pre><code>python -m pytest\n</code></pre>"},{"location":"features/features/#stopping-the-tests","title":"Stopping the tests","text":"<p>The user can stop the tests during execution from the operator panel by clicking on the Stop button or using the hardpy stop command. It is possible to stop tests from the console by sending an interrupt, e.g. \u201cCtrl-C\u201d in Linux.</p>"},{"location":"features/features/#checking-the-status-of-tests","title":"Checking the status of tests","text":"<p>The user can check the status of tests using the hardpy status command.</p>"},{"location":"features/features/#database","title":"Database","text":""},{"location":"features/features/#storing-test-result-in-database","title":"Storing test result in database","text":"<p>HardPy allows you to run tests with a running CouchDB database. This is a NoSQL database that ensures that the results of the current test run are committed, even if the tests are aborted early.</p> <p>To save the report history, the user must configure the conftest.py file using the CouchdbLoader or another adapter to save the data. By default, only the current report in the runstore database is stored in CouchDB.</p> <p>The test report format (database scheme) is described in the database runstore section.</p> <p>An example of configuring conftest.py to store test run history can be found in several examples, including the couchdb_load and minute_parity.</p>"},{"location":"features/features/#json","title":"JSON","text":"<p>With HardPy, you can run tests without a database and save the test data to local JSON documents.  These documents have a structure similar to that of databases and documents in CouchDB.  A .hardpy/storage folder is created by default in the project folder,  where test reports can be found in the runstore folder.</p> <p>An example of its use can be found on page JSON storage.</p>"},{"location":"features/features/#other-databases","title":"Other databases","text":"<p>In order to save data to other databases, users will need to write their own adapter class to convert their  HardPy runstore schema data into a suitable format. Next, the user should call their adapter class in the same way that CouchdbLoader  is called in the couchdb_load and minute_parity examples.</p>"},{"location":"features/features/#pytest-markers","title":"Pytest markers","text":""},{"location":"features/features/#running-multiple-test-attempts","title":"Running multiple test attempts","text":"<p>The HardPy allows the user to attach an attempt marker to a test, which allows the test to be repeated as many times as specified in the marker's content in the event of failure.</p>"},{"location":"features/features/#skipping-the-tests","title":"Skipping the tests","text":"<p>The HardPy allows the user to skip tests on dependency markers. The user can specify that a test or an entire module depends on another test. In this way, tests can be defined whose failure will prevent the marked tests from running. The user can also create a dependency on several test cases or modules. If the name of the dependency does not exist, the test will be started.</p> <pre><code>#test_1.py\nimport pytest\n\ndef test_a():\n    assert False\n\n@pytest.mark.dependency(\"test_1::test_a\")\ndef test_b():\n    assert True\n</code></pre> <pre><code>#test_2.py\nimport pytest\n\npytestmark = pytest.mark.dependency(\"test_1\")\n\ndef test_a():\n    assert False\n\ndef test_b():\n    assert True\n\n@pytest.mark.dependency(\"test_2::test_a\")\n@pytest.mark.dependency(\"test_2::test_b\")\ndef test_c():\n    assert True\n</code></pre> <p>An example of its use can be found on page skip test.</p>"},{"location":"features/features/#critical-tests","title":"Critical tests","text":"<p>The HardPy allows the user to skip subsequent tests marked as critical marker, if a critical test fails or is skipped. The user can designate individual tests or entire modules as critical. If a test case is marked as critical, its failure will skip all remaining tests in the current and subsequent modules. If an entire module is marked as critical, the failure of any test within it will skip all remaining tests in that module and subsequent modules.  </p> <p>Example (test level):</p> <pre><code>@pytest.mark.critical\ndef test_a():\n    assert False\n\ndef test_b():\n    assert True\n</code></pre> <p>Example (module level):</p> <pre><code>pytestmark = pytest.mark.critical\n\ndef test_c():\n    assert True\n</code></pre> <p>An example of its use can be found on page critical.</p>"},{"location":"features/features/#test-case-and-test-module-grouping","title":"Test case and test module grouping","text":"<p>The HardPy allows users to organize tests into logical groups: <code>setup</code>, <code>main</code>, and <code>teardown</code>. This grouping helps structure test execution flow and reporting.</p> <p>Users can designate individual tests or entire modules with specific groups. By default, all tests are in the <code>main</code> group.</p> <p>Example (test case level):</p> <pre><code>import pytest\nfrom hardpy.pytest_hardpy.utils.const import Group\n\n@pytest.mark.case_group(Group.SETUP)\ndef test_setup_case():\n    # This test will be executed first as part of setup\n    assert initialize_system()\n\n@pytest.mark.case_group(Group.MAIN)\ndef test_main_functionality():\n    # This is a main test case\n    assert check_system_operation()\n\n@pytest.mark.case_group(Group.TEARDOWN)\ndef test_cleanup():\n    # This test will clean up after execution\n    assert cleanup_resources()\n</code></pre> <p>Example (module level):</p> <pre><code>import pytest\nfrom hardpy.pytest_hardpy.utils.const import Group\n\n# All tests in this module will be teardown tests\npytestmark = pytest.mark.module_group(Group.TEARDOWN)\n\ndef test_cleanup_database():\n    assert clear_database_tables()\n\ndef test_release_resources():\n    assert free_allocated_memory()\n\ndef test_generate_final_report():\n    assert create_execution_summary()\n</code></pre> <p>An example of its use can be found on page test grouping documentation.</p>"},{"location":"features/features/#logging-approaches-in-hardpy","title":"Logging approaches in HardPy","text":"<p>HardPy provides several methods for logging and user interaction during testing. Choose the appropriate method based on whether you need to store information in the database or just display messages to the operator.</p>"},{"location":"features/features/#database-logging-with-set_message","title":"Database logging with set_message","text":"<p>The set_message function stores the log message in the report. Messages without specified keys get auto-generated keys, while known keys update existing messages. Perfect for tracking test progress, recording measurements, system states, and verification results.</p> <pre><code>def test_temperature_sensor():\n    temp = read_temperature()\n    if temp &gt; 50:\n        set_message(f\"Warning: High temperature {temp}\u00b0C\", \"temp_warning\")\n    else:\n        set_message(f\"Normal temperature {temp}\u00b0C\")\n</code></pre>"},{"location":"features/features/#interactive-dialogs-with-run_dialog_box","title":"Interactive dialogs with run_dialog_box","text":"<p>The run_dialog_box function creates dialog boxes for operator input with various widgets (text, numeric, checkboxes). Supports titles, images, and HTML content. Can be used to notify and interact with the user, but does not save the information in the report. The user can save the information separately using the set_case_artifact, set_module_artifact or set_run_artifact functions. Essential for manual equipment verification, test configuration confirmation, and safety checks.</p> <pre><code>def test_manual_calibration():\n    dialog = DialogBox(\n        dialog_text=\"Connect calibration device and press Confirm\",\n        title_bar=\"Calibration Setup\",\n        widget=RadioButtonWidget(options=[\"Ready\", \"Skip\", \"Abort\"])\n    )\n    response = run_dialog_box(dialog)\n    assert response == \"Ready\", \"Calibration was not confirmed\"\n</code></pre>"},{"location":"features/features/#operator-messages-with-set_operator_message","title":"Operator messages with set_operator_message","text":"<p>The set_operator_message function displays non-interactive notifications without database storage. The user can save the information separately using the set_case_artifact, set_module_artifact or set_run_artifact functions. Can be used to notify and interact with the user, but does not save the information in the report. Supports titles, images, and HTML content, with optional blocking until dismissed. Ideal for equipment setup instructions, test phase transitions, and important warnings.</p> <pre><code>def test_system_startup():\n    set_operator_message(\n        msg=\"Please power on all test equipment\",\n        title=\"Initial Setup\",\n        image=ImageComponent(address=\"assets/power_on.png\"),\n        block=True\n    )\n</code></pre>"},{"location":"features/features/#logging-recommendation","title":"Logging recommendation","text":"Method Database Storage User Interaction Best For <code>set_message</code> Yes No Permanent logs <code>run_dialog_box</code> No Yes Test steps requiring input <code>set_operator_message</code> No No Important notifications"},{"location":"features/features/#standcloud","title":"StandCloud","text":""},{"location":"features/features/#storing-test-result-to-standcloud","title":"Storing test result to StandCloud","text":"<p>HardPy allows user to send test results to StandCloud, a data storage and analysis platform. An example of its use can be found on pages StandCloud and StandCloud example.</p>"},{"location":"features/features/#reading-test-result-from-standcloud","title":"Reading test result from StandCloud","text":"<p>HardPy allows user to read test result from StandCloud. An example of its use can be found on page StandCloud reader.</p>"},{"location":"features/features/#other","title":"Other","text":""},{"location":"features/features/#measurement","title":"Measurement","text":"<p>HardPy allows users to save their measurement data to a database and display it in the operator panel. The set_case_measurement function allows each individual test case to store measurements as a list.</p> <p>Display format in operator panel:</p> <ul> <li>Name Value Unit - for measurements with all components (e.g., \"Voltage 12.3 V\")</li> <li>Value Unit - for measurements without names (e.g., \"5\u00b0\", \"98.6%\", \"45\u2033\")  </li> <li>Name Value - for measurements without units (e.g., \"Banana 15\")</li> </ul> <p>Special formatting for units:</p> <ul> <li>Symbols <code>%</code>, <code>\u00b0</code>, <code>\u2032</code>, <code>\u2033</code> are displayed without space (e.g., \"23.5\u00b0C\", \"98.6%\")</li> <li>Other units are displayed with space (e.g., \"12.3 V\", \"3.14 rad\")</li> </ul> <p>Configuration: Enable measurement display in your <code>hardpy.toml</code>:</p> <pre><code>[frontend]\nmeasurement_display = true\n</code></pre> <p>Features:</p> <ul> <li>Support for both numeric and string measurements</li> <li>Clean, minimal tag-based display</li> <li>Proper handling of special units (\u00b0, %, etc.)</li> </ul> <p>An example of its use can be found on page measurement.</p>"},{"location":"features/features/#chart","title":"Chart","text":"<p>HardPy allows users to save their chart (data series) to a database. The set_case_chart  function allows each individual test case to store a single chart. An example of its use can be found on page chart.</p>"},{"location":"features/features/#multiple-hardpy-instances-in-a-single-stand","title":"Multiple HardPy instances in a single stand","text":"<p>A user can run multiple HardPy instances on a single stand. To create a document in the database for storing test data, each stand that  runs on a single computer must have a unique combination of  frontend host and port. The startup is described in the Multiple Stand example.</p>"},{"location":"features/features/#multiple-test-plan-configurations","title":"Multiple test plan configurations","text":"<p>HardPy allows the user to define and manage multiple test configurations within a single project.  This feature enables flexible test execution, allowing different sets of tests or  variations of the same tests to be run by simply selecting a configuration. This is achieved by defining different <code>pytest.ini</code> files for each configuration and referencing  them in the main <code>hardpy.toml</code> file. </p> <p>An example of its use can be found on page Multiple configs.</p>"}]}